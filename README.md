# pytorch Variational Autoencoders
This is a collection of variational autoencoders (or sufficiently related
models) coded in pytorch and available as a Jupyter notebook. The level of
explanation provided between notebooks varies. Links are included within the
notebooks to sources for those interested in learning more. The notebooks are
listed in reverse chronological order. The pictures below are generated by
the model after training. They all feature MNIST images at the moment for
comparative purposes.

## Lagrangian VAE

The Lagrangian VAE is a Variational Autoencoder that optimizes the
Lagrangian dual equation to provide both consistency and meaningful encodings.

[Lagrangian VAE Notebook (draft)](./Lagrangian_VAE.ipynb)

Image reconstruction of MNIST. It's worth noting how much sharper these images
are as opposed to the basic VAE, even after interjecting noise.

![](./images/MNIST-lagVAE-reconstructions.png)

Image reconstruction using sampling of latent space. These images are not as
sharp, but I don't know how to fix that.

![](./images/MNIST-lagVAE-latent_samples.png)

Distribution of encoded means. While not conclusive, this seems to
suggest that the model learns to group similar observations near eachother.

![](./images/lagVAE-mean-distribution.png)

## MADE (Masked Autoencoders for Distribution Estimation)

MADE is an autoencoder that uses masks to provide an autoregressive property.
The use of multiple masks effectively give us one set of parameters that can
be used to form an ensemble.

[MADE Notebook](./MADE.ipynb)

Image reconstruction of MNIST using an ensemble of masks.

![](./images/MADE-ensemble-reconstructions.png)

## Basic Variational Autoencoder

A Variational Autoencoder is a type of deep generative model. Below we can see
some example image reconstructions.

[VAE Notebook](./Basic_VAE.ipynb)

Image Reconstructions of MNIST

![](./images/MNIST-VAE-reconstructions.png)

Image Generation using sampling of latent space

![](./images/MNIST-VAE-latent_samples.png)
