{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Ladder Autoencoders\n",
    "\n",
    "## Summary\n",
    "This notebook describes and implements a [Variational Ladder Autoencoder (VLAE)](https://arxiv.org/abs/1702.08396). Similar to a Variational Autoencoder (VAE), a VLAE learns to both encode a data point into a latent variable and to decode the latent variable into a reconstruction of the data point. Unlike a VAE, a VLAE outputs latent variables at multiple layers of the encoding network, rather than feeding through each layer to one latent variable. The change allows for finer features to be disentangled from coarser features.\n",
    "\n",
    "The architecture implemented here is purely experimental. The original implementation can be found [here](https://github.com/ermongroup/Variational-Ladder-Autoencoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VLAE Class\n",
    "\n",
    "The class used here is similar to the VAE class used in my [VAE notebook](./Basic_VAE.ipynb). However, the 'generate_similar' method now has "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VLAE(nn.Module):\n",
    "    '''\n",
    "    Creates a Variation Autoencoder\n",
    "    \n",
    "    :param encoder_network: (nn.Module)\n",
    "    :param decoder_network: (nn.Module)\n",
    "    :param loss: (str, opt) Either 'kl' or 'mc'. Sets how to calculate loss.\n",
    "    '''\n",
    "    def __init__(self, encoder_network, decoder_network, loss='mc'):\n",
    "        super(VLAE, self).__init__()\n",
    "        \n",
    "        self.add_module('encode', encoder_network)\n",
    "        self.add_module('generate', decoder_network)\n",
    "        self.loss = loss.lower()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Passes forward through encoder to create a sample latent\n",
    "        representation, then decodes that representation.\n",
    "        \n",
    "        :param input: (t.Tensor) \n",
    "        :return: (t.Tensor) Reconstruction of x and it's latent loss\n",
    "        '''\n",
    "        mean, scale = self.encode(input)\n",
    "        eps = t.randn_like(mean)\n",
    "        z = mean+scale*eps\n",
    "        if self.training:\n",
    "            if self.loss == 'mc':\n",
    "                loss = self.MC(z, eps, scale)\n",
    "            elif self.loss == 'kl':\n",
    "                loss = -self.KL(mean, scale)\n",
    "        else:\n",
    "            loss = t.tensor(0)\n",
    "        \n",
    "        return self.generate(z), loss\n",
    "\n",
    "    def generate_similar(self, input, noise=True):\n",
    "        '''\n",
    "        Creates similar examples by encoding the observation and then decoding the \n",
    "        latent observation. If noise is True, then it uses the latent variable\n",
    "        z = mean+eps*scale. Otherwise, z = mean.\n",
    "\n",
    "        :param input: (t.Tensor)\n",
    "        :param noise: (Bool, opt)\n",
    "        :return: (torch.Tensor) Reconstruction of input\n",
    "        '''\n",
    "        mean, scale = self.encode(input)\n",
    "\n",
    "        if noise:\n",
    "            eps = t.randn_like(mean)\n",
    "            z = mean+scale*eps\n",
    "        else:\n",
    "            z = mean\n",
    "\n",
    "        return self.generate(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def MC(z, eps, scale):\n",
    "        '''\n",
    "        Computes fully monte carlo loss for the statistics layer.\n",
    "        \n",
    "        :param z: (t.Tensor) latent variable\n",
    "        :param eps: (t.Tensor) random noise\n",
    "        :param scale: (t.Tensor) scale (or standard deviation)\n",
    "        '''     \n",
    "        loss = t.sum(z.pow(2)/2-(eps.pow(2)/2+scale.log()), 1)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility layer\n",
    "This layer reshapes the input. This is done for convenience when using `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer(nn.Module):\n",
    "    '''\n",
    "    Reshapes the input to be the output using view. Shapes are checked\n",
    "    on forward pass to verify that they are compatible.\n",
    "\n",
    "    :param view_shape: The shape to cast the input to. Given a batch\n",
    "        input of shape (n, _) will be cast to (n, view_shape).\n",
    "    '''\n",
    "    def __init__(self, view_shape):\n",
    "        super(ReshapeLayer, self).__init__()\n",
    "        self.view_shape = view_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Reshapes x to initialized shape.\n",
    "        :param x: (t.Tensor)\n",
    "        :return: (t.Tensor)\n",
    "        '''\n",
    "        output_shape = (x.shape[0],) + self.view_shape\n",
    "        assert self.dimension(x.shape) == self.dimension(output_shape), \\\n",
    "            '{0} and {1} are not compatabile'.format(x.shape, output_shape)\n",
    "        return x.view(output_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def dimension(shape):\n",
    "        #Helper function for checking dimensions\n",
    "        out = 1\n",
    "        for s in shape:\n",
    "            out *= s\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d or type(m) == nn.ConvTranspose2d:\n",
    "        nn.init.kaiming_normal_(m.weight, a=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_size, image_size, z_size, drop_prob=0.1):\n",
    "        super(FeatureEncoder, self).__init__()\n",
    "        self.feature_map = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_channels*image_size**2, hidden_size),\n",
    "            nn.SELU(),\n",
    "            nn.AlphaDropout(p=drop_prob),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "        self.mean = nn.Linear(hidden_size, z_size)\n",
    "        self.log_var = nn.Linear(hidden_size, z_size)\n",
    "        \n",
    "        self.feature_map.apply(init_weights)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.feature_map(input)\n",
    "        mean = self.mean(x)\n",
    "        scale = (self.log_var(x)/2).exp()\n",
    "        return mean, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(ConvDown, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding),\n",
    "            nn.SELU()\n",
    "        )\n",
    "        self.conv.apply(init_weights)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channel_factor, hidden_size, z_size, drop_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = ConvDown(3, channel_factor, 4, 2, 1)\n",
    "        self.feat1 = FeatureEncoder(channel_factor, hidden_size, 16, z_size, drop_prob)\n",
    "        self.conv2 = ConvDown(channel_factor, 2*channel_factor, 4, 2, 1)\n",
    "        self.feat2 = FeatureEncoder(2*channel_factor, hidden_size, 8, z_size, drop_prob)\n",
    "        self.conv3 = ConvDown(2*channel_factor, 4*channel_factor, 4, 2, 1)\n",
    "        self.feat3 = FeatureEncoder(4*channel_factor, hidden_size, 4, z_size, drop_prob)\n",
    "        self.conv4 = ConvDown(4*channel_factor, 8*channel_factor, 4, 1)\n",
    "        self.feat4 = FeatureEncoder(8*channel_factor, hidden_size, 1, z_size, drop_prob)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        mean1, scale1 = self.feat1(x)\n",
    "        x = self.conv2(x)\n",
    "        mean2, scale2 = self.feat2(x)\n",
    "        x = self.conv3(x)\n",
    "        mean3, scale3 = self.feat3(x)\n",
    "        x = self.conv4(x)\n",
    "        mean4, scale4 = self.feat4(x)\n",
    "        means = t.cat([mean1, mean2, mean3, mean4], 1)\n",
    "        scales = t.cat([scale1, scale2, scale3, scale4], 1)\n",
    "        return means, scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following generator uses several layers of latent variables, which are added at each convolution. Originally I tried concatenation, but this failed to produce convincing images after 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDecoder(nn.Module):\n",
    "    def __init__(self, z_size, hidden_size, image_size, out_channels, drop_prob=0.1):\n",
    "        super(FeatureDecoder, self).__init__()\n",
    "        self.feature_map = nn.Sequential(\n",
    "            nn.Linear(z_size, hidden_size),\n",
    "            nn.SELU(),\n",
    "            nn.AlphaDropout(p=drop_prob),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.SELU(),\n",
    "            nn.AlphaDropout(p=drop_prob),\n",
    "            nn.Linear(hidden_size, out_channels*image_size**2),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "        self.out_shape = (out_channels, image_size, image_size)\n",
    "        self.feature_map.apply(init_weights)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.feature_map(input)\n",
    "        return x.reshape((-1, *self.out_shape))\n",
    "    \n",
    "class ConvUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(ConvUp, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.SELU()\n",
    "        )\n",
    "        self.conv.apply(init_weights)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channel_factor, hidden_size, z_size, drop_prob):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.z_size = z_size\n",
    "        self.feat1 = FeatureDecoder(z_size, hidden_size, 1, 8*channel_factor, drop_prob)\n",
    "        self.conv1 = ConvUp(8*channel_factor, 4*channel_factor, 4, 1)\n",
    "        self.feat2 = FeatureDecoder(z_size, hidden_size, 4, 4*channel_factor, drop_prob)\n",
    "        self.conv2 = ConvUp(4*channel_factor, 2*channel_factor, 4, 2, 1)\n",
    "        self.feat3 = FeatureDecoder(z_size, hidden_size, 8, 2*channel_factor, drop_prob)\n",
    "        self.conv3 = ConvUp(2*channel_factor, channel_factor, 4, 2, 1)\n",
    "        self.feat4 = FeatureDecoder(z_size, hidden_size, 16, channel_factor, drop_prob)\n",
    "        self.conv4 = nn.ConvTranspose2d(channel_factor, 3, 4, 2, 1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        #Split into 4 equally sized latent variables\n",
    "        z1, z2, z3, z4 = input.split(self.z_size, 1)\n",
    "        \n",
    "        x1 = self.feat1(z1)\n",
    "        x2 = self.feat2(z2)\n",
    "        x3 = self.feat3(z3)\n",
    "        x4 = self.feat4(z4)\n",
    "        \n",
    "        x = self.conv1(x1)\n",
    "        x = self.conv2(x+x2)\n",
    "        x = self.conv3(x+x3)\n",
    "        return t.tanh(self.conv4(x+x4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network\n",
    "\n",
    "Here we build the encoder and decoder. This is done using the Sequential function. The encoder network needs to end with the statistics layer. The decoder is just a standard decoder. The size of the hidden layers are chosen experimentally.\n",
    "\n",
    "The encoder and decoder networks are based on the Deep Convolution GAN architecture. The images are encoded using strided convolutions with dropout, batch normalization and Leaky ReLU units. The latent encoding is then decoded using strided convolution tranpositions with dropout, batch normalization and ReLU units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 16\n",
    "channel_factor = 16\n",
    "drop_prob = 0.1\n",
    "\n",
    "#Build networks\n",
    "encoder_net = Encoder(channel_factor, latent_dim, latent_dim, drop_prob)\n",
    "\n",
    "decoder_net = Decoder(channel_factor, latent_dim, latent_dim, drop_prob)\n",
    "\n",
    "model = VLAE(encoder_net, decoder_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "For this notebook, we use the MNIST dataset. A drop-in replacement that can be used is the FashionMNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/train_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "transforms = Compose([ToTensor(), Normalize([0.5], [0.5])])\n",
    "training_set = datasets.SVHN(root='./data/', split='train', download=True, transform=transforms)\n",
    "batch_size = 128\n",
    "training_loader = data.DataLoader(dataset=training_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the network parameters\n",
    "\n",
    "Now that we have the network, we need to find parameters that minimizes our estimate above. In order to accomplish this, we run the model over a batch of observations, and then compute the loss. Using stochastic gradient descent methods, we can then find a change of parameters that will (usually) decrease the loss.\n",
    "\n",
    "More explicitely, the encoder network takes in an observation $x$ and produces statistics, $(\\mu, \\Sigma)$, for the latent variable, $z$. Using the Monte Carlo loss or KL divergence above, we can determine a loss for the encoder. We then use $z$ to generate a reconstruction of the observation, $\\hat{x}$. By comparing $x$ and $\\hat{x}$, we have a loss for the decoder. The total loss of the variational autoencoder is the sum of these two losses. Because we are minimizing the sum, there is no guarantee that both components will decrease. In fact, the encoder loss typically increases while the decoder loss typically decreases. This is seen in the training summary printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(input, target):\n",
    "    loss = (input-target).pow(2)/2\n",
    "    #Return batch average\n",
    "    return loss.mean(0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model, data_loader, optimizer, use_gpu=True):\n",
    "    average_recon = 0\n",
    "    average_latent = 0\n",
    "    for n_batch, (x, _) in enumerate(data_loader):\n",
    "        if use_gpu:\n",
    "            x = x.cuda()\n",
    "        \n",
    "        x_pred, latent_loss = model(x)\n",
    "        \n",
    "        #compute -log p(x|z)\n",
    "        recon_loss = mse_loss(x, x_pred)\n",
    "        loss = recon_loss+latent_loss\n",
    "        \n",
    "        #Compute average\n",
    "        average_latent += float(latent_loss-average_latent)/(n_batch+1)\n",
    "        average_recon += float(recon_loss-average_recon)/(n_batch+1)\n",
    "\n",
    "        #Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Print percent complete\n",
    "        print(percent_complete.format(batch_size*n_batch/n_data), end='')\n",
    "        \n",
    "    return average_recon, average_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - recon loss: 126.74 - latent loss: 17.186 - total loss: 143.93\n",
      "Epoch 2/5 - recon loss: 80.278 - latent loss: 14.268 - total loss: 94.546\n",
      "Epoch 3/5 - recon loss: 73.355 - latent loss: 12.973 - total loss: 86.329\n",
      "Epoch 4/5 - recon loss: 69.935 - latent loss: 12.053 - total loss: 81.988\n",
      "Epoch 5/5 - recon loss: 67.799 - latent loss: 11.366 - total loss: 79.165\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "use_gpu = t.cuda.is_available()\n",
    "model.train()\n",
    "\n",
    "#Add to gpu before making the optimizer\n",
    "if use_gpu:\n",
    "    model.cuda(0)\n",
    "\n",
    "optimizer = t.optim.Adamax(model.parameters())\n",
    "\n",
    "epoch_summary_text = '\\rEpoch {0}/{1}'\n",
    "loss_summary_text = '{0} loss: {1:.5}'\n",
    "\n",
    "percent_complete = '\\r{0:.1%}'\n",
    "\n",
    "n_data = len(training_set)\n",
    "for epoch in range(epochs):\n",
    "    recon_loss, latent_loss = train_vae(model, training_loader, optimizer, use_gpu)\n",
    "\n",
    "    #Create summary loss\n",
    "    epoch_summary = epoch_summary_text.format(epoch+1, epochs)\n",
    "    latent_summary = loss_summary_text.format('latent', latent_loss)\n",
    "    recon_summary = loss_summary_text.format('recon', recon_loss)\n",
    "    total_summary = loss_summary_text.format('total', latent_loss+recon_loss)\n",
    "    print(epoch_summary, recon_summary, latent_summary, total_summary, sep=' - ')\n",
    "    \n",
    "t.save(model.state_dict(), './models/vlae.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Image Reconstruction\n",
    "\n",
    "Variational autoencoders can be used to reconstruct observations. This is exactly the training process: we encode the observation to a latent variable, and then use the latent variable to generate an image. After training, the network should be able to reconstruct both the example and similar examples. This is shown below. The images in the top row are the originals, while the images below are the reconstructions. The second row is a reconstruction using the mean, while the lower rows show what would be typically generated during training.\n",
    "\n",
    "The reconstructions are noticeably blurry. This is a fairly common issue with simple VAE models. There are a number of ways to improve the quality of the generated pictures. A few of these are mentioned below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up picture\n",
    "n_images = 6\n",
    "rows=5\n",
    "\n",
    "model.eval()\n",
    "with t.no_grad():\n",
    "    data_sample = next(iter(training_loader))\n",
    "    images, _ = data_sample\n",
    "    images = images[:n_images]\n",
    "    if use_gpu:\n",
    "        images = images.cuda()\n",
    "    \n",
    "    album = [images]\n",
    "    #Create most likely example (i.e. using the mean)\n",
    "    album.append(model.generate_similar(images, noise=False))\n",
    "    \n",
    "    #Create similar examples with noise\n",
    "    for row in range(2, rows):\n",
    "        album.append(model.generate_similar(images))\n",
    "\n",
    "album = t.cat(album, 0)\n",
    "save_image(album, './images/MNIST-VLAE-reconstructions.png', nrow=n_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Reconstructions\n",
    "![Digit reconstructions](./images/MNIST-VLAE-reconstructions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image generation using latent sampling\n",
    "\n",
    "My interest in VAEs are mostly tied to the fact that you can use them to generate new images. One of the original assumptions we made at the onset of our model is that $p_\\ell$ is normally distributed. Thus, we can generate $z$ by drawing from a standard normal distribution $N(0, I)$ and then decode that generated latent variable to form an observation. This is what we see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 10\n",
    "\n",
    "with t.no_grad():\n",
    "    latent_sample = t.randn(grid_size**2, latent_dim*4, device=album.device)\n",
    "    images = model.generate(latent_sample)\n",
    "\n",
    "save_image(images, './images/MNIST-VLAE-latent_samples.png', nrow=grid_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly generated images\n",
    "![Latent Sample image](./images/MNIST-VLAE-latent_samples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Interpolation\n",
    "\n",
    "Another way to use a Variational Autoencoder is to combine the latent encodings. That is, if we have an image $x_1$ and $x_2$ with latent encodings $z_1$ and $z_2$ respectively, we can form a third image $x_3$ from a combination of $z_1$ and $z_2$. This is shown below. The first and last images are the original. The second and fourth are the recreations of the original. The image in the middle, $x_3$, is formed using the latent encoding $(1-\\alpha)z_1+\\alpha z_2$, with $0 \\leq \\alpha \\leq 1$. By changing the sliders, the center changes. \n",
    "\n",
    "Note: if training the model locally would take too long, then I recommend downloading the file `/models/vae.pkl` and loading it using `t.load(model, 'vae.pkl')` to allow for experimentation with the below widget. This model can also be further trained if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173779f2fe264962ae89746f8249f84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(BoundedIntText(value=1, description='Image 1', max=1000, min=1), BoundedIntText(value=2,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_1_slider = widgets.BoundedIntText(\n",
    "    value=1,\n",
    "    min=1,\n",
    "    max=1000,\n",
    "    description='Image 1'\n",
    ")\n",
    "\n",
    "image_2_slider = widgets.BoundedIntText(\n",
    "    value=2,\n",
    "    min=1,\n",
    "    max=1000,\n",
    "    description='Image 2'\n",
    ")\n",
    "\n",
    "interactive_plot = widgets.interactive(image_mix, image_idx_1=image_1_slider, image_idx_2=image_2_slider, mix_value=mix_slider)\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '150px'\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d1f1acded8445f828b33e2a5beefa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='Image 1', min=1), IntSlider(value=101, description='Imag…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def image_mix(latent_1, latent_2, layer_idx, mix_value):\n",
    "    with t.no_grad():\n",
    "        #Grab sample images\n",
    "        image_1 = training_set[image_idx_1-1][0]\n",
    "        image_2 = training_set[image_idx_2-1][0]\n",
    "        #Encode\n",
    "        input = t.stack([image_1, image_2], 0)\n",
    "        if use_gpu:\n",
    "            input = input.cuda()\n",
    "        latent = model.encode(input)[0]\n",
    "        #Split up latent codes\n",
    "        latent_1 = latent[0].split(latent_dim, 1)\n",
    "        latent_2 = latent[1].split(latent_dim, 1)\n",
    "        #Mix latent codes\n",
    "        mixed_latent = (1-mix_value)*latent_1[layer_idx]+mix_value*latent_2[layer_idx]\n",
    "        #Decode into images\n",
    "        latent = t.stack([latent[0], mixed_latent, latent[1]], 0)\n",
    "        images = model.generate(latent)\n",
    "        #Turn into grid\n",
    "        images = make_grid(t.cat([input[0].unsqueeze(0), images, input[1].unsqueeze(0)]), 5)\n",
    "        #Permute for imshow\n",
    "        images = (images.permute(1,2,0)+1)/2\n",
    "        plt.imshow(images.cpu())\n",
    "        \n",
    "model.eval()\n",
    "\n",
    "mix_slider = widgets.FloatSlider(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=1,\n",
    "    step=0.05,\n",
    "    description='Transition'\n",
    ")\n",
    "\n",
    "image_1_slider = widgets.IntSlider(\n",
    "    value=1,\n",
    "    min=1,\n",
    "    max=100,\n",
    "    description='Image 1'\n",
    ")\n",
    "\n",
    "image_2_slider = widgets.IntSlider(\n",
    "    value=101,\n",
    "    min=101,\n",
    "    max=200,\n",
    "    description='Image 2'\n",
    ")\n",
    "\n",
    "interactive_plot = widgets.interactive(image_mix, image_idx_1=image_1_slider, image_idx_2=image_2_slider, mix_value=mix_slider)\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '150px'\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (3, 32, 32) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-a56dc36141bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Programs\\Anaconda3\\envs\\pytorch_nb\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2681\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2682\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[1;32m-> 2683\u001b[1;33m         None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2684\u001b[0m     \u001b[0msci\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2685\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda3\\envs\\pytorch_nb\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1599\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1601\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda3\\envs\\pytorch_nb\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    367\u001b[0m                 \u001b[1;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[1;32m--> 369\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda3\\envs\\pytorch_nb\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    367\u001b[0m                 \u001b[1;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[1;32m--> 369\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda3\\envs\\pytorch_nb\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5669\u001b[0m                               resample=resample, **kwargs)\n\u001b[0;32m   5670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5671\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5672\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5673\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda3\\envs\\pytorch_nb\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    688\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m    689\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[1;32m--> 690\u001b[1;33m                             .format(self._A.shape))\n\u001b[0m\u001b[0;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (3, 32, 32) for image data"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAMmUlEQVR4nO3bYYjkd33H8ffHXFNpGrWYFeTuNJFeqtdQiF3SFKFGTMslhbsnIncQWkvw0Br7QCmkWFKJjxppBeFae7QSFTSePqiLnAS0EYt4mg3R6F24sj1ts0SaU9M8EY2h3z6Y0U7mu3v7v8vszC19v2Bh/v/5zex3h7n3/ue//0tVIUmTXrToASRdfgyDpMYwSGoMg6TGMEhqDIOkZsswJPlokqeSfGeT+5Pkw0nWkjyW5PWzH1PSPA05YrgfOHCB+28D9o2/jgJ//8LHkrRIW4ahqr4C/OgCSw4BH6+RU8DLkrxyVgNKmr9dM3iO3cATE9vr433fn16Y5Cijowquuuqq337ta187g28vaTOPPPLID6pq6WIfN4swZIN9G15nXVXHgeMAy8vLtbq6OoNvL2kzSf7jUh43i79KrAN7J7b3AE/O4HklLcgswrAC/NH4rxM3A89UVfsYIWnn2PKjRJJPAbcA1yRZB/4K+CWAqvoIcBK4HVgDfgz8yXYNK2k+tgxDVR3Z4v4C3jWziSQtnFc+SmoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagaFIcmBJGeTrCW5e4P7X5XkoSSPJnksye2zH1XSvGwZhiRXAMeA24D9wJEk+6eW/SVwoqpuBA4DfzfrQSXNz5AjhpuAtao6V1XPAg8Ah6bWFPCS8e2XAk/ObkRJ8zYkDLuBJya218f7Jr0fuCPJOnASePdGT5TkaJLVJKvnz5+/hHElzcOQMGSDfTW1fQS4v6r2ALcDn0jSnruqjlfVclUtLy0tXfy0kuZiSBjWgb0T23voHxXuBE4AVNXXgBcD18xiQEnzNyQMDwP7klyX5EpGJxdXptb8J/BmgCSvYxQGPytIO9SWYaiq54C7gAeBxxn99eF0knuTHBwvey/w9iTfAj4FvK2qpj9uSNohdg1ZVFUnGZ1UnNx3z8TtM8AbZjuapEXxykdJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQYBkmNYZDUGAZJjWGQ1BgGSY1hkNQMCkOSA0nOJllLcvcma96a5EyS00k+OdsxJc3Trq0WJLkCOAb8PrAOPJxkparOTKzZB/wF8IaqejrJK7ZrYEnbb8gRw03AWlWdq6pngQeAQ1Nr3g4cq6qnAarqqdmOKWmehoRhN/DExPb6eN+k64Hrk3w1yakkBzZ6oiRHk6wmWT1//vylTSxp2w0JQzbYV1Pbu4B9wC3AEeAfk7ysPajqeFUtV9Xy0tLSxc4qaU6GhGEd2DuxvQd4coM1n6uqn1XVd4GzjEIhaQcaEoaHgX1JrktyJXAYWJla88/AmwCSXMPoo8W5WQ4qaX62DENVPQfcBTwIPA6cqKrTSe5NcnC87EHgh0nOAA8Bf15VP9yuoSVtr1RNny6Yj+Xl5VpdXV3I95b+v0jySFUtX+zjvPJRUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSMygMSQ4kOZtkLcndF1j3liSVZHl2I0qaty3DkOQK4BhwG7AfOJJk/wbrrgb+DPj6rIeUNF9DjhhuAtaq6lxVPQs8ABzaYN0HgPuAn8xwPkkLMCQMu4EnJrbXx/t+IcmNwN6q+vyFnijJ0SSrSVbPnz9/0cNKmo8hYcgG++oXdyYvAj4EvHerJ6qq41W1XFXLS0tLw6eUNFdDwrAO7J3Y3gM8ObF9NXAD8OUk3wNuBlY8ASntXEPC8DCwL8l1Sa4EDgMrP7+zqp6pqmuq6tqquhY4BRysqtVtmVjSttsyDFX1HHAX8CDwOHCiqk4nuTfJwe0eUNL87RqyqKpOAien9t2zydpbXvhYkhbJKx8lNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVJjGCQ1hkFSYxgkNYZBUmMYJDWGQVIzKAxJDiQ5m2Qtyd0b3P+eJGeSPJbkS0lePftRJc3LlmFIcgVwDLgN2A8cSbJ/atmjwHJV/RbwWeC+WQ8qaX6GHDHcBKxV1bmqehZ4ADg0uaCqHqqqH483TwF7ZjumpHkaEobdwBMT2+vjfZu5E/jCRnckOZpkNcnq+fPnh08paa6GhCEb7KsNFyZ3AMvABze6v6qOV9VyVS0vLS0Nn1LSXO0asGYd2DuxvQd4cnpRkluB9wFvrKqfzmY8SYsw5IjhYWBfkuuSXAkcBlYmFyS5EfgH4GBVPTX7MSXN05ZhqKrngLuAB4HHgRNVdTrJvUkOjpd9EPhV4DNJvplkZZOnk7QDDPkoQVWdBE5O7btn4vatM55L0gJ55aOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6TGMEhqDIOkxjBIagyDpMYwSGoMg6RmUBiSHEhyNslakrs3uP+Xk3x6fP/Xk1w760Elzc+WYUhyBXAMuA3YDxxJsn9q2Z3A01X168CHgL+e9aCS5mfIEcNNwFpVnauqZ4EHgENTaw4BHxvf/izw5iSZ3ZiS5mnXgDW7gScmtteB39lsTVU9l+QZ4OXADyYXJTkKHB1v/jTJdy5l6AW5hqmf5zK2k2aFnTXvTpoV4Dcu5UFDwrDRb/66hDVU1XHgOECS1apaHvD9Lws7ad6dNCvsrHl30qwwmvdSHjfko8Q6sHdiew/w5GZrkuwCXgr86FIGkrR4Q8LwMLAvyXVJrgQOAytTa1aAPx7ffgvwL1XVjhgk7QxbfpQYnzO4C3gQuAL4aFWdTnIvsFpVK8A/AZ9IssboSOHwgO99/AXMvQg7ad6dNCvsrHl30qxwifPGX+ySpnnlo6TGMEhqtj0MO+ly6gGzvifJmSSPJflSklcvYs6JeS4478S6tySpJAv7M9uQWZO8dfz6nk7yyXnPODXLVu+FVyV5KMmj4/fD7YuYczzLR5M8tdl1QRn58PhneSzJ67d80qrati9GJyv/HXgNcCXwLWD/1Jo/BT4yvn0Y+PR2zvQCZ30T8Cvj2+9c1KxD5x2vuxr4CnAKWL5cZwX2AY8CvzbefsXl/NoyOqn3zvHt/cD3Fjjv7wGvB76zyf23A19gdL3RzcDXt3rO7T5i2EmXU285a1U9VFU/Hm+eYnRNx6IMeW0BPgDcB/xknsNNGTLr24FjVfU0QFU9NecZJw2Zt4CXjG+/lH5tz9xU1Ve48HVDh4CP18gp4GVJXnmh59zuMGx0OfXuzdZU1XPAzy+nnrchs066k1GFF2XLeZPcCOytqs/Pc7ANDHltrweuT/LVJKeSHJjbdN2Qed8P3JFkHTgJvHs+o12Si31vD7ok+oWY2eXUczB4jiR3AMvAG7d1ogu74LxJXsTof7q+bV4DXcCQ13YXo48TtzA6EvvXJDdU1X9v82wbGTLvEeD+qvqbJL/L6DqeG6rqf7Z/vIt20f/GtvuIYSddTj1kVpLcCrwPOFhVP53TbBvZat6rgRuALyf5HqPPlisLOgE59H3wuar6WVV9FzjLKBSLMGTeO4ETAFX1NeDFjP6D1eVo0Hv7ebb5pMgu4BxwHf93Euc3p9a8i+effDyxoBM4Q2a9kdFJqX2LmPFi551a/2UWd/JxyGt7APjY+PY1jA59X34Zz/sF4G3j268b/0PLAt8P17L5ycc/5PknH7+x5fPNYeDbgX8b/4N633jfvYx+48KotJ8B1oBvAK9Z4Iu71axfBP4L+Ob4a2VRsw6Zd2rtwsIw8LUN8LfAGeDbwOHL+bVl9JeIr46j8U3gDxY466eA7wM/Y3R0cCfwDuAdE6/tsfHP8u0h7wMviZbUeOWjpMYwSGoMg6TGMEhqDIOkxjBIagyDpOZ/AS9qX9SUF4NfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.im(training_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "If you are interested in learning more about variational autoencoders, here is what I recommend:\n",
    "* [Sticking the Landing](https://arxiv.org/abs/1703.09194). As mentioned above, it explains why the MC loss is used rather than KL divergence.\n",
    "* [Towards a deeper understanding of VAEs](https://arxiv.org/abs/1702.08658v1), which explains in part why the blurry images above occur. I had originally believed it was a result of using an inappropriate loss function, but it turns out that this is actually caused by the decoder using the average reconstruction. In our notation, this means the decoder is learning to return $\\mathbb{E}_{p(x|z)}[x]$. The paper also discuss how to avoid blurry images.\n",
    "* [Improved Variational Inference with Inverse Autoregressive Flow](https://arxiv.org/abs/1606.04934v2), which describes how to encode to non-diagonal covariance matrices $\\Sigma(x)$ in a computationally straight-forward way. Another approach based on this paper is offered in [convex combination linear Inverse Autoregressive Flow](https://arxiv.org/abs/1706.0232), which is more easily applied.\n",
    "* It's also worth pointing out that you can use *several* latent variables. There are two variants I'm aware of for this: the Ladder Variational Autoencoder and the Variational Ladder Autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Comments\n",
    "\n",
    "I made this notebook to have an easy way of sharing my interest in VAEs with others. If you find an error or have any suggestions, please let me know."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
