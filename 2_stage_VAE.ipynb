{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Stage VAE\n",
    "\n",
    "An article from NeurIPS 2018 explained a way of altering Variational Autoencoders that leads to better image results. This is measured against the tensorflow implementation of FID, which is used to determine image diversity (i.e. how many different images the generator can produce) and image quality (i.e. how similar the generated images are to real images, as measured by a particular network). \n",
    "\n",
    "In brief, the alteration is to change how we measure the reconstruction. A typical measure of the reconstruction is the log probability of the outcome. Given an observation $x$, \n",
    "\n",
    "\n",
    "This shortcoming is that the network is more apt to learn the *shape* of the manifold rather than the *distribution of data* on the manifold. Because of this, a standard VAE finds itself producing blurry pictures due to this mismatch in distribution. To remedy this, they propose a simple fix: add a second VAE to learn the distribution of data, once the shape of the manifold has been reasonably learned.\n",
    "\n",
    "This notebook is an implementation of this idea in pytorch. The network used herein is much smaller, but the result should still be observable with a small network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor, Pad\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "from math import sqrt, exp, log, pi\n",
    "\n",
    "from matplotlib.pylab import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose training device (cpu vs gpu/cuda) based on availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class image_loader:\n",
    "    def __init__(self, dataset, colorize=False):\n",
    "        '''\n",
    "        Utility class used to dispose of image labels.\n",
    "        :param dataset: ([Tensor, label]) List of tensors\n",
    "            and labels.\n",
    "        :param colorize: (Bool, optional) Adds random\n",
    "            color if true. Intended for single channel\n",
    "            images.\n",
    "        '''\n",
    "        self.dataset = dataset\n",
    "        self.colorize = colorize\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Returns the image corresponding to index idx. If\n",
    "        the colorize attribute is True, the image is then\n",
    "        randomly colorized. If color is not none, then\n",
    "        that color is used.\n",
    "        '''\n",
    "        \n",
    "        x = self.dataset[idx][0]\n",
    "        if self.colorize:\n",
    "            x = x*torch.rand((3, 1, 1), device=x.device)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def color(self, idx, color):\n",
    "        if type(color) == str:\n",
    "            color = self.hex_to_tensor(color)\n",
    "        x = self.dataset[idx][0]\n",
    "        color = color.reshape((3, 1, 1)).to(device=x.device)\n",
    "        return x*color\n",
    "\n",
    "    @staticmethod\n",
    "    def hex_to_tensor(hex_color):\n",
    "        assert hex_color[0] == '#'\n",
    "        rgb = [int(hex_color[1+2*i:3+2*i], 16)/255. for i in range(3)]\n",
    "        return torch.tensor(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose data set and batch size\n",
    "transforms = Compose([Pad(2), ToTensor()])\n",
    "dataset = datasets.FashionMNIST(root='./data/', train=True, download=False, transform=transforms)\n",
    "image_set = image_loader(dataset, colorize=True)\n",
    "batch_size = 125\n",
    "training_loader = torch.utils.data.DataLoader(dataset=image_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Set image size. This is done for convenience when changing data.\n",
    "image_shape = (3, 32, 32)\n",
    "image_size = image_shape[0]*image_shape[1]*image_shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ce604358c8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(to_pil_image(image_set[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        self.mean = nn.Linear(in_features, out_features, bias=bias)\n",
    "        self.log_var = nn.Linear(in_features, out_features, bias=bias)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        mean = self.mean(input)\n",
    "        scale = (self.log_var(input)/2).exp()\n",
    "        return mean, scale\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'VariationalEncoder(in_features={0}, out_features={1}, bias={2})'.format(\n",
    "            self.mean.in_features,\n",
    "            self.mean.out_features,\n",
    "            self.mean.bias is not None\n",
    "        )\n",
    "    \n",
    "class ReshapeLayer(nn.Module):\n",
    "    '''\n",
    "    Reshapes the input to be the output using view. Shapes are checked\n",
    "    on forward pass to verify that they are compatible.\n",
    "\n",
    "    :param view_shape: The shape to cast the input to. Given a batch\n",
    "        input of shape (n, _) will be cast to (n, view_shape).\n",
    "    '''\n",
    "    def __init__(self, out_shape):\n",
    "        super(ReshapeLayer, self).__init__()\n",
    "        self.out_shape = out_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Reshapes x to initialized shape.\n",
    "        :param x: (torch.Tensor)\n",
    "        :return: (torch.Tensor)\n",
    "        '''\n",
    "        output_shape = (x.shape[0],) + self.out_shape\n",
    "        assert self.dimension(x.shape) == self.dimension(output_shape), \\\n",
    "            '{0} and {1} are not compatabile'.format(x.shape, output_shape)\n",
    "        return x.reshape(output_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def dimension(shape):\n",
    "        #Helper function for checking dimensions\n",
    "        out = 1\n",
    "        for s in shape:\n",
    "            out *= s\n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'ReshapeLayer(out_shape={0})'.format(self.out_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class learned_log_prob(nn.Module):\n",
    "    def __init__(self, log_gamma=0.):\n",
    "        super(learned_log_prob, self).__init__()\n",
    "        self.log_gamma = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        log_prob = torch.sum(torch.pow((input.flatten(1)-target.flatten(1))/self.log_gamma.exp(), 2)/2 \\\n",
    "                         + self.log_gamma \\\n",
    "                         + log(2*pi)/2, 1)\n",
    "        return log_prob\n",
    "    \n",
    "    @property\n",
    "    def gamma(self):\n",
    "        return self.log_gamma.exp()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'learned_log_prob(gamma={0:.3f})'.format(float(self.gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(mean, scale):\n",
    "        '''\n",
    "        Computes the KL divergence of a normal distribution with the parameters above\n",
    "        from N(0, 1) on a batch and then returns the mean.\n",
    "\n",
    "        :param means: (torch.Tensor)\n",
    "        :param scale: (torch.Tensor)\n",
    "        :return: The KL divergence\n",
    "        '''\n",
    "        variance = scale.pow(2)\n",
    "        loss = torch.sum(variance+mean.pow(2)-torch.log(variance)-1, 1)/2.\n",
    "        #Return batch average\n",
    "        return loss.mean()\n",
    "\n",
    "def MSE(input, target):\n",
    "        '''\n",
    "        Computes mean squared error of a sample.\n",
    "        \n",
    "        :param x: (torch.Tensor)\n",
    "        :param y: (torch.Tensor)\n",
    "        :return: (torch.Tensor)\n",
    "        '''\n",
    "        loss = torch.sum(torch.pow(input.flatten(1)-target.flatten(1), 2)/2, 1)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(intro_dict, progress_dict):\n",
    "    '''\n",
    "    Prints information from two dictionaries. The first dictionary is printed verbatim,\n",
    "    while the second dictionary is rounded to 4 decimals. The values of the second\n",
    "    dictionary must be floats.\n",
    "    \n",
    "    :param intro_dict: (dict) Information about the current time. The {key: value}\n",
    "        pair can have any value that can be used in a str formatorch.\n",
    "    :param progress_dict: (dict) Information about current progress. The {key: value}\n",
    "        pair must have a float for the value.\n",
    "    '''\n",
    "    intro_text = '{}: {:4}'\n",
    "    intros = [intro_text.format(key, value) for key, value in intro_dict.items()]\n",
    "    update_text = '{}: {:.4f}'\n",
    "    updates = [update_text.format(key, value) for key, value in progress_dict.items()]\n",
    "    print(*intros, *updates, sep=' - ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train first variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_VAE(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    learned_log_prob,\n",
    "    optimizer,\n",
    "    optim_schedule,\n",
    "    training_loader,\n",
    "    epochs,\n",
    "    device='cpu',\n",
    "):\n",
    "    print('Training network.')\n",
    "    #Train big network\n",
    "    batches = len(training_loader)\n",
    "    for epoch in range(epochs):\n",
    "        running_MSE = 0.\n",
    "        running_z = 0.\n",
    "        running_log_prob = 0.\n",
    "        for batch_nb, x in enumerate(training_loader):\n",
    "            x = x.to(device)\n",
    "\n",
    "            #Encode to z\n",
    "            means, scales = encoder(x)\n",
    "            eps = torch.randn_like(scales, device=device)\n",
    "            z = eps*scales+means\n",
    "\n",
    "            #Decode to image\n",
    "            x_hat = decoder(z)\n",
    "\n",
    "            #Compute loss\n",
    "            z_loss = KL(means, scales)\n",
    "\n",
    "            log_loss = torch.mean(learned_log_prob(x, x_hat))\n",
    "            loss = z_loss + log_loss\n",
    "\n",
    "            #Track statistics. Use float to detach the graph from the tensors\n",
    "            running_log_prob = 0.9*running_log_prob + 0.1*float(log_loss)\n",
    "            running_z = 0.9*running_z + 0.1*float(z_loss)\n",
    "            running_MSE = 0.9*running_MSE + 0.1*float(MSE(x, x_hat))\n",
    "\n",
    "            #Update network\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if batch_nb % 50 == 49:\n",
    "                #Print progress\n",
    "                info_dict = {\n",
    "                    'Epoch': epoch,\n",
    "                    'Batch': batch_nb+1\n",
    "                }\n",
    "                progress_dict = {\n",
    "                    'log prob': running_log_prob,\n",
    "                    'Encoding loss': running_z,\n",
    "                    'MSE': running_MSE,\n",
    "                    'gamma': float(learned_log_prob.gamma)\n",
    "                }\n",
    "                print_progress(info_dict, progress_dict)\n",
    "\n",
    "        #Update learning rate\n",
    "        optim_schedule.step()\n",
    "\n",
    "    print('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(96, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(96, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (5): ReLU()\n",
      "  (6): Flatten()\n",
      "  (7): Linear(in_features=1536, out_features=128, bias=True)\n",
      "  (8): ReLU()\n",
      "  (9): VariationalEncoder(in_features=128, out_features=32, bias=False)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=1536, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): ReshapeLayer(out_shape=(96, 4, 4))\n",
      "  (5): ReLU()\n",
      "  (6): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (7): ReLU()\n",
      "  (8): ConvTranspose2d(96, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (9): ReLU()\n",
      "  (10): ConvTranspose2d(96, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (11): Sigmoid()\n",
      ")\n",
      "learned_log_prob(gamma=1.000)\n"
     ]
    }
   ],
   "source": [
    "z_size = 32\n",
    "nc = image_shape[0]\n",
    "filters = 32*nc\n",
    "\n",
    "#Build networks\n",
    "big_encoder = nn.Sequential(\n",
    "    #current shape: 3 x 32 x 32\n",
    "    nn.Conv2d(nc, filters, 4, 2, 1),\n",
    "    nn.ReLU(),\n",
    "    #current shape: filters x 16 x 16\n",
    "    nn.Conv2d(filters, filters, 4, 2, 1),\n",
    "    nn.ReLU(),\n",
    "    #current shape: filters x 8 x 8\n",
    "    nn.Conv2d(filters, filters, 4, 2, 1),\n",
    "    nn.ReLU(),\n",
    "    #current shape: filters x 4 x 4\n",
    "    nn.Flatten(),\n",
    "    #current shape: 16*filters\n",
    "    nn.Linear(16*filters, 4*z_size, bias=True),\n",
    "    nn.ReLU(),\n",
    "    VariationalEncoder(4*z_size, z_size, bias=False)\n",
    ")\n",
    "\n",
    "big_decoder = nn.Sequential(\n",
    "    nn.Linear(z_size, 4*z_size, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4*z_size, 16*filters, bias=True),\n",
    "    nn.ReLU(),\n",
    "    ReshapeLayer((filters, 4, 4)),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(filters, filters, 4, 2, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(filters, filters, 4, 2, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.ConvTranspose2d(filters, nc, 4, 2, 1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "big_log_prob = learned_log_prob()\n",
    "\n",
    "#Load previously trained models, if they exist. Passes if they do not.\n",
    "try:\n",
    "    big_encoder.load_state_dict(torch.load('./samples/2layer/big_encoder.pkl'))\n",
    "    big_decoder.load_state_dict(torch.load('./samples/2layer/big_decoder.pkl'))\n",
    "    big_log_prob.load_state_dict(torch.load('./samples/2layer/big_log_prob.pkl'))\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "#Move networks to selected device\n",
    "big_encoder.to(device)\n",
    "big_decoder.to(device)\n",
    "big_log_prob.to(device)\n",
    "\n",
    "print(big_encoder)\n",
    "print(big_decoder)\n",
    "print(big_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect parameters\n",
    "big_params = list(big_encoder.parameters())+list(big_decoder.parameters())+list(big_log_prob.parameters())\n",
    "\n",
    "#Set up optimizer\n",
    "lr = 1e-4\n",
    "big_optimizer = torch.optim.Adam(big_params, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network.\n",
      "Epoch:    0 - Batch:   50 - log prob: 2942.2979 - Encoding loss: 8.1130 - MSE: 145.2013 - gamma: 0.9949\n",
      "Epoch:    0 - Batch:  100 - log prob: 2872.4262 - Encoding loss: 2.2085 - MSE: 76.7127 - gamma: 0.9897\n",
      "Epoch:    0 - Batch:  150 - log prob: 2831.1643 - Encoding loss: 2.4851 - MSE: 51.4374 - gamma: 0.9845\n",
      "Epoch:    0 - Batch:  200 - log prob: 2810.1523 - Encoding loss: 1.9568 - MSE: 46.0864 - gamma: 0.9794\n",
      "Epoch:    0 - Batch:  250 - log prob: 2792.0700 - Encoding loss: 1.9264 - MSE: 43.5056 - gamma: 0.9744\n",
      "Epoch:    0 - Batch:  300 - log prob: 2774.8160 - Encoding loss: 1.9674 - MSE: 41.6611 - gamma: 0.9694\n",
      "Epoch:    0 - Batch:  350 - log prob: 2758.0451 - Encoding loss: 2.1502 - MSE: 40.2457 - gamma: 0.9644\n",
      "Epoch:    0 - Batch:  400 - log prob: 2740.8635 - Encoding loss: 2.6116 - MSE: 38.4383 - gamma: 0.9595\n",
      "Epoch:    0 - Batch:  450 - log prob: 2723.2536 - Encoding loss: 2.9905 - MSE: 36.2459 - gamma: 0.9547\n",
      "Epoch:    1 - Batch:   50 - log prob: 2684.6266 - Encoding loss: 3.2435 - MSE: 35.7593 - gamma: 0.9469\n",
      "Epoch:    1 - Batch:  100 - log prob: 2681.3471 - Encoding loss: 3.2724 - MSE: 34.1387 - gamma: 0.9422\n",
      "Epoch:    1 - Batch:  150 - log prob: 2666.0753 - Encoding loss: 3.3374 - MSE: 33.9800 - gamma: 0.9374\n",
      "Epoch:    1 - Batch:  200 - log prob: 2649.8422 - Encoding loss: 3.2349 - MSE: 33.0281 - gamma: 0.9327\n",
      "Epoch:    1 - Batch:  250 - log prob: 2635.1338 - Encoding loss: 3.4844 - MSE: 33.3967 - gamma: 0.9280\n",
      "Epoch:    1 - Batch:  300 - log prob: 2620.4157 - Encoding loss: 3.6133 - MSE: 33.7304 - gamma: 0.9233\n",
      "Epoch:    1 - Batch:  350 - log prob: 2602.8397 - Encoding loss: 3.6223 - MSE: 31.6303 - gamma: 0.9187\n",
      "Epoch:    1 - Batch:  400 - log prob: 2588.3973 - Encoding loss: 3.9242 - MSE: 32.1827 - gamma: 0.9140\n",
      "Epoch:    1 - Batch:  450 - log prob: 2572.0123 - Encoding loss: 4.0710 - MSE: 31.1044 - gamma: 0.9094\n",
      "Epoch:    2 - Batch:   50 - log prob: 2533.3803 - Encoding loss: 4.1664 - MSE: 29.8974 - gamma: 0.9021\n",
      "Epoch:    2 - Batch:  100 - log prob: 2531.0549 - Encoding loss: 4.4664 - MSE: 29.7495 - gamma: 0.8976\n",
      "Epoch:    2 - Batch:  150 - log prob: 2515.5948 - Encoding loss: 4.6842 - MSE: 29.3920 - gamma: 0.8931\n",
      "Epoch:    2 - Batch:  200 - log prob: 2500.4555 - Encoding loss: 4.6933 - MSE: 29.3418 - gamma: 0.8886\n",
      "Epoch:    2 - Batch:  250 - log prob: 2484.9720 - Encoding loss: 4.8353 - MSE: 29.0129 - gamma: 0.8842\n",
      "Epoch:    2 - Batch:  300 - log prob: 2469.0136 - Encoding loss: 4.8368 - MSE: 28.3179 - gamma: 0.8798\n",
      "Epoch:    2 - Batch:  350 - log prob: 2453.9193 - Encoding loss: 5.0947 - MSE: 28.2922 - gamma: 0.8753\n",
      "Epoch:    2 - Batch:  400 - log prob: 2438.1901 - Encoding loss: 5.0314 - MSE: 27.7787 - gamma: 0.8710\n",
      "Epoch:    2 - Batch:  450 - log prob: 2422.9809 - Encoding loss: 5.1041 - MSE: 27.6599 - gamma: 0.8666\n",
      "Epoch:    3 - Batch:   50 - log prob: 2386.0273 - Encoding loss: 5.1659 - MSE: 27.1898 - gamma: 0.8597\n",
      "Epoch:    3 - Batch:  100 - log prob: 2382.9519 - Encoding loss: 5.1738 - MSE: 27.0284 - gamma: 0.8554\n",
      "Epoch:    3 - Batch:  150 - log prob: 2368.0122 - Encoding loss: 5.4747 - MSE: 27.0502 - gamma: 0.8511\n",
      "Epoch:    3 - Batch:  200 - log prob: 2353.0045 - Encoding loss: 5.3775 - MSE: 27.0617 - gamma: 0.8468\n",
      "Epoch:    3 - Batch:  250 - log prob: 2337.2175 - Encoding loss: 5.3584 - MSE: 26.5163 - gamma: 0.8426\n",
      "Epoch:    3 - Batch:  300 - log prob: 2323.0036 - Encoding loss: 5.4535 - MSE: 27.0834 - gamma: 0.8384\n",
      "Epoch:    3 - Batch:  350 - log prob: 2305.7357 - Encoding loss: 5.4445 - MSE: 25.5064 - gamma: 0.8342\n",
      "Epoch:    3 - Batch:  400 - log prob: 2291.9490 - Encoding loss: 5.6007 - MSE: 26.3590 - gamma: 0.8300\n",
      "Epoch:    3 - Batch:  450 - log prob: 2276.3002 - Encoding loss: 5.7115 - MSE: 25.9157 - gamma: 0.8259\n",
      "Epoch:    4 - Batch:   50 - log prob: 2239.1262 - Encoding loss: 5.5973 - MSE: 24.7847 - gamma: 0.8193\n",
      "Epoch:    4 - Batch:  100 - log prob: 2235.9066 - Encoding loss: 5.7253 - MSE: 25.0258 - gamma: 0.8152\n",
      "Epoch:    4 - Batch:  150 - log prob: 2220.8991 - Encoding loss: 5.7699 - MSE: 24.9830 - gamma: 0.8111\n",
      "Epoch:    4 - Batch:  200 - log prob: 2206.2333 - Encoding loss: 5.9061 - MSE: 25.1930 - gamma: 0.8071\n",
      "Epoch:    4 - Batch:  250 - log prob: 2190.7392 - Encoding loss: 5.7383 - MSE: 24.8586 - gamma: 0.8031\n",
      "Epoch:    4 - Batch:  300 - log prob: 2176.1786 - Encoding loss: 5.7587 - MSE: 25.1251 - gamma: 0.7991\n",
      "Epoch:    4 - Batch:  350 - log prob: 2161.8904 - Encoding loss: 5.9975 - MSE: 25.5559 - gamma: 0.7951\n",
      "Epoch:    4 - Batch:  400 - log prob: 2145.7797 - Encoding loss: 6.0089 - MSE: 24.8308 - gamma: 0.7911\n",
      "Epoch:    4 - Batch:  450 - log prob: 2129.7484 - Encoding loss: 5.9259 - MSE: 24.1680 - gamma: 0.7872\n",
      "Epoch:    5 - Batch:   50 - log prob: 2095.4982 - Encoding loss: 5.8974 - MSE: 24.4235 - gamma: 0.7809\n",
      "Epoch:    5 - Batch:  100 - log prob: 2091.9249 - Encoding loss: 6.1486 - MSE: 24.8518 - gamma: 0.7770\n",
      "Epoch:    5 - Batch:  150 - log prob: 2075.8909 - Encoding loss: 6.0558 - MSE: 24.1617 - gamma: 0.7731\n",
      "Epoch:    5 - Batch:  200 - log prob: 2061.5099 - Encoding loss: 6.1437 - MSE: 24.4955 - gamma: 0.7693\n",
      "Epoch:    5 - Batch:  250 - log prob: 2045.8140 - Encoding loss: 6.3527 - MSE: 24.0491 - gamma: 0.7654\n",
      "Epoch:    5 - Batch:  300 - log prob: 2030.7669 - Encoding loss: 6.1924 - MSE: 23.9850 - gamma: 0.7616\n",
      "Epoch:    5 - Batch:  350 - log prob: 2015.9387 - Encoding loss: 6.4881 - MSE: 24.0421 - gamma: 0.7578\n",
      "Epoch:    5 - Batch:  400 - log prob: 2000.1708 - Encoding loss: 6.6694 - MSE: 23.5615 - gamma: 0.7540\n",
      "Epoch:    5 - Batch:  450 - log prob: 1983.4314 - Encoding loss: 7.0134 - MSE: 22.5446 - gamma: 0.7503\n",
      "Epoch:    6 - Batch:   50 - log prob: 1945.6759 - Encoding loss: 7.4077 - MSE: 20.4116 - gamma: 0.7443\n",
      "Epoch:    6 - Batch:  100 - log prob: 1940.6735 - Encoding loss: 7.5902 - MSE: 20.4645 - gamma: 0.7406\n",
      "Epoch:    6 - Batch:  150 - log prob: 1924.3174 - Encoding loss: 7.7423 - MSE: 19.7077 - gamma: 0.7369\n",
      "Epoch:    6 - Batch:  200 - log prob: 1909.4110 - Encoding loss: 7.9879 - MSE: 19.7729 - gamma: 0.7332\n",
      "Epoch:    6 - Batch:  250 - log prob: 1892.7885 - Encoding loss: 8.1215 - MSE: 18.9231 - gamma: 0.7295\n",
      "Epoch:    6 - Batch:  300 - log prob: 1877.7197 - Encoding loss: 8.2426 - MSE: 18.9067 - gamma: 0.7259\n",
      "Epoch:    6 - Batch:  350 - log prob: 1862.4887 - Encoding loss: 8.2850 - MSE: 18.8025 - gamma: 0.7223\n",
      "Epoch:    6 - Batch:  400 - log prob: 1847.2333 - Encoding loss: 8.3366 - MSE: 18.6850 - gamma: 0.7186\n",
      "Epoch:    6 - Batch:  450 - log prob: 1831.2354 - Encoding loss: 8.2290 - MSE: 18.1865 - gamma: 0.7151\n",
      "Epoch:    7 - Batch:   50 - log prob: 1798.7515 - Encoding loss: 8.1890 - MSE: 18.5713 - gamma: 0.7094\n",
      "Epoch:    7 - Batch:  100 - log prob: 1792.2562 - Encoding loss: 8.2579 - MSE: 18.2491 - gamma: 0.7058\n",
      "Epoch:    7 - Batch:  150 - log prob: 1776.0215 - Encoding loss: 8.2568 - MSE: 17.6209 - gamma: 0.7023\n",
      "Epoch:    7 - Batch:  200 - log prob: 1761.3622 - Encoding loss: 8.4298 - MSE: 17.7970 - gamma: 0.6988\n",
      "Epoch:    7 - Batch:  250 - log prob: 1746.3957 - Encoding loss: 8.4360 - MSE: 17.8172 - gamma: 0.6953\n",
      "Epoch:    7 - Batch:  300 - log prob: 1731.5587 - Encoding loss: 8.4966 - MSE: 17.8958 - gamma: 0.6918\n",
      "Epoch:    7 - Batch:  350 - log prob: 1715.7139 - Encoding loss: 8.4750 - MSE: 17.4922 - gamma: 0.6884\n",
      "Epoch:    7 - Batch:  400 - log prob: 1701.1917 - Encoding loss: 8.3984 - MSE: 17.7164 - gamma: 0.6849\n",
      "Epoch:    7 - Batch:  450 - log prob: 1686.1744 - Encoding loss: 8.4341 - MSE: 17.7031 - gamma: 0.6815\n",
      "Epoch:    8 - Batch:   50 - log prob: 1652.6265 - Encoding loss: 8.4051 - MSE: 17.1851 - gamma: 0.6761\n",
      "Epoch:    8 - Batch:  100 - log prob: 1646.3476 - Encoding loss: 8.5493 - MSE: 17.3219 - gamma: 0.6727\n",
      "Epoch:    8 - Batch:  150 - log prob: 1631.5649 - Encoding loss: 8.8340 - MSE: 17.3877 - gamma: 0.6694\n",
      "Epoch:    8 - Batch:  200 - log prob: 1616.3357 - Encoding loss: 8.6154 - MSE: 17.2691 - gamma: 0.6660\n",
      "Epoch:    8 - Batch:  250 - log prob: 1600.3816 - Encoding loss: 8.6379 - MSE: 16.8321 - gamma: 0.6627\n",
      "Epoch:    8 - Batch:  300 - log prob: 1586.6224 - Encoding loss: 8.7421 - MSE: 17.3584 - gamma: 0.6594\n",
      "Epoch:    8 - Batch:  350 - log prob: 1571.3891 - Encoding loss: 8.7838 - MSE: 17.2348 - gamma: 0.6561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    8 - Batch:  400 - log prob: 1557.0277 - Encoding loss: 8.9050 - MSE: 17.4830 - gamma: 0.6528\n",
      "Epoch:    8 - Batch:  450 - log prob: 1541.4465 - Encoding loss: 8.7616 - MSE: 17.2061 - gamma: 0.6496\n",
      "Epoch:    9 - Batch:   50 - log prob: 1508.9087 - Encoding loss: 8.7831 - MSE: 16.8063 - gamma: 0.6444\n",
      "Epoch:    9 - Batch:  100 - log prob: 1500.6530 - Encoding loss: 8.8357 - MSE: 16.4094 - gamma: 0.6412\n",
      "Epoch:    9 - Batch:  150 - log prob: 1486.3589 - Encoding loss: 8.9287 - MSE: 16.6580 - gamma: 0.6380\n",
      "Epoch:    9 - Batch:  200 - log prob: 1471.4947 - Encoding loss: 8.9740 - MSE: 16.6839 - gamma: 0.6348\n",
      "Epoch:    9 - Batch:  250 - log prob: 1456.6654 - Encoding loss: 9.0207 - MSE: 16.7206 - gamma: 0.6317\n",
      "Epoch:    9 - Batch:  300 - log prob: 1440.1135 - Encoding loss: 9.1335 - MSE: 16.0738 - gamma: 0.6285\n",
      "Epoch:    9 - Batch:  350 - log prob: 1425.8093 - Encoding loss: 9.1769 - MSE: 16.3185 - gamma: 0.6254\n",
      "Epoch:    9 - Batch:  400 - log prob: 1411.1913 - Encoding loss: 9.0587 - MSE: 16.4319 - gamma: 0.6223\n",
      "Epoch:    9 - Batch:  450 - log prob: 1396.2089 - Encoding loss: 9.1807 - MSE: 16.4017 - gamma: 0.6192\n",
      "Epoch:   10 - Batch:   50 - log prob: 1364.3805 - Encoding loss: 9.2938 - MSE: 15.9993 - gamma: 0.6143\n",
      "Epoch:   10 - Batch:  100 - log prob: 1356.9686 - Encoding loss: 9.2511 - MSE: 16.2246 - gamma: 0.6112\n",
      "Epoch:   10 - Batch:  150 - log prob: 1340.8975 - Encoding loss: 9.3473 - MSE: 15.7755 - gamma: 0.6082\n",
      "Epoch:   10 - Batch:  200 - log prob: 1325.3277 - Encoding loss: 9.3162 - MSE: 15.5288 - gamma: 0.6051\n",
      "Epoch:   10 - Batch:  250 - log prob: 1310.5918 - Encoding loss: 9.4130 - MSE: 15.5894 - gamma: 0.6021\n",
      "Epoch:   10 - Batch:  300 - log prob: 1296.7479 - Encoding loss: 9.5938 - MSE: 15.9667 - gamma: 0.5991\n",
      "Epoch:   10 - Batch:  350 - log prob: 1280.6690 - Encoding loss: 9.4809 - MSE: 15.5383 - gamma: 0.5962\n",
      "Epoch:   10 - Batch:  400 - log prob: 1266.5437 - Encoding loss: 9.6297 - MSE: 15.8057 - gamma: 0.5932\n",
      "Epoch:   10 - Batch:  450 - log prob: 1251.8877 - Encoding loss: 9.7308 - MSE: 15.8813 - gamma: 0.5902\n",
      "Epoch:   11 - Batch:   50 - log prob: 1220.5237 - Encoding loss: 9.6214 - MSE: 15.3952 - gamma: 0.5855\n",
      "Epoch:   11 - Batch:  100 - log prob: 1210.9128 - Encoding loss: 9.7324 - MSE: 15.0973 - gamma: 0.5826\n",
      "Epoch:   11 - Batch:  150 - log prob: 1196.7951 - Encoding loss: 9.8958 - MSE: 15.3424 - gamma: 0.5797\n",
      "Epoch:   11 - Batch:  200 - log prob: 1181.9957 - Encoding loss: 9.7005 - MSE: 15.3651 - gamma: 0.5768\n",
      "Epoch:   11 - Batch:  250 - log prob: 1167.4163 - Encoding loss: 9.9434 - MSE: 15.4567 - gamma: 0.5740\n",
      "Epoch:   11 - Batch:  300 - log prob: 1149.4594 - Encoding loss: 10.0352 - MSE: 14.4439 - gamma: 0.5711\n",
      "Epoch:   11 - Batch:  350 - log prob: 1135.0055 - Encoding loss: 10.1171 - MSE: 14.5828 - gamma: 0.5683\n",
      "Epoch:   11 - Batch:  400 - log prob: 1118.6632 - Encoding loss: 10.2592 - MSE: 14.1138 - gamma: 0.5655\n",
      "Epoch:   11 - Batch:  450 - log prob: 1103.3178 - Encoding loss: 10.3908 - MSE: 13.9717 - gamma: 0.5626\n",
      "Epoch:   12 - Batch:   50 - log prob: 1071.9012 - Encoding loss: 10.5925 - MSE: 13.2984 - gamma: 0.5582\n",
      "Epoch:   12 - Batch:  100 - log prob: 1062.1177 - Encoding loss: 11.0546 - MSE: 13.2240 - gamma: 0.5554\n",
      "Epoch:   12 - Batch:  150 - log prob: 1045.7866 - Encoding loss: 11.3192 - MSE: 12.7858 - gamma: 0.5526\n",
      "Epoch:   12 - Batch:  200 - log prob: 1029.2154 - Encoding loss: 11.8838 - MSE: 12.2962 - gamma: 0.5498\n",
      "Epoch:   12 - Batch:  250 - log prob: 1011.3410 - Encoding loss: 12.0670 - MSE: 11.4297 - gamma: 0.5471\n",
      "Epoch:   12 - Batch:  300 - log prob: 995.5036 - Encoding loss: 12.3179 - MSE: 11.1871 - gamma: 0.5444\n",
      "Epoch:   12 - Batch:  350 - log prob: 980.1151 - Encoding loss: 12.5092 - MSE: 11.0796 - gamma: 0.5416\n",
      "Epoch:   12 - Batch:  400 - log prob: 964.6232 - Encoding loss: 12.4135 - MSE: 10.9427 - gamma: 0.5389\n",
      "Epoch:   12 - Batch:  450 - log prob: 949.9725 - Encoding loss: 12.3046 - MSE: 11.0506 - gamma: 0.5362\n",
      "Epoch:   13 - Batch:   50 - log prob: 920.4065 - Encoding loss: 12.5991 - MSE: 10.7943 - gamma: 0.5319\n",
      "Epoch:   13 - Batch:  100 - log prob: 910.0018 - Encoding loss: 12.5717 - MSE: 10.7888 - gamma: 0.5293\n",
      "Epoch:   13 - Batch:  150 - log prob: 893.6093 - Encoding loss: 12.7094 - MSE: 10.3968 - gamma: 0.5266\n",
      "Epoch:   13 - Batch:  200 - log prob: 879.9863 - Encoding loss: 13.0092 - MSE: 10.7783 - gamma: 0.5240\n",
      "Epoch:   13 - Batch:  250 - log prob: 863.8661 - Encoding loss: 12.7818 - MSE: 10.4711 - gamma: 0.5214\n",
      "Epoch:   13 - Batch:  300 - log prob: 848.9723 - Encoding loss: 12.9465 - MSE: 10.4988 - gamma: 0.5188\n",
      "Epoch:   13 - Batch:  350 - log prob: 833.3709 - Encoding loss: 13.0089 - MSE: 10.3357 - gamma: 0.5162\n",
      "Epoch:   13 - Batch:  400 - log prob: 819.1515 - Encoding loss: 13.1627 - MSE: 10.5382 - gamma: 0.5136\n",
      "Epoch:   13 - Batch:  450 - log prob: 802.4335 - Encoding loss: 13.1915 - MSE: 10.0807 - gamma: 0.5111\n",
      "Epoch:   14 - Batch:   50 - log prob: 775.7464 - Encoding loss: 13.2937 - MSE: 10.3821 - gamma: 0.5070\n",
      "Epoch:   14 - Batch:  100 - log prob: 762.6555 - Encoding loss: 13.0700 - MSE: 9.8739 - gamma: 0.5045\n",
      "Epoch:   14 - Batch:  150 - log prob: 747.8285 - Encoding loss: 13.1199 - MSE: 9.9076 - gamma: 0.5019\n",
      "Epoch:   14 - Batch:  200 - log prob: 731.7578 - Encoding loss: 13.0408 - MSE: 9.6341 - gamma: 0.4994\n",
      "Epoch:   14 - Batch:  250 - log prob: 716.9647 - Encoding loss: 13.1173 - MSE: 9.6801 - gamma: 0.4969\n",
      "Epoch:   14 - Batch:  300 - log prob: 702.4211 - Encoding loss: 13.2435 - MSE: 9.7848 - gamma: 0.4945\n",
      "Epoch:   14 - Batch:  350 - log prob: 687.2524 - Encoding loss: 13.3654 - MSE: 9.7335 - gamma: 0.4920\n",
      "Epoch:   14 - Batch:  400 - log prob: 671.3603 - Encoding loss: 13.3526 - MSE: 9.5089 - gamma: 0.4895\n",
      "Epoch:   14 - Batch:  450 - log prob: 657.2726 - Encoding loss: 13.3234 - MSE: 9.7163 - gamma: 0.4871\n",
      "Epoch:   15 - Batch:   50 - log prob: 630.2739 - Encoding loss: 13.4289 - MSE: 9.7251 - gamma: 0.4832\n",
      "Epoch:   15 - Batch:  100 - log prob: 617.6002 - Encoding loss: 13.4988 - MSE: 9.5281 - gamma: 0.4808\n",
      "Epoch:   15 - Batch:  150 - log prob: 602.4639 - Encoding loss: 13.4387 - MSE: 9.4782 - gamma: 0.4784\n",
      "Epoch:   15 - Batch:  200 - log prob: 586.7381 - Encoding loss: 13.6068 - MSE: 9.2984 - gamma: 0.4760\n",
      "Epoch:   15 - Batch:  250 - log prob: 572.0537 - Encoding loss: 13.6018 - MSE: 9.3553 - gamma: 0.4737\n",
      "Epoch:   15 - Batch:  300 - log prob: 556.8573 - Encoding loss: 13.6854 - MSE: 9.2956 - gamma: 0.4713\n",
      "Epoch:   15 - Batch:  350 - log prob: 541.6411 - Encoding loss: 13.5814 - MSE: 9.2311 - gamma: 0.4689\n",
      "Epoch:   15 - Batch:  400 - log prob: 527.6664 - Encoding loss: 13.8072 - MSE: 9.4382 - gamma: 0.4666\n",
      "Epoch:   15 - Batch:  450 - log prob: 511.7240 - Encoding loss: 13.8729 - MSE: 9.2147 - gamma: 0.4643\n",
      "Epoch:   16 - Batch:   50 - log prob: 484.7377 - Encoding loss: 13.8109 - MSE: 9.0504 - gamma: 0.4606\n",
      "Epoch:   16 - Batch:  100 - log prob: 474.0559 - Encoding loss: 13.9862 - MSE: 9.4440 - gamma: 0.4583\n",
      "Epoch:   16 - Batch:  150 - log prob: 457.6873 - Encoding loss: 13.9124 - MSE: 9.1319 - gamma: 0.4560\n",
      "Epoch:   16 - Batch:  200 - log prob: 442.4944 - Encoding loss: 14.0748 - MSE: 9.0697 - gamma: 0.4538\n",
      "Epoch:   16 - Batch:  250 - log prob: 428.0312 - Encoding loss: 14.0321 - MSE: 9.1572 - gamma: 0.4515\n",
      "Epoch:   16 - Batch:  300 - log prob: 414.1655 - Encoding loss: 14.3765 - MSE: 9.3611 - gamma: 0.4492\n",
      "Epoch:   16 - Batch:  350 - log prob: 397.1047 - Encoding loss: 14.0823 - MSE: 8.9207 - gamma: 0.4470\n",
      "Epoch:   16 - Batch:  400 - log prob: 383.0110 - Encoding loss: 14.3718 - MSE: 9.0766 - gamma: 0.4448\n",
      "Epoch:   16 - Batch:  450 - log prob: 368.3446 - Encoding loss: 14.4500 - MSE: 9.1156 - gamma: 0.4426\n",
      "Epoch:   17 - Batch:   50 - log prob: 340.3738 - Encoding loss: 14.2638 - MSE: 8.6147 - gamma: 0.4391\n",
      "Epoch:   17 - Batch:  100 - log prob: 327.9936 - Encoding loss: 14.5340 - MSE: 8.7868 - gamma: 0.4369\n",
      "Epoch:   17 - Batch:  150 - log prob: 314.5085 - Encoding loss: 14.7380 - MSE: 9.0453 - gamma: 0.4347\n",
      "Epoch:   17 - Batch:  200 - log prob: 298.6181 - Encoding loss: 14.6778 - MSE: 8.8475 - gamma: 0.4325\n",
      "Epoch:   17 - Batch:  250 - log prob: 284.1157 - Encoding loss: 14.6323 - MSE: 8.9107 - gamma: 0.4304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   17 - Batch:  300 - log prob: 267.4913 - Encoding loss: 14.7371 - MSE: 8.5830 - gamma: 0.4282\n",
      "Epoch:   17 - Batch:  350 - log prob: 253.8719 - Encoding loss: 14.6745 - MSE: 8.8076 - gamma: 0.4261\n",
      "Epoch:   17 - Batch:  400 - log prob: 239.2945 - Encoding loss: 14.8874 - MSE: 8.8528 - gamma: 0.4240\n",
      "Epoch:   17 - Batch:  450 - log prob: 223.8867 - Encoding loss: 14.8922 - MSE: 8.7473 - gamma: 0.4219\n",
      "Epoch:   18 - Batch:   50 - log prob: 198.0304 - Encoding loss: 14.8872 - MSE: 8.5192 - gamma: 0.4185\n",
      "Epoch:   18 - Batch:  100 - log prob: 185.2819 - Encoding loss: 14.7617 - MSE: 8.7326 - gamma: 0.4165\n",
      "Epoch:   18 - Batch:  150 - log prob: 170.1030 - Encoding loss: 15.0402 - MSE: 8.6677 - gamma: 0.4144\n",
      "Epoch:   18 - Batch:  200 - log prob: 156.2457 - Encoding loss: 15.2227 - MSE: 8.8284 - gamma: 0.4123\n",
      "Epoch:   18 - Batch:  250 - log prob: 140.1415 - Encoding loss: 15.3266 - MSE: 8.6053 - gamma: 0.4103\n",
      "Epoch:   18 - Batch:  300 - log prob: 125.5870 - Encoding loss: 15.2826 - MSE: 8.6455 - gamma: 0.4082\n",
      "Epoch:   18 - Batch:  350 - log prob: 110.0907 - Encoding loss: 15.3514 - MSE: 8.5279 - gamma: 0.4062\n",
      "Epoch:   18 - Batch:  400 - log prob: 94.9299 - Encoding loss: 15.4107 - MSE: 8.4661 - gamma: 0.4042\n",
      "Epoch:   18 - Batch:  450 - log prob: 80.2041 - Encoding loss: 15.2526 - MSE: 8.4760 - gamma: 0.4022\n",
      "Epoch:   19 - Batch:   50 - log prob: 56.9206 - Encoding loss: 15.5527 - MSE: 8.5484 - gamma: 0.3990\n",
      "Epoch:   19 - Batch:  100 - log prob: 40.0965 - Encoding loss: 15.5092 - MSE: 8.2090 - gamma: 0.3970\n",
      "Epoch:   19 - Batch:  150 - log prob: 27.4369 - Encoding loss: 15.7158 - MSE: 8.5403 - gamma: 0.3950\n",
      "Epoch:   19 - Batch:  200 - log prob: 11.8695 - Encoding loss: 15.7115 - MSE: 8.4134 - gamma: 0.3931\n",
      "Epoch:   19 - Batch:  250 - log prob: -5.3130 - Encoding loss: 15.8139 - MSE: 8.0419 - gamma: 0.3911\n",
      "Epoch:   19 - Batch:  300 - log prob: -19.6861 - Encoding loss: 15.7711 - MSE: 8.1037 - gamma: 0.3892\n",
      "Epoch:   19 - Batch:  350 - log prob: -34.0765 - Encoding loss: 15.9998 - MSE: 8.1603 - gamma: 0.3872\n",
      "Epoch:   19 - Batch:  400 - log prob: -48.1045 - Encoding loss: 15.9707 - MSE: 8.2689 - gamma: 0.3853\n",
      "Epoch:   19 - Batch:  450 - log prob: -64.3367 - Encoding loss: 15.9950 - MSE: 8.0501 - gamma: 0.3834\n",
      "Epoch:   20 - Batch:   50 - log prob: -79.7737 - Encoding loss: 16.0784 - MSE: 8.2816 - gamma: 0.3813\n",
      "Epoch:   20 - Batch:  100 - log prob: -88.5725 - Encoding loss: 16.0693 - MSE: 8.1707 - gamma: 0.3804\n",
      "Epoch:   20 - Batch:  150 - log prob: -98.0731 - Encoding loss: 16.1941 - MSE: 7.8639 - gamma: 0.3794\n",
      "Epoch:   20 - Batch:  200 - log prob: -104.2514 - Encoding loss: 16.2861 - MSE: 8.0359 - gamma: 0.3785\n",
      "Epoch:   20 - Batch:  250 - log prob: -109.7944 - Encoding loss: 16.3532 - MSE: 8.2963 - gamma: 0.3775\n",
      "Epoch:   20 - Batch:  300 - log prob: -118.0596 - Encoding loss: 16.4303 - MSE: 8.1679 - gamma: 0.3766\n",
      "Epoch:   20 - Batch:  350 - log prob: -126.2261 - Encoding loss: 16.2653 - MSE: 8.0548 - gamma: 0.3757\n",
      "Epoch:   20 - Batch:  400 - log prob: -132.7824 - Encoding loss: 16.5693 - MSE: 8.1685 - gamma: 0.3747\n",
      "Epoch:   20 - Batch:  450 - log prob: -140.1115 - Encoding loss: 16.3799 - MSE: 8.1734 - gamma: 0.3738\n",
      "Epoch:   21 - Batch:   50 - log prob: -152.8650 - Encoding loss: 16.4823 - MSE: 7.8924 - gamma: 0.3723\n",
      "Epoch:   21 - Batch:  100 - log prob: -160.0365 - Encoding loss: 16.6167 - MSE: 8.0640 - gamma: 0.3714\n",
      "Epoch:   21 - Batch:  150 - log prob: -168.4755 - Encoding loss: 16.5882 - MSE: 7.9172 - gamma: 0.3704\n",
      "Epoch:   21 - Batch:  200 - log prob: -175.8691 - Encoding loss: 16.5674 - MSE: 7.9135 - gamma: 0.3695\n",
      "Epoch:   21 - Batch:  250 - log prob: -183.6916 - Encoding loss: 16.5473 - MSE: 7.8515 - gamma: 0.3686\n",
      "Epoch:   21 - Batch:  300 - log prob: -189.1533 - Encoding loss: 16.8238 - MSE: 8.1091 - gamma: 0.3677\n",
      "Epoch:   21 - Batch:  350 - log prob: -199.4535 - Encoding loss: 16.7157 - MSE: 7.7127 - gamma: 0.3668\n",
      "Epoch:   21 - Batch:  400 - log prob: -202.4103 - Encoding loss: 16.7425 - MSE: 8.3036 - gamma: 0.3659\n",
      "Epoch:   21 - Batch:  450 - log prob: -213.5988 - Encoding loss: 16.6138 - MSE: 7.7911 - gamma: 0.3650\n",
      "Epoch:   22 - Batch:   50 - log prob: -223.6870 - Encoding loss: 16.7014 - MSE: 7.8259 - gamma: 0.3635\n",
      "Epoch:   22 - Batch:  100 - log prob: -233.5558 - Encoding loss: 16.7134 - MSE: 7.6825 - gamma: 0.3626\n",
      "Epoch:   22 - Batch:  150 - log prob: -239.7415 - Encoding loss: 17.0423 - MSE: 7.8376 - gamma: 0.3617\n",
      "Epoch:   22 - Batch:  200 - log prob: -245.5630 - Encoding loss: 17.0172 - MSE: 8.0373 - gamma: 0.3608\n",
      "Epoch:   22 - Batch:  250 - log prob: -254.4309 - Encoding loss: 17.0808 - MSE: 7.8401 - gamma: 0.3599\n",
      "Epoch:   22 - Batch:  300 - log prob: -261.6356 - Encoding loss: 17.0109 - MSE: 7.8590 - gamma: 0.3590\n",
      "Epoch:   22 - Batch:  350 - log prob: -269.0250 - Encoding loss: 17.1333 - MSE: 7.8537 - gamma: 0.3581\n",
      "Epoch:   22 - Batch:  400 - log prob: -278.6885 - Encoding loss: 16.9855 - MSE: 7.5585 - gamma: 0.3572\n",
      "Epoch:   22 - Batch:  450 - log prob: -285.4524 - Encoding loss: 16.7841 - MSE: 7.6352 - gamma: 0.3563\n",
      "Epoch:   23 - Batch:   50 - log prob: -295.5882 - Encoding loss: 17.0048 - MSE: 7.6128 - gamma: 0.3549\n",
      "Epoch:   23 - Batch:  100 - log prob: -302.8425 - Encoding loss: 17.1651 - MSE: 7.8511 - gamma: 0.3540\n",
      "Epoch:   23 - Batch:  150 - log prob: -309.5775 - Encoding loss: 17.1972 - MSE: 7.9279 - gamma: 0.3531\n",
      "Epoch:   23 - Batch:  200 - log prob: -319.3434 - Encoding loss: 17.4001 - MSE: 7.6263 - gamma: 0.3523\n",
      "Epoch:   23 - Batch:  250 - log prob: -327.2550 - Encoding loss: 17.3845 - MSE: 7.5574 - gamma: 0.3514\n",
      "Epoch:   23 - Batch:  300 - log prob: -332.0310 - Encoding loss: 17.3097 - MSE: 7.8738 - gamma: 0.3505\n",
      "Epoch:   23 - Batch:  350 - log prob: -339.9185 - Encoding loss: 17.4706 - MSE: 7.8059 - gamma: 0.3496\n",
      "Epoch:   23 - Batch:  400 - log prob: -348.8777 - Encoding loss: 17.2516 - MSE: 7.6088 - gamma: 0.3488\n",
      "Epoch:   23 - Batch:  450 - log prob: -356.3007 - Encoding loss: 17.4269 - MSE: 7.5993 - gamma: 0.3479\n",
      "Epoch:   24 - Batch:   50 - log prob: -365.3723 - Encoding loss: 17.4098 - MSE: 7.6577 - gamma: 0.3465\n",
      "Epoch:   24 - Batch:  100 - log prob: -375.2789 - Encoding loss: 17.5001 - MSE: 7.6101 - gamma: 0.3457\n",
      "Epoch:   24 - Batch:  150 - log prob: -382.2754 - Encoding loss: 17.7160 - MSE: 7.6522 - gamma: 0.3448\n",
      "Epoch:   24 - Batch:  200 - log prob: -389.2124 - Encoding loss: 17.6813 - MSE: 7.6987 - gamma: 0.3439\n",
      "Epoch:   24 - Batch:  250 - log prob: -397.2028 - Encoding loss: 17.5338 - MSE: 7.6208 - gamma: 0.3431\n",
      "Epoch:   24 - Batch:  300 - log prob: -404.1059 - Encoding loss: 17.9002 - MSE: 7.6718 - gamma: 0.3422\n",
      "Epoch:   24 - Batch:  350 - log prob: -410.0630 - Encoding loss: 17.9603 - MSE: 7.8315 - gamma: 0.3414\n",
      "Epoch:   24 - Batch:  400 - log prob: -419.9668 - Encoding loss: 17.8489 - MSE: 7.5316 - gamma: 0.3405\n",
      "Epoch:   24 - Batch:  450 - log prob: -426.1645 - Encoding loss: 17.9408 - MSE: 7.6622 - gamma: 0.3397\n",
      "Epoch:   25 - Batch:   50 - log prob: -437.4538 - Encoding loss: 17.9159 - MSE: 7.4179 - gamma: 0.3383\n",
      "Epoch:   25 - Batch:  100 - log prob: -446.3045 - Encoding loss: 17.8521 - MSE: 7.5354 - gamma: 0.3375\n",
      "Epoch:   25 - Batch:  150 - log prob: -453.2007 - Encoding loss: 17.9704 - MSE: 7.5854 - gamma: 0.3366\n",
      "Epoch:   25 - Batch:  200 - log prob: -460.9773 - Encoding loss: 17.9342 - MSE: 7.5339 - gamma: 0.3358\n",
      "Epoch:   25 - Batch:  250 - log prob: -467.5532 - Encoding loss: 18.1218 - MSE: 7.6174 - gamma: 0.3350\n",
      "Epoch:   25 - Batch:  300 - log prob: -477.6933 - Encoding loss: 17.8427 - MSE: 7.3019 - gamma: 0.3341\n",
      "Epoch:   25 - Batch:  350 - log prob: -484.8196 - Encoding loss: 18.2643 - MSE: 7.3252 - gamma: 0.3333\n",
      "Epoch:   25 - Batch:  400 - log prob: -489.9001 - Encoding loss: 18.3798 - MSE: 7.5739 - gamma: 0.3325\n",
      "Epoch:   25 - Batch:  450 - log prob: -498.6639 - Encoding loss: 18.2510 - MSE: 7.4142 - gamma: 0.3316\n",
      "Epoch:   26 - Batch:   50 - log prob: -507.7147 - Encoding loss: 18.3631 - MSE: 7.3833 - gamma: 0.3303\n",
      "Epoch:   26 - Batch:  100 - log prob: -516.1626 - Encoding loss: 18.3060 - MSE: 7.5783 - gamma: 0.3295\n",
      "Epoch:   26 - Batch:  150 - log prob: -526.9579 - Encoding loss: 18.2564 - MSE: 7.2026 - gamma: 0.3287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   26 - Batch:  200 - log prob: -532.5597 - Encoding loss: 18.4231 - MSE: 7.3881 - gamma: 0.3279\n",
      "Epoch:   26 - Batch:  250 - log prob: -538.3761 - Encoding loss: 18.3956 - MSE: 7.5482 - gamma: 0.3271\n",
      "Epoch:   26 - Batch:  300 - log prob: -547.5177 - Encoding loss: 18.5738 - MSE: 7.3522 - gamma: 0.3262\n",
      "Epoch:   26 - Batch:  350 - log prob: -555.1433 - Encoding loss: 18.1985 - MSE: 7.3188 - gamma: 0.3254\n",
      "Epoch:   26 - Batch:  400 - log prob: -562.6536 - Encoding loss: 18.3454 - MSE: 7.2976 - gamma: 0.3246\n",
      "Epoch:   26 - Batch:  450 - log prob: -568.4595 - Encoding loss: 18.7615 - MSE: 7.4553 - gamma: 0.3238\n",
      "Epoch:   27 - Batch:   50 - log prob: -579.2644 - Encoding loss: 18.4588 - MSE: 7.2004 - gamma: 0.3225\n",
      "Epoch:   27 - Batch:  100 - log prob: -588.7158 - Encoding loss: 18.6136 - MSE: 7.3207 - gamma: 0.3217\n",
      "Epoch:   27 - Batch:  150 - log prob: -594.6273 - Encoding loss: 18.8256 - MSE: 7.4656 - gamma: 0.3209\n",
      "Epoch:   27 - Batch:  200 - log prob: -602.5722 - Encoding loss: 18.6144 - MSE: 7.3986 - gamma: 0.3201\n",
      "Epoch:   27 - Batch:  250 - log prob: -610.3963 - Encoding loss: 18.9075 - MSE: 7.3447 - gamma: 0.3193\n",
      "Epoch:   27 - Batch:  300 - log prob: -618.9345 - Encoding loss: 18.6573 - MSE: 7.2184 - gamma: 0.3185\n",
      "Epoch:   27 - Batch:  350 - log prob: -624.2154 - Encoding loss: 18.7422 - MSE: 7.4219 - gamma: 0.3177\n",
      "Epoch:   27 - Batch:  400 - log prob: -632.3672 - Encoding loss: 18.8980 - MSE: 7.3348 - gamma: 0.3169\n",
      "Epoch:   27 - Batch:  450 - log prob: -638.5178 - Encoding loss: 19.2346 - MSE: 7.4485 - gamma: 0.3162\n",
      "Epoch:   28 - Batch:   50 - log prob: -649.6286 - Encoding loss: 18.9145 - MSE: 7.1348 - gamma: 0.3149\n",
      "Epoch:   28 - Batch:  100 - log prob: -657.7259 - Encoding loss: 19.2218 - MSE: 7.4189 - gamma: 0.3141\n",
      "Epoch:   28 - Batch:  150 - log prob: -667.7890 - Encoding loss: 18.9710 - MSE: 7.1471 - gamma: 0.3133\n",
      "Epoch:   28 - Batch:  200 - log prob: -673.3476 - Encoding loss: 19.2091 - MSE: 7.3165 - gamma: 0.3126\n",
      "Epoch:   28 - Batch:  250 - log prob: -681.0476 - Encoding loss: 19.3041 - MSE: 7.2756 - gamma: 0.3118\n",
      "Epoch:   28 - Batch:  300 - log prob: -685.8134 - Encoding loss: 19.5596 - MSE: 7.5189 - gamma: 0.3110\n",
      "Epoch:   28 - Batch:  350 - log prob: -698.3053 - Encoding loss: 19.1210 - MSE: 7.0150 - gamma: 0.3102\n",
      "Epoch:   28 - Batch:  400 - log prob: -703.9230 - Encoding loss: 19.0984 - MSE: 7.1755 - gamma: 0.3095\n",
      "Epoch:   28 - Batch:  450 - log prob: -710.4629 - Encoding loss: 19.2026 - MSE: 7.2460 - gamma: 0.3087\n",
      "Epoch:   29 - Batch:   50 - log prob: -717.7617 - Encoding loss: 19.3700 - MSE: 7.2706 - gamma: 0.3075\n",
      "Epoch:   29 - Batch:  100 - log prob: -728.2909 - Encoding loss: 19.5056 - MSE: 7.3454 - gamma: 0.3067\n",
      "Epoch:   29 - Batch:  150 - log prob: -738.5888 - Encoding loss: 19.4713 - MSE: 7.0633 - gamma: 0.3059\n",
      "Epoch:   29 - Batch:  200 - log prob: -743.5851 - Encoding loss: 19.5898 - MSE: 7.2759 - gamma: 0.3052\n",
      "Epoch:   29 - Batch:  250 - log prob: -751.3630 - Encoding loss: 19.4364 - MSE: 7.2283 - gamma: 0.3044\n",
      "Epoch:   29 - Batch:  300 - log prob: -758.4315 - Encoding loss: 19.5797 - MSE: 7.2464 - gamma: 0.3036\n",
      "Epoch:   29 - Batch:  350 - log prob: -766.6969 - Encoding loss: 19.4942 - MSE: 7.1536 - gamma: 0.3029\n",
      "Epoch:   29 - Batch:  400 - log prob: -773.0266 - Encoding loss: 19.8401 - MSE: 7.2390 - gamma: 0.3021\n",
      "Epoch:   29 - Batch:  450 - log prob: -781.4276 - Encoding loss: 19.7578 - MSE: 7.1349 - gamma: 0.3014\n",
      "Epoch:   30 - Batch:   50 - log prob: -788.8745 - Encoding loss: 19.7471 - MSE: 7.1093 - gamma: 0.3002\n",
      "Epoch:   30 - Batch:  100 - log prob: -800.1470 - Encoding loss: 19.6928 - MSE: 7.1473 - gamma: 0.2994\n",
      "Epoch:   30 - Batch:  150 - log prob: -806.2642 - Encoding loss: 20.0432 - MSE: 7.2509 - gamma: 0.2987\n",
      "Epoch:   30 - Batch:  200 - log prob: -815.3260 - Encoding loss: 19.8270 - MSE: 7.0896 - gamma: 0.2980\n",
      "Epoch:   30 - Batch:  250 - log prob: -823.9835 - Encoding loss: 19.8119 - MSE: 6.9657 - gamma: 0.2972\n",
      "Epoch:   30 - Batch:  300 - log prob: -831.7155 - Encoding loss: 19.8589 - MSE: 6.9242 - gamma: 0.2965\n",
      "Epoch:   30 - Batch:  350 - log prob: -837.6372 - Encoding loss: 19.7788 - MSE: 7.0415 - gamma: 0.2957\n",
      "Epoch:   30 - Batch:  400 - log prob: -842.2707 - Encoding loss: 20.1856 - MSE: 7.2692 - gamma: 0.2950\n",
      "Epoch:   30 - Batch:  450 - log prob: -853.7184 - Encoding loss: 19.8324 - MSE: 6.9038 - gamma: 0.2943\n",
      "Epoch:   31 - Batch:   50 - log prob: -858.5564 - Encoding loss: 19.8382 - MSE: 7.0697 - gamma: 0.2931\n",
      "Epoch:   31 - Batch:  100 - log prob: -870.7238 - Encoding loss: 20.0157 - MSE: 7.0596 - gamma: 0.2924\n",
      "Epoch:   31 - Batch:  150 - log prob: -878.3747 - Encoding loss: 20.3685 - MSE: 7.0265 - gamma: 0.2916\n",
      "Epoch:   31 - Batch:  200 - log prob: -886.6680 - Encoding loss: 19.9962 - MSE: 6.9369 - gamma: 0.2909\n",
      "Epoch:   31 - Batch:  250 - log prob: -892.4875 - Encoding loss: 20.4031 - MSE: 7.0569 - gamma: 0.2902\n",
      "Epoch:   31 - Batch:  300 - log prob: -898.7703 - Encoding loss: 20.3185 - MSE: 7.1367 - gamma: 0.2895\n",
      "Epoch:   31 - Batch:  350 - log prob: -905.2899 - Encoding loss: 20.5097 - MSE: 7.1954 - gamma: 0.2888\n",
      "Epoch:   31 - Batch:  400 - log prob: -913.7292 - Encoding loss: 20.4310 - MSE: 7.0939 - gamma: 0.2880\n",
      "Epoch:   31 - Batch:  450 - log prob: -921.7310 - Encoding loss: 20.4457 - MSE: 7.0296 - gamma: 0.2873\n",
      "Epoch:   32 - Batch:   50 - log prob: -929.1449 - Encoding loss: 20.2974 - MSE: 6.9418 - gamma: 0.2862\n",
      "Epoch:   32 - Batch:  100 - log prob: -944.7224 - Encoding loss: 20.1159 - MSE: 6.6843 - gamma: 0.2855\n",
      "Epoch:   32 - Batch:  150 - log prob: -950.7038 - Encoding loss: 20.5462 - MSE: 6.7891 - gamma: 0.2848\n",
      "Epoch:   32 - Batch:  200 - log prob: -957.6124 - Encoding loss: 20.5772 - MSE: 6.8155 - gamma: 0.2840\n",
      "Epoch:   32 - Batch:  250 - log prob: -963.9781 - Encoding loss: 20.4800 - MSE: 6.8846 - gamma: 0.2833\n",
      "Epoch:   32 - Batch:  300 - log prob: -970.0503 - Encoding loss: 20.7556 - MSE: 6.9760 - gamma: 0.2826\n",
      "Epoch:   32 - Batch:  350 - log prob: -977.2642 - Encoding loss: 20.6474 - MSE: 6.9760 - gamma: 0.2819\n",
      "Epoch:   32 - Batch:  400 - log prob: -986.6186 - Encoding loss: 20.8717 - MSE: 6.8064 - gamma: 0.2812\n",
      "Epoch:   32 - Batch:  450 - log prob: -992.7586 - Encoding loss: 20.8333 - MSE: 6.8916 - gamma: 0.2805\n",
      "Epoch:   33 - Batch:   50 - log prob: -1001.6041 - Encoding loss: 20.5772 - MSE: 6.6647 - gamma: 0.2794\n",
      "Epoch:   33 - Batch:  100 - log prob: -1013.1797 - Encoding loss: 20.9343 - MSE: 6.7587 - gamma: 0.2787\n",
      "Epoch:   33 - Batch:  150 - log prob: -1021.1123 - Encoding loss: 20.9237 - MSE: 6.7053 - gamma: 0.2780\n",
      "Epoch:   33 - Batch:  200 - log prob: -1030.0602 - Encoding loss: 20.7375 - MSE: 6.5721 - gamma: 0.2773\n",
      "Epoch:   33 - Batch:  250 - log prob: -1031.7049 - Encoding loss: 21.2874 - MSE: 6.9992 - gamma: 0.2766\n",
      "Epoch:   33 - Batch:  300 - log prob: -1042.9208 - Encoding loss: 20.8650 - MSE: 6.6923 - gamma: 0.2760\n",
      "Epoch:   33 - Batch:  350 - log prob: -1049.5683 - Encoding loss: 20.9705 - MSE: 6.7349 - gamma: 0.2753\n",
      "Epoch:   33 - Batch:  400 - log prob: -1056.9252 - Encoding loss: 21.0149 - MSE: 6.7235 - gamma: 0.2746\n",
      "Epoch:   33 - Batch:  450 - log prob: -1061.6246 - Encoding loss: 21.0688 - MSE: 6.9113 - gamma: 0.2739\n",
      "Epoch:   34 - Batch:   50 - log prob: -1068.7845 - Encoding loss: 21.2324 - MSE: 6.7893 - gamma: 0.2728\n",
      "Epoch:   34 - Batch:  100 - log prob: -1083.9389 - Encoding loss: 20.9459 - MSE: 6.6396 - gamma: 0.2721\n",
      "Epoch:   34 - Batch:  150 - log prob: -1087.4762 - Encoding loss: 21.2025 - MSE: 6.9117 - gamma: 0.2715\n",
      "Epoch:   34 - Batch:  200 - log prob: -1095.4106 - Encoding loss: 21.2758 - MSE: 6.8560 - gamma: 0.2708\n",
      "Epoch:   34 - Batch:  250 - log prob: -1104.2029 - Encoding loss: 21.2971 - MSE: 6.7377 - gamma: 0.2701\n",
      "Epoch:   34 - Batch:  300 - log prob: -1111.7849 - Encoding loss: 21.3992 - MSE: 6.7091 - gamma: 0.2694\n",
      "Epoch:   34 - Batch:  350 - log prob: -1119.9167 - Encoding loss: 21.4632 - MSE: 6.6407 - gamma: 0.2688\n",
      "Epoch:   34 - Batch:  400 - log prob: -1129.1809 - Encoding loss: 21.2669 - MSE: 6.4914 - gamma: 0.2681\n",
      "Epoch:   34 - Batch:  450 - log prob: -1132.0519 - Encoding loss: 21.5050 - MSE: 6.8012 - gamma: 0.2674\n",
      "Epoch:   35 - Batch:   50 - log prob: -1136.8962 - Encoding loss: 21.2151 - MSE: 6.8201 - gamma: 0.2664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   35 - Batch:  100 - log prob: -1151.2358 - Encoding loss: 21.6625 - MSE: 6.7594 - gamma: 0.2657\n",
      "Epoch:   35 - Batch:  150 - log prob: -1157.0413 - Encoding loss: 21.8580 - MSE: 6.8574 - gamma: 0.2651\n",
      "Epoch:   35 - Batch:  200 - log prob: -1166.0314 - Encoding loss: 21.8311 - MSE: 6.7293 - gamma: 0.2644\n",
      "Epoch:   35 - Batch:  250 - log prob: -1174.0217 - Encoding loss: 21.8830 - MSE: 6.6722 - gamma: 0.2637\n",
      "Epoch:   35 - Batch:  300 - log prob: -1178.1438 - Encoding loss: 21.8827 - MSE: 6.8826 - gamma: 0.2631\n",
      "Epoch:   35 - Batch:  350 - log prob: -1189.3779 - Encoding loss: 21.5158 - MSE: 6.6009 - gamma: 0.2624\n",
      "Epoch:   35 - Batch:  400 - log prob: -1197.5301 - Encoding loss: 21.6737 - MSE: 6.5333 - gamma: 0.2618\n",
      "Epoch:   35 - Batch:  450 - log prob: -1201.9386 - Encoding loss: 22.0425 - MSE: 6.7220 - gamma: 0.2611\n",
      "Epoch:   36 - Batch:   50 - log prob: -1206.5137 - Encoding loss: 21.8404 - MSE: 6.7307 - gamma: 0.2601\n",
      "Epoch:   36 - Batch:  100 - log prob: -1221.8294 - Encoding loss: 22.0559 - MSE: 6.6312 - gamma: 0.2594\n",
      "Epoch:   36 - Batch:  150 - log prob: -1226.3882 - Encoding loss: 22.0236 - MSE: 6.8073 - gamma: 0.2588\n",
      "Epoch:   36 - Batch:  200 - log prob: -1237.8106 - Encoding loss: 21.8641 - MSE: 6.5211 - gamma: 0.2582\n",
      "Epoch:   36 - Batch:  250 - log prob: -1240.6496 - Encoding loss: 22.0958 - MSE: 6.8076 - gamma: 0.2575\n",
      "Epoch:   36 - Batch:  300 - log prob: -1251.6666 - Encoding loss: 21.9377 - MSE: 6.5509 - gamma: 0.2569\n",
      "Epoch:   36 - Batch:  350 - log prob: -1258.2665 - Encoding loss: 22.2479 - MSE: 6.5877 - gamma: 0.2562\n",
      "Epoch:   36 - Batch:  400 - log prob: -1263.8742 - Encoding loss: 22.1062 - MSE: 6.6884 - gamma: 0.2556\n",
      "Epoch:   36 - Batch:  450 - log prob: -1273.1980 - Encoding loss: 22.1937 - MSE: 6.5459 - gamma: 0.2550\n",
      "Epoch:   37 - Batch:   50 - log prob: -1277.4970 - Encoding loss: 22.1159 - MSE: 6.5469 - gamma: 0.2540\n",
      "Epoch:   37 - Batch:  100 - log prob: -1293.0951 - Encoding loss: 22.2033 - MSE: 6.4575 - gamma: 0.2533\n",
      "Epoch:   37 - Batch:  150 - log prob: -1299.9681 - Encoding loss: 22.1465 - MSE: 6.4770 - gamma: 0.2527\n",
      "Epoch:   37 - Batch:  200 - log prob: -1303.9484 - Encoding loss: 22.4485 - MSE: 6.6775 - gamma: 0.2521\n",
      "Epoch:   37 - Batch:  250 - log prob: -1309.5103 - Encoding loss: 22.5497 - MSE: 6.7755 - gamma: 0.2514\n",
      "Epoch:   37 - Batch:  300 - log prob: -1322.9350 - Encoding loss: 22.3464 - MSE: 6.3775 - gamma: 0.2508\n",
      "Epoch:   37 - Batch:  350 - log prob: -1326.2912 - Encoding loss: 22.6696 - MSE: 6.6144 - gamma: 0.2502\n",
      "Epoch:   37 - Batch:  400 - log prob: -1333.9629 - Encoding loss: 22.6184 - MSE: 6.5792 - gamma: 0.2496\n",
      "Epoch:   37 - Batch:  450 - log prob: -1340.7062 - Encoding loss: 22.7963 - MSE: 6.6019 - gamma: 0.2489\n",
      "Epoch:   38 - Batch:   50 - log prob: -1344.5179 - Encoding loss: 22.5945 - MSE: 6.6063 - gamma: 0.2480\n",
      "Epoch:   38 - Batch:  100 - log prob: -1356.3643 - Encoding loss: 22.9271 - MSE: 6.7719 - gamma: 0.2473\n",
      "Epoch:   38 - Batch:  150 - log prob: -1371.9942 - Encoding loss: 22.6441 - MSE: 6.2541 - gamma: 0.2467\n",
      "Epoch:   38 - Batch:  200 - log prob: -1374.4473 - Encoding loss: 22.5989 - MSE: 6.5381 - gamma: 0.2461\n",
      "Epoch:   38 - Batch:  250 - log prob: -1380.7316 - Encoding loss: 22.8547 - MSE: 6.5875 - gamma: 0.2455\n",
      "Epoch:   38 - Batch:  300 - log prob: -1391.5631 - Encoding loss: 22.9508 - MSE: 6.3631 - gamma: 0.2449\n",
      "Epoch:   38 - Batch:  350 - log prob: -1394.8473 - Encoding loss: 22.8616 - MSE: 6.5917 - gamma: 0.2443\n",
      "Epoch:   38 - Batch:  400 - log prob: -1403.5031 - Encoding loss: 23.0617 - MSE: 6.4986 - gamma: 0.2437\n",
      "Epoch:   38 - Batch:  450 - log prob: -1408.4077 - Encoding loss: 22.8393 - MSE: 6.6276 - gamma: 0.2431\n",
      "Epoch:   39 - Batch:   50 - log prob: -1410.7796 - Encoding loss: 22.9846 - MSE: 6.6907 - gamma: 0.2421\n",
      "Epoch:   39 - Batch:  100 - log prob: -1433.8332 - Encoding loss: 22.9961 - MSE: 6.2145 - gamma: 0.2415\n",
      "Epoch:   39 - Batch:  150 - log prob: -1439.0592 - Encoding loss: 23.1088 - MSE: 6.3261 - gamma: 0.2409\n",
      "Epoch:   39 - Batch:  200 - log prob: -1442.2596 - Encoding loss: 23.0559 - MSE: 6.5512 - gamma: 0.2403\n",
      "Epoch:   39 - Batch:  250 - log prob: -1451.7749 - Encoding loss: 23.1321 - MSE: 6.4104 - gamma: 0.2397\n",
      "Epoch:   39 - Batch:  300 - log prob: -1457.8364 - Encoding loss: 23.3585 - MSE: 6.4688 - gamma: 0.2391\n",
      "Epoch:   39 - Batch:  350 - log prob: -1464.5273 - Encoding loss: 23.2684 - MSE: 6.4906 - gamma: 0.2385\n",
      "Epoch:   39 - Batch:  400 - log prob: -1470.6237 - Encoding loss: 23.2777 - MSE: 6.5454 - gamma: 0.2379\n",
      "Epoch:   39 - Batch:  450 - log prob: -1477.3178 - Encoding loss: 23.6043 - MSE: 6.5659 - gamma: 0.2373\n",
      "Epoch:   40 - Batch:   50 - log prob: -1479.9305 - Encoding loss: 23.5047 - MSE: 6.4319 - gamma: 0.2367\n",
      "Epoch:   40 - Batch:  100 - log prob: -1495.1181 - Encoding loss: 23.4221 - MSE: 6.2392 - gamma: 0.2364\n",
      "Epoch:   40 - Batch:  150 - log prob: -1494.7833 - Encoding loss: 23.4661 - MSE: 6.4577 - gamma: 0.2361\n",
      "Epoch:   40 - Batch:  200 - log prob: -1496.3437 - Encoding loss: 23.7981 - MSE: 6.5672 - gamma: 0.2358\n",
      "Epoch:   40 - Batch:  250 - log prob: -1504.4802 - Encoding loss: 23.3441 - MSE: 6.3113 - gamma: 0.2355\n",
      "Epoch:   40 - Batch:  300 - log prob: -1506.8828 - Encoding loss: 23.4266 - MSE: 6.3742 - gamma: 0.2352\n",
      "Epoch:   40 - Batch:  350 - log prob: -1511.8302 - Encoding loss: 23.4316 - MSE: 6.2963 - gamma: 0.2349\n",
      "Epoch:   40 - Batch:  400 - log prob: -1516.5465 - Encoding loss: 23.5776 - MSE: 6.2316 - gamma: 0.2346\n",
      "Epoch:   40 - Batch:  450 - log prob: -1518.5128 - Encoding loss: 23.4240 - MSE: 6.3180 - gamma: 0.2344\n",
      "Epoch:   41 - Batch:   50 - log prob: -1513.4732 - Encoding loss: 23.6722 - MSE: 6.4421 - gamma: 0.2339\n",
      "Epoch:   41 - Batch:  100 - log prob: -1527.5627 - Encoding loss: 23.7225 - MSE: 6.3235 - gamma: 0.2336\n",
      "Epoch:   41 - Batch:  150 - log prob: -1532.0979 - Encoding loss: 23.5837 - MSE: 6.2713 - gamma: 0.2333\n",
      "Epoch:   41 - Batch:  200 - log prob: -1536.8810 - Encoding loss: 23.6314 - MSE: 6.2037 - gamma: 0.2330\n",
      "Epoch:   41 - Batch:  250 - log prob: -1536.3025 - Encoding loss: 23.9443 - MSE: 6.4269 - gamma: 0.2327\n",
      "Epoch:   41 - Batch:  300 - log prob: -1542.1352 - Encoding loss: 23.5895 - MSE: 6.3025 - gamma: 0.2324\n",
      "Epoch:   41 - Batch:  350 - log prob: -1542.7828 - Encoding loss: 23.7176 - MSE: 6.4581 - gamma: 0.2321\n",
      "Epoch:   41 - Batch:  400 - log prob: -1547.5295 - Encoding loss: 23.7789 - MSE: 6.3922 - gamma: 0.2319\n",
      "Epoch:   41 - Batch:  450 - log prob: -1552.6231 - Encoding loss: 23.7265 - MSE: 6.3081 - gamma: 0.2316\n",
      "Epoch:   42 - Batch:   50 - log prob: -1550.9959 - Encoding loss: 23.5874 - MSE: 6.2364 - gamma: 0.2311\n",
      "Epoch:   42 - Batch:  100 - log prob: -1559.6484 - Encoding loss: 23.7630 - MSE: 6.4206 - gamma: 0.2308\n",
      "Epoch:   42 - Batch:  150 - log prob: -1568.0743 - Encoding loss: 23.7410 - MSE: 6.1624 - gamma: 0.2305\n",
      "Epoch:   42 - Batch:  200 - log prob: -1563.3563 - Encoding loss: 24.0264 - MSE: 6.6002 - gamma: 0.2302\n",
      "Epoch:   42 - Batch:  250 - log prob: -1570.2392 - Encoding loss: 23.8188 - MSE: 6.4219 - gamma: 0.2300\n",
      "Epoch:   42 - Batch:  300 - log prob: -1573.5114 - Encoding loss: 23.9764 - MSE: 6.4350 - gamma: 0.2297\n",
      "Epoch:   42 - Batch:  350 - log prob: -1577.9899 - Encoding loss: 23.8806 - MSE: 6.3845 - gamma: 0.2294\n",
      "Epoch:   42 - Batch:  400 - log prob: -1585.7669 - Encoding loss: 23.7133 - MSE: 6.1612 - gamma: 0.2291\n",
      "Epoch:   42 - Batch:  450 - log prob: -1590.5860 - Encoding loss: 23.8689 - MSE: 6.0940 - gamma: 0.2288\n",
      "Epoch:   43 - Batch:   50 - log prob: -1585.2628 - Encoding loss: 23.7495 - MSE: 6.2071 - gamma: 0.2284\n",
      "Epoch:   43 - Batch:  100 - log prob: -1593.1921 - Encoding loss: 24.0077 - MSE: 6.4343 - gamma: 0.2281\n",
      "Epoch:   43 - Batch:  150 - log prob: -1599.4090 - Encoding loss: 24.1055 - MSE: 6.2965 - gamma: 0.2278\n",
      "Epoch:   43 - Batch:  200 - log prob: -1604.3635 - Encoding loss: 24.0552 - MSE: 6.2225 - gamma: 0.2275\n",
      "Epoch:   43 - Batch:  250 - log prob: -1603.8896 - Encoding loss: 24.1148 - MSE: 6.4291 - gamma: 0.2272\n",
      "Epoch:   43 - Batch:  300 - log prob: -1604.4353 - Encoding loss: 24.2770 - MSE: 6.5818 - gamma: 0.2269\n",
      "Epoch:   43 - Batch:  350 - log prob: -1613.4704 - Encoding loss: 24.0306 - MSE: 6.2976 - gamma: 0.2267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   43 - Batch:  400 - log prob: -1617.6538 - Encoding loss: 23.9860 - MSE: 6.2637 - gamma: 0.2264\n",
      "Epoch:   43 - Batch:  450 - log prob: -1622.1216 - Encoding loss: 24.3112 - MSE: 6.2152 - gamma: 0.2261\n",
      "Epoch:   44 - Batch:   50 - log prob: -1619.6023 - Encoding loss: 24.0666 - MSE: 6.1722 - gamma: 0.2256\n",
      "Epoch:   44 - Batch:  100 - log prob: -1630.0298 - Encoding loss: 24.3798 - MSE: 6.2764 - gamma: 0.2254\n",
      "Epoch:   44 - Batch:  150 - log prob: -1633.5615 - Encoding loss: 24.2098 - MSE: 6.2781 - gamma: 0.2251\n",
      "Epoch:   44 - Batch:  200 - log prob: -1636.0192 - Encoding loss: 24.1419 - MSE: 6.3317 - gamma: 0.2248\n",
      "Epoch:   44 - Batch:  250 - log prob: -1637.2197 - Encoding loss: 24.1317 - MSE: 6.4481 - gamma: 0.2245\n",
      "Epoch:   44 - Batch:  300 - log prob: -1644.7191 - Encoding loss: 24.1833 - MSE: 6.2473 - gamma: 0.2242\n",
      "Epoch:   44 - Batch:  350 - log prob: -1650.5024 - Encoding loss: 24.7001 - MSE: 6.1335 - gamma: 0.2240\n",
      "Epoch:   44 - Batch:  400 - log prob: -1653.4350 - Encoding loss: 24.3355 - MSE: 6.1632 - gamma: 0.2237\n",
      "Epoch:   44 - Batch:  450 - log prob: -1653.6849 - Encoding loss: 24.5669 - MSE: 6.3266 - gamma: 0.2234\n",
      "Epoch:   45 - Batch:   50 - log prob: -1652.6086 - Encoding loss: 24.3790 - MSE: 6.2025 - gamma: 0.2230\n",
      "Epoch:   45 - Batch:  100 - log prob: -1663.9232 - Encoding loss: 24.3122 - MSE: 6.2691 - gamma: 0.2227\n",
      "Epoch:   45 - Batch:  150 - log prob: -1671.2791 - Encoding loss: 24.2518 - MSE: 6.0813 - gamma: 0.2224\n",
      "Epoch:   45 - Batch:  200 - log prob: -1674.1307 - Encoding loss: 24.4736 - MSE: 6.1145 - gamma: 0.2221\n",
      "Epoch:   45 - Batch:  250 - log prob: -1676.9355 - Encoding loss: 24.3315 - MSE: 6.1495 - gamma: 0.2219\n",
      "Epoch:   45 - Batch:  300 - log prob: -1679.0231 - Encoding loss: 24.7235 - MSE: 6.2196 - gamma: 0.2216\n",
      "Epoch:   45 - Batch:  350 - log prob: -1680.7071 - Encoding loss: 24.4727 - MSE: 6.3092 - gamma: 0.2213\n",
      "Epoch:   45 - Batch:  400 - log prob: -1683.2056 - Encoding loss: 24.5291 - MSE: 6.3584 - gamma: 0.2210\n",
      "Epoch:   45 - Batch:  450 - log prob: -1688.1853 - Encoding loss: 24.6493 - MSE: 6.2862 - gamma: 0.2208\n",
      "Epoch:   46 - Batch:   50 - log prob: -1687.5509 - Encoding loss: 24.2590 - MSE: 6.1338 - gamma: 0.2203\n",
      "Epoch:   46 - Batch:  100 - log prob: -1696.9965 - Encoding loss: 24.3914 - MSE: 6.2986 - gamma: 0.2200\n",
      "Epoch:   46 - Batch:  150 - log prob: -1701.6022 - Encoding loss: 24.7016 - MSE: 6.2477 - gamma: 0.2198\n",
      "Epoch:   46 - Batch:  200 - log prob: -1705.5626 - Encoding loss: 24.7497 - MSE: 6.2256 - gamma: 0.2195\n",
      "Epoch:   46 - Batch:  250 - log prob: -1709.0987 - Encoding loss: 24.7480 - MSE: 6.2241 - gamma: 0.2192\n",
      "Epoch:   46 - Batch:  300 - log prob: -1712.6902 - Encoding loss: 24.5509 - MSE: 6.2197 - gamma: 0.2189\n",
      "Epoch:   46 - Batch:  350 - log prob: -1720.4266 - Encoding loss: 24.4875 - MSE: 6.0172 - gamma: 0.2187\n",
      "Epoch:   46 - Batch:  400 - log prob: -1718.9388 - Encoding loss: 24.6912 - MSE: 6.2560 - gamma: 0.2184\n",
      "Epoch:   46 - Batch:  450 - log prob: -1723.7447 - Encoding loss: 24.6879 - MSE: 6.1938 - gamma: 0.2181\n",
      "Epoch:   47 - Batch:   50 - log prob: -1721.3046 - Encoding loss: 24.7212 - MSE: 6.1215 - gamma: 0.2177\n",
      "Epoch:   47 - Batch:  100 - log prob: -1732.5558 - Encoding loss: 24.6871 - MSE: 6.2052 - gamma: 0.2174\n",
      "Epoch:   47 - Batch:  150 - log prob: -1735.7902 - Encoding loss: 24.7899 - MSE: 6.2201 - gamma: 0.2172\n",
      "Epoch:   47 - Batch:  200 - log prob: -1743.2936 - Encoding loss: 24.6360 - MSE: 6.0317 - gamma: 0.2169\n",
      "Epoch:   47 - Batch:  250 - log prob: -1744.8498 - Encoding loss: 24.6644 - MSE: 6.1232 - gamma: 0.2166\n",
      "Epoch:   47 - Batch:  300 - log prob: -1746.9419 - Encoding loss: 24.8391 - MSE: 6.1892 - gamma: 0.2163\n",
      "Epoch:   47 - Batch:  350 - log prob: -1753.2821 - Encoding loss: 24.7402 - MSE: 6.0565 - gamma: 0.2161\n",
      "Epoch:   47 - Batch:  400 - log prob: -1756.5059 - Encoding loss: 24.8343 - MSE: 6.0692 - gamma: 0.2158\n",
      "Epoch:   47 - Batch:  450 - log prob: -1762.4381 - Encoding loss: 24.8354 - MSE: 5.9561 - gamma: 0.2155\n",
      "Epoch:   48 - Batch:   50 - log prob: -1758.4236 - Encoding loss: 24.9483 - MSE: 5.9502 - gamma: 0.2151\n",
      "Epoch:   48 - Batch:  100 - log prob: -1764.3517 - Encoding loss: 25.1464 - MSE: 6.2867 - gamma: 0.2148\n",
      "Epoch:   48 - Batch:  150 - log prob: -1767.8928 - Encoding loss: 24.8608 - MSE: 6.2862 - gamma: 0.2146\n",
      "Epoch:   48 - Batch:  200 - log prob: -1777.8790 - Encoding loss: 24.9983 - MSE: 5.9876 - gamma: 0.2143\n",
      "Epoch:   48 - Batch:  250 - log prob: -1777.9024 - Encoding loss: 25.0035 - MSE: 6.1470 - gamma: 0.2140\n",
      "Epoch:   48 - Batch:  300 - log prob: -1782.4225 - Encoding loss: 24.9433 - MSE: 6.0998 - gamma: 0.2138\n",
      "Epoch:   48 - Batch:  350 - log prob: -1786.9914 - Encoding loss: 24.9908 - MSE: 6.0508 - gamma: 0.2135\n",
      "Epoch:   48 - Batch:  400 - log prob: -1786.4948 - Encoding loss: 25.1809 - MSE: 6.2324 - gamma: 0.2132\n",
      "Epoch:   48 - Batch:  450 - log prob: -1793.2275 - Encoding loss: 25.0729 - MSE: 6.0850 - gamma: 0.2130\n",
      "Epoch:   49 - Batch:   50 - log prob: -1788.5198 - Encoding loss: 25.2314 - MSE: 6.1006 - gamma: 0.2126\n",
      "Epoch:   49 - Batch:  100 - log prob: -1801.3798 - Encoding loss: 25.2870 - MSE: 6.1243 - gamma: 0.2123\n",
      "Epoch:   49 - Batch:  150 - log prob: -1806.0738 - Encoding loss: 25.1131 - MSE: 6.0724 - gamma: 0.2120\n",
      "Epoch:   49 - Batch:  200 - log prob: -1810.8926 - Encoding loss: 25.1771 - MSE: 6.0127 - gamma: 0.2118\n",
      "Epoch:   49 - Batch:  250 - log prob: -1810.7520 - Encoding loss: 25.1888 - MSE: 6.1753 - gamma: 0.2115\n",
      "Epoch:   49 - Batch:  300 - log prob: -1818.1204 - Encoding loss: 25.2616 - MSE: 6.0018 - gamma: 0.2112\n",
      "Epoch:   49 - Batch:  350 - log prob: -1821.9222 - Encoding loss: 25.3876 - MSE: 5.9880 - gamma: 0.2110\n",
      "Epoch:   49 - Batch:  400 - log prob: -1821.1608 - Encoding loss: 25.4095 - MSE: 6.1768 - gamma: 0.2107\n",
      "Epoch:   49 - Batch:  450 - log prob: -1825.6676 - Encoding loss: 25.3918 - MSE: 6.1313 - gamma: 0.2104\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "#Set number of epochs\n",
    "big_epochs = 50\n",
    "\n",
    "#Set up learning rate decay schedule. Set to decay by half every 40%\n",
    "epochs_per_decay = max(big_epochs*2//5, 1)\n",
    "big_schedule = torch.optim.lr_scheduler.StepLR(big_optimizer, epochs_per_decay, 0.5)\n",
    "\n",
    "train_VAE(\n",
    "    big_encoder,\n",
    "    big_decoder,\n",
    "    big_log_prob,\n",
    "    big_optimizer,\n",
    "    big_schedule,\n",
    "    training_loader,\n",
    "    big_epochs,\n",
    "    device\n",
    ")\n",
    "\n",
    "torch.save(big_encoder.state_dict(), './samples/2layer/big_encoder.pkl')\n",
    "torch.save(big_decoder.state_dict(), './samples/2layer/big_decoder.pkl')\n",
    "torch.save(big_log_prob.state_dict(), './samples/2layer/big_log_prob.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting means and scales.\n",
      "Extraction done.\n"
     ]
    }
   ],
   "source": [
    "class normal_dataset:\n",
    "    def __init__(self, mean_list, scale_list):\n",
    "        assert len(mean_list) == len(scale_list)\n",
    "        self.mean_list = mean_list\n",
    "        self.scale_list = scale_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mean_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        mean = self.mean_list[idx].squeeze()\n",
    "        scale = self.scale_list[idx].squeeze()\n",
    "        eps = torch.randn_like(mean, requires_grad=False)\n",
    "        return mean+scale*eps\n",
    "\n",
    "def extract_posterior(encoder, dataloader, device='cpu'):\n",
    "    #Store whether we are in training or evaluation mode\n",
    "    training = encoder.training\n",
    "    \n",
    "    #Put in evaluate mode\n",
    "    encoder.eval()\n",
    "    \n",
    "    #Start lists of statistics\n",
    "    means = []\n",
    "    scales = []\n",
    "    \n",
    "    #Disable gradient storage - they aren't needed for this\n",
    "    with torch.no_grad():\n",
    "        #Extract images, process to statistics, then append\n",
    "        for images in dataloader:\n",
    "            images = images.to(device)\n",
    "            batch_means, batch_scales = encoder(images)\n",
    "            means.append(batch_means)\n",
    "            scales.append(batch_scales)\n",
    "        #Combine from list of batch tensors into one tensor for means, scales\n",
    "        means = torch.cat(means, 0)\n",
    "        scales = torch.cat(scales, 0)\n",
    "        #Split into one tensor per image\n",
    "        means_dataset = means.split(1)\n",
    "        scales_dataset = scales.split(1)\n",
    "    \n",
    "    #Restore to previous mode (eval/training)\n",
    "    encoder.train(training)\n",
    "    return means_dataset, scales_dataset\n",
    "\n",
    "#Create data from big encoder\n",
    "print('Extracting means and scales.')\n",
    "mean_data, scale_data = extract_posterior(big_encoder, training_loader, device)\n",
    "statistics_dataset = normal_dataset(mean_data, scale_data)\n",
    "statistics_loader = torch.utils.data.DataLoader(statistics_dataset, batch_size=batch_size, shuffle=True)\n",
    "print('Extraction done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatLinear(nn.Module):\n",
    "    def __init__(self, in_size, out_size, activ, depth=3, bias=True):\n",
    "        super(CatLinear, self).__init__()\n",
    "        assert depth >= 1, \\\n",
    "            'Expected depth >=1, but got {0}'.format(depth)\n",
    "        \n",
    "        sizes = [in_size] + [out_size]*depth\n",
    "        in_sizes, out_sizes = sizes[:-1], sizes[1:]\n",
    "        layers = [nn.Linear(in_size, out_size, bias)]\n",
    "        layers += [nn.Linear(out_size, out_size, bias)\n",
    "                   for _ in range(depth-1)]\n",
    "        self.cat_net = nn.ModuleList(layers)\n",
    "        self.activ = activ\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        for layer in self.cat_net:\n",
    "            x = self.activ(layer(x))\n",
    "        return torch.cat([input, x], 1)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'CatLinear(in_features={0}, out_features={1}, activ={2}, depth={3}, bias={4})'.format(\n",
    "            self.cat_net[0].in_features, \n",
    "            self.cat_net[0].in_features+self.cat_net[-1].out_features, \n",
    "            self.activ, \n",
    "            len(self.cat_net), \n",
    "            self.cat_net[0].bias is not None\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train second variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): CatLinear(in_features=32, out_features=96, activ=ReLU(), depth=3, bias=True)\n",
      "  (1): VariationalEncoder(in_features=96, out_features=32, bias=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): CatLinear(in_features=32, out_features=96, activ=ReLU(), depth=3, bias=True)\n",
      "  (1): Linear(in_features=96, out_features=32, bias=True)\n",
      ")\n",
      "learned_log_prob(gamma=1.000)\n"
     ]
    }
   ],
   "source": [
    "#Set up second network\n",
    "h_size = 2*z_size\n",
    "depth = 3\n",
    "\n",
    "small_encoder = nn.Sequential(\n",
    "    CatLinear(z_size, h_size, nn.ReLU(), depth, bias=True),\n",
    "    VariationalEncoder(h_size+z_size, z_size, bias=True)\n",
    ")\n",
    "\n",
    "small_decoder = nn.Sequential(\n",
    "    CatLinear(z_size, h_size, nn.ReLU(), depth, bias=True),\n",
    "    nn.Linear(h_size+z_size, z_size, bias=True)\n",
    ")\n",
    "\n",
    "small_log_prob = learned_log_prob()\n",
    "\n",
    "#Move to proper device\n",
    "small_encoder.to(device)\n",
    "small_decoder.to(device)\n",
    "small_log_prob.to(device)\n",
    "#Load previously trained models, if they exist. Passes if they do not.\n",
    "try:\n",
    "    small_encoder.load_state_dict(torch.load('./samples/2layer/small_encoder.pkl'))\n",
    "    small_decoder.load_state_dict(torch.load('./samples/2layer/small_decoder.pkl'))\n",
    "    small_log_prob.load_state_dict(torch.load('./samples/2layer/small_log_prob.pkl'))\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "print(small_encoder)\n",
    "print(small_decoder)\n",
    "print(small_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect parameters\n",
    "small_params = list(small_encoder.parameters())+list(small_decoder.parameters())+list(small_log_prob.parameters())\n",
    "\n",
    "#Set up optimizers\n",
    "lr = 1e-4\n",
    "small_optimizer = torch.optim.Adam(small_params, lr=lr)\n",
    "epochs_per_update = 30\n",
    "small_schedule = torch.optim.lr_scheduler.StepLR(small_optimizer, epochs_per_update, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network.\n",
      "Epoch:    0 - Batch:   50 - log prob: 47.3941 - Encoding loss: 2.5333 - MSE: 18.1562 - gamma: 1.0047\n",
      "Epoch:    0 - Batch:  100 - log prob: 47.3476 - Encoding loss: 2.2803 - MSE: 17.9714 - gamma: 1.0087\n",
      "Epoch:    0 - Batch:  150 - log prob: 46.5486 - Encoding loss: 2.0120 - MSE: 17.1638 - gamma: 1.0114\n",
      "Epoch:    0 - Batch:  200 - log prob: 46.3490 - Encoding loss: 1.8180 - MSE: 16.9622 - gamma: 1.0133\n",
      "Epoch:    0 - Batch:  250 - log prob: 45.9015 - Encoding loss: 1.6320 - MSE: 16.5031 - gamma: 1.0143\n",
      "Epoch:    0 - Batch:  300 - log prob: 45.5149 - Encoding loss: 1.4730 - MSE: 16.1055 - gamma: 1.0140\n",
      "Epoch:    0 - Batch:  350 - log prob: 45.3115 - Encoding loss: 1.3580 - MSE: 15.8975 - gamma: 1.0128\n",
      "Epoch:    0 - Batch:  400 - log prob: 45.4233 - Encoding loss: 1.2934 - MSE: 16.0135 - gamma: 1.0108\n",
      "Epoch:    0 - Batch:  450 - log prob: 45.0165 - Encoding loss: 1.2187 - MSE: 15.6009 - gamma: 1.0084\n",
      "Epoch:    1 - Batch:   50 - log prob: 44.6010 - Encoding loss: 1.1500 - MSE: 15.3412 - gamma: 1.0035\n",
      "Epoch:    1 - Batch:  100 - log prob: 44.8264 - Encoding loss: 1.1520 - MSE: 15.4199 - gamma: 1.0006\n",
      "Epoch:    1 - Batch:  150 - log prob: 44.7087 - Encoding loss: 1.1346 - MSE: 15.3056 - gamma: 0.9971\n",
      "Epoch:    1 - Batch:  200 - log prob: 44.6532 - Encoding loss: 1.1330 - MSE: 15.2541 - gamma: 0.9944\n",
      "Epoch:    1 - Batch:  250 - log prob: 44.7593 - Encoding loss: 1.1541 - MSE: 15.3615 - gamma: 0.9915\n",
      "Epoch:    1 - Batch:  300 - log prob: 44.7171 - Encoding loss: 1.1563 - MSE: 15.3220 - gamma: 0.9887\n",
      "Epoch:    1 - Batch:  350 - log prob: 44.4010 - Encoding loss: 1.1640 - MSE: 15.0157 - gamma: 0.9864\n",
      "Epoch:    1 - Batch:  400 - log prob: 44.4044 - Encoding loss: 1.1858 - MSE: 15.0218 - gamma: 0.9837\n",
      "Epoch:    1 - Batch:  450 - log prob: 44.5824 - Encoding loss: 1.2325 - MSE: 15.1955 - gamma: 0.9815\n",
      "Epoch:    2 - Batch:   50 - log prob: 44.1683 - Encoding loss: 1.2656 - MSE: 14.9418 - gamma: 0.9782\n",
      "Epoch:    2 - Batch:  100 - log prob: 44.2517 - Encoding loss: 1.3045 - MSE: 14.8826 - gamma: 0.9759\n",
      "Epoch:    2 - Batch:  150 - log prob: 44.1644 - Encoding loss: 1.3234 - MSE: 14.8006 - gamma: 0.9735\n",
      "Epoch:    2 - Batch:  200 - log prob: 44.0184 - Encoding loss: 1.3298 - MSE: 14.6646 - gamma: 0.9709\n",
      "Epoch:    2 - Batch:  250 - log prob: 44.0114 - Encoding loss: 1.3517 - MSE: 14.6601 - gamma: 0.9683\n",
      "Epoch:    2 - Batch:  300 - log prob: 43.9246 - Encoding loss: 1.3681 - MSE: 14.5804 - gamma: 0.9666\n",
      "Epoch:    2 - Batch:  350 - log prob: 43.9470 - Encoding loss: 1.3920 - MSE: 14.6024 - gamma: 0.9649\n",
      "Epoch:    2 - Batch:  400 - log prob: 43.9391 - Encoding loss: 1.4171 - MSE: 14.5961 - gamma: 0.9629\n",
      "Epoch:    2 - Batch:  450 - log prob: 43.7654 - Encoding loss: 1.4360 - MSE: 14.4363 - gamma: 0.9611\n",
      "Epoch:    3 - Batch:   50 - log prob: 43.7009 - Encoding loss: 1.5167 - MSE: 14.5118 - gamma: 0.9589\n",
      "Epoch:    3 - Batch:  100 - log prob: 43.9049 - Encoding loss: 1.5222 - MSE: 14.5673 - gamma: 0.9581\n",
      "Epoch:    3 - Batch:  150 - log prob: 43.7796 - Encoding loss: 1.5310 - MSE: 14.4522 - gamma: 0.9568\n",
      "Epoch:    3 - Batch:  200 - log prob: 43.5180 - Encoding loss: 1.5074 - MSE: 14.2137 - gamma: 0.9551\n",
      "Epoch:    3 - Batch:  250 - log prob: 43.6111 - Encoding loss: 1.5363 - MSE: 14.2998 - gamma: 0.9535\n",
      "Epoch:    3 - Batch:  300 - log prob: 43.8719 - Encoding loss: 1.5677 - MSE: 14.5371 - gamma: 0.9527\n",
      "Epoch:    3 - Batch:  350 - log prob: 43.6860 - Encoding loss: 1.5987 - MSE: 14.3684 - gamma: 0.9523\n",
      "Epoch:    3 - Batch:  400 - log prob: 43.6825 - Encoding loss: 1.6049 - MSE: 14.3655 - gamma: 0.9515\n",
      "Epoch:    3 - Batch:  450 - log prob: 43.6263 - Encoding loss: 1.6027 - MSE: 14.3151 - gamma: 0.9498\n",
      "Epoch:    4 - Batch:   50 - log prob: 43.3858 - Encoding loss: 1.6397 - MSE: 14.2275 - gamma: 0.9488\n",
      "Epoch:    4 - Batch:  100 - log prob: 43.7053 - Encoding loss: 1.6790 - MSE: 14.3872 - gamma: 0.9483\n",
      "Epoch:    4 - Batch:  150 - log prob: 43.6113 - Encoding loss: 1.6693 - MSE: 14.3022 - gamma: 0.9472\n",
      "Epoch:    4 - Batch:  200 - log prob: 43.3677 - Encoding loss: 1.6511 - MSE: 14.0841 - gamma: 0.9460\n",
      "Epoch:    4 - Batch:  250 - log prob: 43.5211 - Encoding loss: 1.6979 - MSE: 14.2217 - gamma: 0.9447\n",
      "Epoch:    4 - Batch:  300 - log prob: 43.6721 - Encoding loss: 1.7237 - MSE: 14.3564 - gamma: 0.9447\n",
      "Epoch:    4 - Batch:  350 - log prob: 43.4825 - Encoding loss: 1.7251 - MSE: 14.1873 - gamma: 0.9443\n",
      "Epoch:    4 - Batch:  400 - log prob: 43.6308 - Encoding loss: 1.7515 - MSE: 14.3196 - gamma: 0.9444\n",
      "Epoch:    4 - Batch:  450 - log prob: 43.7056 - Encoding loss: 1.7893 - MSE: 14.3862 - gamma: 0.9441\n",
      "Epoch:    5 - Batch:   50 - log prob: 43.2932 - Encoding loss: 1.7559 - MSE: 14.1453 - gamma: 0.9430\n",
      "Epoch:    5 - Batch:  100 - log prob: 43.5679 - Encoding loss: 1.8046 - MSE: 14.2640 - gamma: 0.9423\n",
      "Epoch:    5 - Batch:  150 - log prob: 43.4684 - Encoding loss: 1.8219 - MSE: 14.1750 - gamma: 0.9414\n",
      "Epoch:    5 - Batch:  200 - log prob: 43.3290 - Encoding loss: 1.7815 - MSE: 14.0517 - gamma: 0.9405\n",
      "Epoch:    5 - Batch:  250 - log prob: 43.2781 - Encoding loss: 1.7898 - MSE: 14.0069 - gamma: 0.9396\n",
      "Epoch:    5 - Batch:  300 - log prob: 43.5225 - Encoding loss: 1.8358 - MSE: 14.2227 - gamma: 0.9396\n",
      "Epoch:    5 - Batch:  350 - log prob: 43.3437 - Encoding loss: 1.8169 - MSE: 14.0649 - gamma: 0.9397\n",
      "Epoch:    5 - Batch:  400 - log prob: 43.3784 - Encoding loss: 1.8717 - MSE: 14.0956 - gamma: 0.9389\n",
      "Epoch:    5 - Batch:  450 - log prob: 43.2435 - Encoding loss: 1.8643 - MSE: 13.9768 - gamma: 0.9384\n",
      "Epoch:    6 - Batch:   50 - log prob: 43.0823 - Encoding loss: 1.8785 - MSE: 13.9592 - gamma: 0.9374\n"
     ]
    }
   ],
   "source": [
    "small_epochs = 50\n",
    "try:\n",
    "    train_VAE(\n",
    "        small_encoder,\n",
    "        small_decoder,\n",
    "        small_log_prob,\n",
    "        small_optimizer,\n",
    "        small_schedule,\n",
    "        statistics_loader,\n",
    "        small_epochs,\n",
    "        device\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "torch.save(small_encoder.state_dict(), './samples/2layer/small_encoder.pkl')\n",
    "torch.save(small_decoder.state_dict(), './samples/2layer/small_decoder.pkl')\n",
    "torch.save(small_log_prob.state_dict(), './samples/2layer/small_log_prob.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def show_image(ax, images, nrow=8):\n",
    "    images = make_grid(images, nrow).cpu()\n",
    "    images = to_pil_image(images)\n",
    "    ax.imshow(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [64 x 32], m2: [64 x 128] at C:/w/1/s/tmp_conda_3.7_183424/conda/conda-bld/pytorch_1570818936694/work/aten/src\\THC/generic/THCTensorMathBlas.cu:290",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-e59b90ef8174>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m#Construct image from 2 layer VAE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmall_decoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mimage_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbig_decoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mimage_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda3\\envs\\pytorch_nb\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda3\\envs\\pytorch_nb\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda3\\envs\\pytorch_nb\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-4676a82654fb>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat_net\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda3\\envs\\pytorch_nb\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda3\\envs\\pytorch_nb\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda3\\envs\\pytorch_nb\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1368\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [64 x 32], m2: [64 x 128] at C:/w/1/s/tmp_conda_3.7_183424/conda/conda-bld/pytorch_1570818936694/work/aten/src\\THC/generic/THCTensorMathBlas.cu:290"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pylab import subplots\n",
    "#Put networks in evaluation mode.\n",
    "big_encoder.eval()\n",
    "big_decoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_rows = 8\n",
    "    fig, (ax1, ax2) = subplots(1, 2, figsize=(12, 6))\n",
    "    u = torch.randn((image_rows**2, z_size), device=device)\n",
    "    \n",
    "    #Construct images from 1 layer VAE\n",
    "    image_1 = big_decoder(u)\n",
    "    image_1 = image_1.reshape((-1, *image_shape))\n",
    "    \n",
    "    #Construct image from 2 layer VAE\n",
    "    z = small_decoder(u)\n",
    "    image_2 = big_decoder(z)\n",
    "    image_2 = image_2.reshape((-1, *image_shape))\n",
    "\n",
    "#Display images\n",
    "show_image(ax1, image_1, image_rows)\n",
    "show_image(ax2, image_2, image_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd79b0858a4475ca65c975454c25de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(BoundedIntText(value=0, description='Image 1', max=60000), BoundedIntText(value=1, descr"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#Image slider 1\n",
    "is1 = widgets.BoundedIntText(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len(image_set),\n",
    "    description='Image 1'\n",
    ")\n",
    "\n",
    "#Color picker 1\n",
    "cp1 = widgets.ColorPicker(\n",
    "    description='Color 1',\n",
    "    value='#00FF00',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "#Image slider 2\n",
    "is2 = widgets.BoundedIntText(\n",
    "    value=1,\n",
    "    min=0,\n",
    "    max=len(image_set),\n",
    "    description='Image 2'\n",
    ")\n",
    "\n",
    "#Color picker 2\n",
    "cp2 = widgets.ColorPicker(\n",
    "    description='Color 2',\n",
    "    value='#FF0000',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "width = 4\n",
    "steps = width**2\n",
    "\n",
    "def hex_to_tensor(hex_color):\n",
    "    rgb = [int(hex_color[1+i:3+i], 16)/255. for i in range(3)]\n",
    "    return torch.tensor(rgb)\n",
    "\n",
    "def update_image(idx_1, idx_2, color_1, color_2):\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(12, 3))\n",
    "    with torch.no_grad():\n",
    "        #Get images\n",
    "        image_1 = image_set.color(idx_1, color_1)\n",
    "        image_2 = image_set.color(idx_2, color_2)\n",
    "        \n",
    "        show_image(ax[0], image_1)\n",
    "        show_image(ax[3], image_2)\n",
    "        \n",
    "        image_1, image_2 = image_1.to(device), image_2.to(device)\n",
    "        a = torch.linspace(0, 1, steps, device=device).unsqueeze(1)\n",
    "\n",
    "        #Find means from big encoder and interpolate\n",
    "        z0 = big_encoder(image_1.unsqueeze(0))[0]\n",
    "        z1 = big_encoder(image_2.unsqueeze(0))[0]\n",
    "        zs = z0*(1-a)+a*z1\n",
    "\n",
    "        #stage 1 reconstructions\n",
    "        image_1 = big_decoder(zs)\n",
    "        image_1 = image_1.reshape((-1, *image_shape))\n",
    "        show_image(ax[1], image_1, width)\n",
    "\n",
    "        #Find further means from stage 2 and interpolate\n",
    "        u0 = small_encoder(z0)[0]\n",
    "        u1 = small_encoder(z1)[0]\n",
    "        us = u0*(1-a)+a*u1\n",
    "\n",
    "        #Stage 2 reconstructions\n",
    "        zs_2 = small_decoder(us)\n",
    "        image_2 = big_decoder(zs_2)\n",
    "        image_2 = image_2.reshape((-1, *image_shape))\n",
    "        show_image(ax[2], image_2, width)\n",
    "        return fig\n",
    "        \n",
    "interactive_plot = widgets.interactive(update_image, idx_1=is1, idx_2=is2, color_1=cp1, color_2=cp2)\n",
    "output = interactive_plot.children[-1]\n",
    "\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadb73b534294a8aa1c3fd856b27658f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ColorPicker(value='#FFFFFF', description='Pick a color for image 1')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_1_color = widgets.ColorPicker(\n",
    "    concise=False,\n",
    "    description='Pick a color for image 1',\n",
    "    value='#FFFFFF',\n",
    "    disabled=False\n",
    ")\n",
    "display(image_1_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ce7c320fc8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "color = image_1_color.value\n",
    "example = image_set.color(3, color)\n",
    "plt.imshow(to_pil_image(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
