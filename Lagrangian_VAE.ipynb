{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagrangrian Variational Autoencoder (Draft)\n",
    "\n",
    "[[source](https://arxiv.org/abs/1806.06514)]\n",
    "[[TensorFlow](https://github.com/ermongroup/lagvae)]\n",
    "\n",
    "Note: The code below is functional. However, the explanation here is still in draft form.\n",
    "\n",
    "This is implemented with PyTorch and Lightning. The code is based on the [paper](https://arxiv.org/abs/1806.06514) and [tensorflow code](https://github.com/ermongroup/lagvae) by Ermon et al.\n",
    "\n",
    "The most notable change from a basic VAE is that instead of directly optimizing the loss, they are optimizing the [Lagrangian dual problem](https://en.wikipedia.org/wiki/Duality_(optimization)#Dual_problem). A tutorial on convex optimization involving the Lagrangian dual problem can be found [here](http://eceweb.ucsd.edu/~gert/ECE273/hindiTutorial2.pdf). If you've taken vector calculus, you may have seen a form of the Lagrangian before as Lagrange multipliers. The idea is similar, but now more generally applicable.\n",
    "\n",
    "The basic VAE gives us a process by which to find a probability distribution $p(x, z)$, where $x$ are observations and $z$ are latent variables. This is done by factoring $p(x, z)$ to $p(x|z)p(z)$. The distribution $p(z)$ is specified to be normal a priori, while $p(x|z) = p(x|z, \\theta)$ is a parameterized distribution that is learned through the decoder. To make the process of finding $p(x)$ tractable, we introduce the variational distribution $q(z|x)$, which approximates $p(z|x)$. We can similarly define a distribution $q(x, z) = q(z|x)q(x)$. Ideally, the variational distribution $q(z|x) = p(z|x)$, and so we would like $p(x, z) = q(x, z)$. If the two joint distributions are equal, then their marginals will be equal.\n",
    "\n",
    "This provides another way of training the model, which is to enforce consistency. The basic VAE is trained to maximize the evidence lower bound. As shown in the paper above, this is equivalent to the optimization problem\n",
    "\n",
    "$$ \\min_\\theta \\text{KL}(q(x, z | \\theta) \\Vert p(x, z | \\theta))$$\n",
    "\n",
    "where KL is the Kullback-Leibler divergence. Other variational autoencoders can be shown to be equivalent to minimization problems using different divergences. With this in mind, we let $\\mathcal{D}$ be a vector of divergences between probabilities. We require that $\\mathcal{D} = 0$ (so that $\\mathcal{D}_i = 0$ for each $i$) if and only if $p(x, z) = q(x, z)$. This allows us to use $\\mathcal{D}$ to enforce consistency. As this depends on the parameters for the distributions, we have that $\\mathcal{D}$ depends on $\\theta$, i.e. $\\mathcal{D} = \\mathcal{D}(\\theta)$. I'll write the latter to emphasize the connection to theta.\n",
    "\n",
    "However, having consistent marginals does not guarantee that there is any meaningful relationship between $x$ and $z$. The paper tackles this latter problem by using an extra function $f(\\theta)$. The specific function that is chosen is based on our preferences between consistent distributions. The choice in the code below for $f$ is one that encodes a preference for mutual information. Our optimization problem has now become\n",
    "\n",
    "$$\\min_\\theta f(\\theta) \\text{ where } \\mathcal{D}(\\theta) = 0.$$\n",
    "\n",
    "This has the dual problem\n",
    "\n",
    "$$\\max_{\\lambda \\geq 0}\\min_\\theta \\left[f(\\theta)+\\lambda^T\\mathcal{D}(\\theta)\\right],$$\n",
    "\n",
    "where $\\theta$ includes choices where $\\mathcal{D}(\\theta) \\geq 0$. This second equation comes from considering the convex conjugate of $f$. in the dual space, which gives rise to the name 'dual problem.' The important factor is that the same parameter $\\theta$ is the solution for both equations when we have strong duality.\n",
    "\n",
    "We do need to make a slight adjustment, however. We will not in practice have $\\mathcal{D} = 0$ for all models as a result of having finite capacity. Instead, we choose a 'consistency constraint' vector $\\epsilon$ that limits the size of $\\mathcal{D}$. In this situation, we optimize\n",
    "\n",
    "$$\\min_\\theta f(\\theta) \\text{ where } \\mathcal{D}_i \\leq \\epsilon_i.$$\n",
    "\n",
    "which has the corresponding dual equation\n",
    "\n",
    "$$\\max_{\\lambda \\geq 0}\\min_\\theta \\left[f(\\theta)+\\lambda^T(\\mathcal{D}-\\epsilon)\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "### Variational Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalEncoder(nn.Module):\n",
    "    '''\n",
    "    Encodes an observation to the latent variable parameters.\n",
    "    \n",
    "    :param z_dim: (int) Dimension of latent variable\n",
    "    '''\n",
    "    def __init__(self, z_dim=2):\n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28**2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mean = nn.Linear(128, z_dim)\n",
    "        self.logvar = nn.Linear(128, z_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.network(input)\n",
    "        \n",
    "        mean = self.mean(x)\n",
    "        scale = (self.logvar(x)/2).exp()\n",
    "        return mean, scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalDecoder(nn.Module):\n",
    "    '''\n",
    "    Decodes the latent variable to an observation.\n",
    "    \n",
    "    :param z_dim: (int) Dimension of latent variable\n",
    "    '''\n",
    "    def __init__(self, z_dim=2):\n",
    "        super(VariationalDecoder, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(z_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 28**2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.network(z)\n",
    "        return x.view((-1,1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagrangian Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LagrangianVAE(nn.Module):\n",
    "    '''\n",
    "    Implements a Lagrangian VAE. This differs from a other VAEs as it is optimized\n",
    "    according to a dual optimization problem.\n",
    "    \n",
    "    :param encoder: (nn.Module) Encoder network. Should return mean, scale.\n",
    "    :param decoder: (nn.Module) Decoder network. Should return the observation reconstruction.\n",
    "    :param mi: (float) Mutual information parameter. A positive value places emphasis on\n",
    "        mutual information under p(x, z). A negative value emphasizes mutual information\n",
    "        under q(x, z).\n",
    "        \n",
    "        More directly, a positive value emphasizes the reconstruction error while a negative\n",
    "        value emphasizes the latent error.\n",
    "    :param e: ((float)) Relaxed consistency constraint. This is used to make the optimization\n",
    "        problem tractible.\n",
    "    :param lmbda: ((float)) Starting value for lambda.\n",
    "    :param mmd_scale: (float) How much to scale the maximum mean discrepency.\n",
    "    :param optimize_lambda: (Bool) Whether lambda will be optimized over. Setting to False\n",
    "        will fix lambda at its starting value.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 mi,\n",
    "                 e = (1., 1.),\n",
    "                 lmbda = (1., 1.)):\n",
    "        super(LagrangianVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        #The feasability parameter. (See section 5.2 in the paper)\n",
    "        self.e = e\n",
    "        #Mutual information parameter\n",
    "        self.mi = mi\n",
    "\n",
    "        self.L1 = nn.Parameter(torch.tensor(lmbda[0]))\n",
    "        self.L2 = nn.Parameter(torch.tensor(lmbda[1]))\n",
    "\n",
    "    def forward(self, input):\n",
    "        mean, scale = self.encoder(input)\n",
    "\n",
    "        eps = torch.randn_like(mean)\n",
    "        z = mean+eps*scale\n",
    "\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def generate_similar(self, input, noise=True):\n",
    "        '''\n",
    "        Generates similar images to input.\n",
    "\n",
    "        :param input: (torch.Tensor) Original observation\n",
    "        :param noise: (Bool) Whether to inject noise into the reconstruction.\n",
    "            By default, z = N(mu, Sigma). If noise is False, then z = mu.\n",
    "        '''\n",
    "        mean, scale = self.encoder(input)\n",
    "        if noise:\n",
    "            eps = torch.randn_like(mean)\n",
    "            z = mean+eps*scale\n",
    "        else:\n",
    "            z = mean\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def child_parameters(self):\n",
    "        '''\n",
    "        Construct generator that returns all parameters but lambda.\n",
    "        '''\n",
    "        for network in [self.encoder, self.decoder]:\n",
    "            for parameter in network.parameters():\n",
    "                yield parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmd_loss(z):\n",
    "    '''\n",
    "    Compute maximum mean discrepency for the latent variable z.\n",
    "\n",
    "    :param z: (torch.Tensor)\n",
    "    :return: (torch.Tensor) \n",
    "    '''\n",
    "    true_sample = torch.randn_like(z)\n",
    "    return 10000.*compute_mmd(z, true_sample) # Scale is built in\n",
    "\n",
    "def elbo_loss(z, eps, scale):\n",
    "    '''\n",
    "    Compute ELBO loss for the encoding.\n",
    "\n",
    "    :param z: (torch.Tensor) Latent variable\n",
    "    :param eps: (torch.Tensor) Noise used to generate z. Should have\n",
    "        eps ~ N(0, I). \n",
    "    :param scale: (torch.Tensor) Scale used to generate z.\n",
    "    :return: (torch.Tensor) evidence lower bound.\n",
    "    '''\n",
    "    return torch.sum(z.pow(2)/2-eps.pow(2)/2-scale.log(), 1)\n",
    "\n",
    "#Computes maximal mean divergence (mmd)\n",
    "def compute_mmd(x, y):\n",
    "    x_kernel = compute_kernel(x, x)\n",
    "    y_kernel = compute_kernel(y, y)\n",
    "    xy_kernel = compute_kernel(x, y)\n",
    "    mmd = x_kernel.mean() + y_kernel.mean() - 2*xy_kernel.mean()\n",
    "    return mmd\n",
    "\n",
    "def compute_kernel(x, y):\n",
    "    x_size = x.size(0)\n",
    "    y_size = y.size(0)\n",
    "    dim = x.size(1)\n",
    "    x = x.unsqueeze(1) # (x_size, 1, dim)\n",
    "    y = y.unsqueeze(0) # (1, y_size, dim)\n",
    "    tiled_x = x.expand(x_size, y_size, dim)\n",
    "    tiled_y = y.expand(x_size, y_size, dim)\n",
    "    kernel_input = (tiled_x - tiled_y).pow(2).mean(2)/float(dim)\n",
    "    return torch.exp(-kernel_input) # (x_size, y_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the network\n",
    "\n",
    "### Solving a max-min problem\n",
    "The most notable change from a basic VAE is that instead of directly optimizing the loss, we are attempting to solve the Lagrangian dual problem. While we are attempting to minimize the objective with respect to $\\theta$, we are trying to maximize it with respect to $\\lambda$. The issue that this causes is that the optimizers in pytorch are designed to minimize only. It turns out there is an easy fix, though: after calling backwards, change the stored gradient to be negative. (Linearity of differentation is great for a lot of reasons.) Alternatively, you can use `(-loss).backward(retain_graph=True)`. As pytorch graphs are dynamic, we need to explicitely retain the graph. The reason I didn't use this is that it requires us to zero the gradients and call `loss.backward()` again to update the other parameters. \n",
    "\n",
    "We can use one optimizer for both sets of parameters if we take care to update the gradient first. We use an optimizer for each as in the original code, as it allows for greater control over our parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_set = datasets.MNIST(root='./data/', train=True, download=False, transform=ToTensor())\n",
    "batch_size = 100\n",
    "training_loader = data.DataLoader(dataset=training_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lagVAE(z_size, mi, e, lmbda = (1., 1.)):\n",
    "    encoder = VariationalEncoder(z_size)\n",
    "    decoder = VariationalDecoder(z_size)\n",
    "    return LagrangianVAE(encoder, decoder, mi, e, lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lagVAE(model, data_loader, optimizer, lambda_optim, device='cpu'):\n",
    "    L1, L2 = model.L1, model.L2\n",
    "    e1, e2 = model.e\n",
    "    mi = model.mi\n",
    "    \n",
    "    average_recon = 0.\n",
    "    average_latent = 0.\n",
    "    for n_batch, (x, _) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        lambda_optim.zero_grad()\n",
    "\n",
    "        x = x.to(device=device)\n",
    "        mean, scale = model.encoder(x)\n",
    "\n",
    "        eps = torch.randn_like(mean)\n",
    "        z = mean+eps*scale\n",
    "\n",
    "        x_pred = model.decoder(z)\n",
    "\n",
    "        #compute -log p(x|z)\n",
    "        nll = F.mse_loss(x, x_pred, reduction='sum') / len(x)\n",
    "\n",
    "        #latent errors\n",
    "        mmd = mmd_loss(z)\n",
    "        elbo = elbo_loss(z, eps, scale).mean()\n",
    "\n",
    "        if mi <= 0:\n",
    "            loss = L1 * nll + (L1 - mi) * elbo + L2 * mmd - L1 * e1 - L2 * e2\n",
    "        else:\n",
    "            loss = (L1 + mi) * nll + L1 * elbo + L2 * mmd - L1 * e1 - L2 * e2\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        update_lambda(L1, L2, lambda_optim)\n",
    "\n",
    "        average_latent = 0.9 * average_latent + 0.1 * float(mmd + elbo)\n",
    "        average_recon = 0.9 * average_recon + 0.1 * float(nll)\n",
    "\n",
    "        if n_batch % 30 == 0:\n",
    "            print('\\t'.join([\n",
    "                '\\rnll: {0:.4f}'.format(average_recon),\n",
    "                'lat: {0:.4f}'.format(average_latent),\n",
    "                'L1: {0:.3f}'.format(L1),\n",
    "                'L2: {0:.3f}'.format(L2)]),\n",
    "                end=''\n",
    "            )\n",
    "\n",
    "def update_lambda(L1, L2, optim):\n",
    "    # Updates lambda grad to maximize and clamps after update\n",
    "    L1.grad *= -1.\n",
    "    L2.grad *= -1.\n",
    "    optim.step()\n",
    "    with torch.no_grad():\n",
    "        L1.clamp_(0.001, 100)\n",
    "        L2.clamp_(0.001, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nll: 53.3741\tlat: 23.7278\tL1: 0.965\tL2: 1.0633\tEpoch 1/10\n",
      "nll: 52.2113\tlat: 22.5336\tL1: 0.905\tL2: 1.118\tEpoch 2/10\n",
      "nll: 50.5696\tlat: 23.8000\tL1: 0.845\tL2: 1.174\tEpoch 3/10\n",
      "nll: 49.7108\tlat: 25.9649\tL1: 0.785\tL2: 1.230\tEpoch 4/10\n",
      "nll: 48.7753\tlat: 25.3169\tL1: 0.724\tL2: 1.285\tEpoch 5/10\n",
      "nll: 47.9363\tlat: 23.2921\tL1: 0.664\tL2: 1.340\tEpoch 6/10\n",
      "nll: 48.1585\tlat: 25.6364\tL1: 0.604\tL2: 1.396\tEpoch 7/10\n",
      "nll: 48.5680\tlat: 24.7749\tL1: 0.544\tL2: 1.451\tEpoch 8/10\n",
      "nll: 48.0952\tlat: 29.3923\tL1: 0.484\tL2: 1.508\tEpoch 9/10\n",
      "nll: 48.0837\tlat: 24.2637\tL1: 0.424\tL2: 1.563\tEpoch 10/10\n"
     ]
    }
   ],
   "source": [
    "e = (86., 5.)\n",
    "mi = 1\n",
    "lmbda = (1., 1.)\n",
    "z_dim = 16\n",
    "\n",
    "model = make_lagVAE(z_dim, mi, e, lmbda)\n",
    "model = model.to(device)\n",
    "\n",
    "optim = torch.optim.Adam(model.child_parameters(), lr=1e-4)\n",
    "lambda_optim = torch.optim.RMSprop([model.L1, model.L2], lr=0.0001)\n",
    "\n",
    "epoch_summary_text = '\\rEpoch {0}/{1}'\n",
    "loss_summary_text = '{0} loss: {1:.5}'\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_lagVAE(model, training_loader, optim, lambda_optim, device)\n",
    "\n",
    "    #Create summary loss\n",
    "    print('\\tEpoch {0}/{1}'.format(epoch+1, epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing model parameters\n",
    "\n",
    "If $\\lambda$ is fixed, then we are no longer solving the dual problem. However, fixed choices of $\\lambda$ can correspond to other VAEs. In the code above, we can disable optimizating $\\lambda$. This effectively makes $\\lambda$ a hyperparameter that must be chosen by the user, and turns the Lagrangian VAE into an InfoVAE. However, the InfoVAE does not guarantee that our solution will be consistent according to our choice of $\\epsilon$. The benefit of the Lagrangian VAE is that by optimizing $\\lambda$, we enforce the constraints. If we have $\\mathcal{D}_i > \\epsilon_i$, then $\\lambda_i$ will grow increasingly positive until the constraint is satisfied. This forces the optimization problem to weigh the violation of the consistency more heavily.\n",
    "\n",
    "The paper goes into detail about how a number of previous VAE models can be recovered by specific choices of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_lagvae = True\n",
    "e = (86., 5.)\n",
    "mi = 1\n",
    "lmbda = (1., 1.)\n",
    "z_dim = 2\n",
    "\n",
    "dataset = 'mnist'\n",
    "lagVAE = make_lagVAE(z_dim, mi, e, lmbda, optimize_lambda=use_lagvae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model = VAE_template(lagVAE, dataset, batch_size=1000)\n",
    "\n",
    "trainer = Trainer(gpus=[0],\n",
    "                  max_nb_epochs=300,\n",
    "                  checkpoint_callback=checkpoint_callback)\n",
    "\n",
    "trainer.fit(model)\n",
    "torch.save(model.state_dict(), './models/lagvae.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Reconstruction of images\n",
    "Below are image reconstructions, similar to my [VAE notebook](./basic_VAE.ipynb). A notable difference though is that these images are much sharper. The noisy reconstructions also have greater consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "image_dir = './images/'\n",
    "image_desc = 'lagvae.png'\n",
    "title_desc = 'with LagrangianVAE'\n",
    "\n",
    "def image_plot(axis, image, **kwargs):\n",
    "    axis.imshow(image.view(28,28), cmap='Greys', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up picture\n",
    "ax_settings = {'aspect':'equal', 'xticklabels':[], 'yticklabels':[], 'xticks':[], 'yticks':[]}\n",
    "fig, ax = plt.subplots(5, 6, subplot_kw=ax_settings, figsize=(9, 7))\n",
    "if dataset.lower() == 'mnist':\n",
    "    dset = datasets.MNIST(root='./', train=True, download=False, transform=transforms.ToTensor())\n",
    "elif dataset.lower() == 'fashion':\n",
    "    dset = datasets.FashionMNIST(root='./', train=True, download=False, transform=transforms.ToTensor())\n",
    "loader = data.DataLoader(dset, batch_size=6, shuffle=False)\n",
    "image_sample, _ = next(iter(loader))\n",
    "\n",
    "\n",
    "lagVAE.cpu()\n",
    "lagVAE.eval()\n",
    "with torch.no_grad():\n",
    "    recons = lagVAE.generate_similar(image_sample, noise=False)\n",
    "    for idx in range(6):\n",
    "        image_plot(ax[0, idx], image_sample[idx,:])\n",
    "        image_plot(ax[1, idx], torch.sigmoid(recons[idx,:]))\n",
    "        \n",
    "    #Create similar examples with noise\n",
    "    for row in range(2, 5):\n",
    "        recons = lagVAE.generate_similar(image_sample)\n",
    "        for idx in range(6):\n",
    "            image_plot(ax[row, idx], torch.sigmoid(recons[idx,:]))\n",
    "            \n",
    "fig.suptitle('LagrangianVAE image reconstruction')\n",
    "plt.savefig('./images/lagvae-recon.png')\n",
    "ax[0,-1].text(30, 15, 'Original')\n",
    "ax[1,-1].text(30, 15, 'Reconstruction from mean')\n",
    "ax[2,-1].text(30, 15, 'Noisy reconstructions')\n",
    "plt.savefig('./images/MNIST-lagVAE-reconstructions.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstructions using sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 4, subplot_kw=ax_settings, figsize=(6, 6))\n",
    "with torch.no_grad():\n",
    "    for ax_row in axes:\n",
    "        for ax in ax_row:\n",
    "            latent_sample = torch.randn(1, z_dim)\n",
    "            out = lagVAE.decoder(latent_sample)\n",
    "            image_plot(ax, torch.sigmoid(out[0]))\n",
    "        \n",
    "fig.suptitle('Image generation with LagrangianVAE')\n",
    "plt.savefig('./images/MNIST-lagVAE-latent_samples.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of means\n",
    "\n",
    "Below we use Linear Discriminant Analysis as a dimension reduction tool to see the distribution of means. The image below shows us the result of the encodings on validation examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "lda = make_pipeline(StandardScaler(), LinearDiscriminantAnalysis(n_components=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_label = model.val_dataloader.dataset.dataset.classes\n",
    "\n",
    "model.eval()\n",
    "with t.no_grad():\n",
    "    means = []\n",
    "    labels = []\n",
    "    for image, label in model.val_dataloader:\n",
    "        mean, _ = lagVAE.encoder(image)\n",
    "        means.append(mean)\n",
    "        labels.append(label)\n",
    "    means = torch.cat(means)\n",
    "    labels = torch.cat(labels)\n",
    "    lda.fit(means, labels)\n",
    "    X_embed = lda.transform(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "class_labels = model.val_dataloader.dataset.dataset.classes\n",
    "scatter = ax.scatter(X_embed[:, 0], X_embed[:, 1], c=labels, s=15, cmap='tab10')\n",
    "handles, _ = scatter.legend_elements()\n",
    "legend1 = ax.legend(handles, class_labels, bbox_to_anchor=(1.05, 1), title=\"Classes\")\n",
    "ax.add_artist(legend1)\n",
    "\n",
    "plt.title('Distribution of encoded means')\n",
    "plt.savefig('./images/lagVAE-mean-distribution.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
