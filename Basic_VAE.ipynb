{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Very Brief Overview of Variational Autoencoders in pytorch (WIP)\n",
    "\n",
    "This notebook implements a Variational Autoencoder (VAE) in pytorch. VAEs are described in detail in this [tutorial](https://arxiv.org/abs/1606.05908). This is an implementation of a VAE as described in the paper, using both fully connected layers and convolution layers. There are a number of architecture changes that can be made. Of particular interest to me are the following:\n",
    "* Using the labels in order to implement a Conditional VAE (CVAE) as described in the tutorial above. This type of VAE uses extra information (the labels) as part of the encoding and decoding process.\n",
    "* Using Inverse Autoregressive Flow modules as described in [IVF](https://arxiv.org/abs/1606.04934). This allows us to model more complex distributions by providing an efficient way to have non-diagonal covariance matrices. In particular, this allows for covariance matrices that separate the data rather than give them elliptic 'islands.'\n",
    "* Using an an [adversarial model](https://arxiv.org/abs/1512.09300) to measure the difference between images. In this notebook, we use 'Binary Categorial Entropy' to measure the difference between each pixel in the image. Measuring the per-pixel difference is a low level way to measure the similarity between images. It gives us a start, but it suffers from issues that we would like to avoid. One apparent issue is that it does not align with a natural interpretation of 'similar' images. This is shown at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import relevant libraries\n",
    "#Libraries used to create network\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Libraries used to retrieve data set and load it\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, RandomAffine, ToTensor\n",
    "\n",
    "file_path = './models/{0}-{1}.pkl'\n",
    "use_gpu = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "\n",
    "The first loss function is defined by how well the image is reconstructed by the network. The more similar each pixel is in the original image is to the same pixel in the reconstructed image, the smaller this loss will be. This loss forces each image reconstruction to be similar to the input image. The `BCEWithLogitsLoss` function is more stable than using a sigmoid activation layer with BCE loss.\n",
    "\n",
    "The second loss function is the KL Divergence of the encoded mean and variance from the standard normal distribution. This imposes structure on the latent space by pushing the latent representation towards being a standard normal. The further the encoded means and variances are from a $N(0, 1)$ distribution, the more the encoding is penalized. Effectively, we are measuring how much information is lost by using $N(0, 1)$ rather than the true distribution, and we are penalized based on that. To encourage the network to encode information, we set a minimum for the KL divergence. This was noted to improve performance in the IVF paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "def KL(means, variances, minimum=0.5, epsilon=1e-8):\n",
    "    '''\n",
    "    Computes the KL divergence of N(means, variances) with N(0, 1) on a batch,\n",
    "    then returns the mean of the batch.\n",
    "\n",
    "    :param means: (torch.Tensor)\n",
    "    :param variances: (torch.Tensor)\n",
    "    :param minimum: (float, optional) Minimum value allows for each KL divergence.\n",
    "    :param epsilon: (float, optional)\n",
    "    :return: The KL divergence\n",
    "    '''\n",
    "    loss = t.sum(variances+means.pow(2)-t.log(variances+epsilon)-1,1)/2.\n",
    "    minimum = minimum*t.ones_like(loss)\n",
    "    stack = t.stack([loss, minimum])\n",
    "    return t.max(stack).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Layers\n",
    "\n",
    "This creates the latent layer and a Reshape layer. The latter is exactly as it sounds; it reshapes the input that passes through it.\n",
    "\n",
    "The former layer is what differentiates this from an autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentLayer(nn.Module):\n",
    "    '''\n",
    "    Creates a layer that takes an input and outputs a sample of latent variables. This is intended to\n",
    "    be a connecting layer, and so it has two fully connected layers - one learns the means in the\n",
    "    latent space, while the other learns the variances. The output for the variance has a softplus\n",
    "    activator as it must be positive.\n",
    "    '''\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(LatentLayer, self).__init__()\n",
    "        self.layer_mu = nn.Linear(input_size, latent_size)\n",
    "        self.layer_var = nn.Linear(input_size, latent_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Compute statistic parameters\n",
    "        mu = self.layer_mu(x)\n",
    "        var = F.softplus(self.layer_var(x))\n",
    "        #Create sample\n",
    "        samples = mu+t.randn_like(mu)*var.sqrt()\n",
    "        return samples, mu, var\n",
    "\n",
    "class ReshapeLayer(nn.Module):\n",
    "    '''\n",
    "    Reshapes the input to be the output using view. Shapes are checked\n",
    "    on forward pass to verify that they are compatible.\n",
    "\n",
    "    :param view_shape: The shape to cast the input to. Given a batch\n",
    "        input of shape (n, _) will be cast to (n, view_shape).\n",
    "    '''\n",
    "    def __init__(self, view_shape):\n",
    "        super(ReshapeLayer, self).__init__()\n",
    "        self.view_shape = view_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Reshapes x to initialized shape. Assumes that x.shape[0] is\n",
    "        the batch size.\n",
    "\n",
    "        :param x: (torch.tensor)\n",
    "        :return: (torch.tensor)\n",
    "        '''\n",
    "        output_shape = (x.shape[0],) + self.view_shape\n",
    "        assert self.dimension(x.shape) == self.dimension(output_shape), '{0} and {1} are not compatabile'.format(x.shape, output_shape)\n",
    "        return x.view(output_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def dimension(shape):\n",
    "        #Helper function for checking dimensions\n",
    "        out = 1\n",
    "        for s in shape:\n",
    "            out *= s\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Class\n",
    "\n",
    "This implements the class itself. In particular, we have an encoder network, a latent layer, and a decoder network. The former "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    '''\n",
    "    Creates a Variation Autoencoder in Torch.\n",
    "    '''\n",
    "    def __init__(self, encoder_network, latent_layer, decoder_network):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.add_module('encoder', encoder_network)\n",
    "        self.add_module('latent', latent_layer)\n",
    "        self.add_module('decoder', decoder_network)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Passes forward through encoder to create a sample latent\n",
    "        representation, then decodes that representation.\n",
    "        \n",
    "        :param x: (torch.Tensor) \n",
    "        :return: (torch.Tensor) Reconstruction of x and it's representation in the latent space\n",
    "        '''\n",
    "        #encode\n",
    "        x = self.encoder(x)\n",
    "        #Sample from a normal distribution with mean mu and variance var\n",
    "        z, mu, var = self.latent(x)\n",
    "        #Decode and return latent variables\n",
    "        return self.decoder(z), (mu, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "Here we use the torch tools to load the MNIST (or FashionMNIST) dataset. The `compose_train` are the transformations we want to apply to the images before they are used by the model. ToTensor converts the PIL image (with values between $[0, 255]$) to a tensor (with values between $[0, 1]$). For both, we use RandomAffine, which does a random rigid movement. For the FashionMNIST data, we also use a RandomHorizontalFlip transformation. These transformations are done to increase the number of examples, which has a positive effect on fitting.\n",
    "\n",
    "We also call DataLoader, which handles batching and shuffling our data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_size = 128\n",
    "compose_train = Compose([RandomAffine(5, (0.1, 0.1)), ToTensor()])\n",
    "training_set = datasets.MNIST(root='./', train=True, download=True, transform=compose_train)\n",
    "train_loader = t.utils.data.DataLoader(dataset=training_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testing_set = datasets.MNIST(root='./', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "data_name = 'MNIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "compose_train = Compose([RandomAffine(1, (0.1, 0.1)), RandomHorizontalFlip(), ToTensor()])\n",
    "\n",
    "training_set = datasets.FashionMNIST(root='./', train=True, download=True, transform=compose_train)\n",
    "train_loader = t.utils.data.DataLoader(dataset=training_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testing_set = datasets.FashionMNIST(root='./', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "data_name = 'Fashion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "A basic variational autoencoder can be broken down into three modules: the encoder network, the latent layer, and the decoder network. \n",
    "\n",
    "To build the networks, we use the Sequential object to build both the encoder and the decoder. To fit them together, we use the latent layer and the VAE class defined earlier. Each network uses SELU activation functions (see [here](https://arxiv.org/abs/1706.02515)).\n",
    "\n",
    "The sizes of the hidden layers are chosen after some experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We name the network here for convenience.\n",
    "model_name = 'flat_vae_model'\n",
    "latent_dim = 32\n",
    "\n",
    "#Build networks\n",
    "encoder_net = nn.Sequential(\n",
    "    ReshapeLayer((28**2,)),\n",
    "    nn.utils.weight_norm(nn.Linear(28**2, 256)),\n",
    "    nn.SELU(),\n",
    "    nn.utils.weight_norm(nn.Linear(256, 128)),\n",
    "    nn.SELU(),\n",
    "    nn.utils.weight_norm(nn.Linear(128, 64)),\n",
    "    nn.SELU(),\n",
    ")\n",
    "\n",
    "latent_net = LatentLayer(64, latent_dim)\n",
    "\n",
    "decoder_net = nn.Sequential(\n",
    "    nn.utils.weight_norm(nn.Linear(latent_dim, 64)),\n",
    "    nn.SELU(),\n",
    "    nn.utils.weight_norm(nn.Linear(64, 128)),\n",
    "    nn.SELU(),\n",
    "    nn.utils.weight_norm(nn.Linear(128, 256)),\n",
    "    nn.SELU(),\n",
    "    nn.utils.weight_norm(nn.Linear(256, 28**2)),\n",
    "    ReshapeLayer((1, 28, 28)),\n",
    ")\n",
    "\n",
    "model = VAE(encoder_net, latent_net, decoder_net)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#We name the network here for convenience.\n",
    "model_name = 'conv_vae_model'\n",
    "#Build networks\n",
    "latent_dim = 32\n",
    "final_size = 6\n",
    "filters = 32\n",
    "\n",
    "encoder_net = nn.Sequential(\n",
    "    nn.Conv2d(1, filters, 5, padding=2),\n",
    "    nn.MaxPool2d(3, 2),\n",
    "    nn.SELU(),\n",
    "    nn.Conv2d(filters, 2*filters, 5, padding=2),\n",
    "    nn.MaxPool2d(3, 2),\n",
    "    nn.SELU(),\n",
    "    ReshapeLayer((2*filters*final_size**2,)),\n",
    ")\n",
    "\n",
    "latent_net = LatentLayer(2*filters*final_size**2, latent_dim)\n",
    "\n",
    "decoder_net = nn.Sequential(\n",
    "    nn.Linear(latent_dim, (2*filters)*(final_size**2)),\n",
    "    ReshapeLayer((2*filters, final_size, final_size)),\n",
    "    nn.SELU(),\n",
    "    nn.ConvTranspose2d(2*filters, 2*filters, 5),\n",
    "    nn.Upsample(scale_factor=1.5),\n",
    "    nn.SELU(),\n",
    "    nn.ConvTranspose2d(2*filters, filters, 5),\n",
    "    nn.Upsample(scale_factor=1.5),\n",
    "    nn.SELU(),\n",
    "    nn.ConvTranspose2d(filters, filters, 5),\n",
    "    nn.SELU(),\n",
    "    nn.Conv2d(filters, 1, 5),\n",
    ")\n",
    "\n",
    "model = VAE(encoder_net, latent_net, decoder_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The goal of training is two-fold: minimize reconstruction error and minimize information loss as measured by KL divergence. Because of this, we train the model to optimize over these two conditions. The `Adam` algorithm is used to optimize the model.\n",
    "\n",
    "The model is called in training mode explicitely using `model.train()` and then moved to the gpu using `model.cuda(0)`. To make it run on the cpu, set `use_gpu` to False at the start. If the model is going to be moved to the gpu, it should be done before loading the parameters into the optimizer.\n",
    "\n",
    "To train the model, we iterate through the data set. We have to send the inputs to the gpu before we can pass it through the model. We keep a running average of the loss to keep us informed on progress. The last three lines of the inner loop are where the training happens in pytorch. `optimizer.zero_grad()` zeros out all derivatives. `loss.backward()` uses autodifferentiation to compute derivatives starting at loss. `optimizer.step()` optimizes the model parameters based on the derivatives from the previous step.\n",
    "\n",
    "If the model has a pretrained model file available, then that can be loaded by using the second code block instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model, data_loader, optimizer, use_gpu=True):\n",
    "    average_recon = 0\n",
    "    average_latent = 0\n",
    "    for n_batch, (x, _) in enumerate(data_loader):\n",
    "        if use_gpu:\n",
    "            x = x.cuda()\n",
    "        \n",
    "        x_pred, (mu, var) = model(x)\n",
    "        \n",
    "        #compute loss\n",
    "        latent_loss = KL(mu, var)\n",
    "        recon_loss = bce(x_pred, x)/batch_size\n",
    "        loss = recon_loss+latent_loss\n",
    "        \n",
    "        #Compute average\n",
    "        average_latent += (latent_loss-average_latent)/(n_batch+1)\n",
    "        average_recon += (recon_loss-average_recon)/(n_batch+1)\n",
    "\n",
    "        #Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Print percent complete\n",
    "        print(percent_complete.format(batch_size*n_batch/n_data), end='')\n",
    "        \n",
    "    return average_recon, average_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rate = 1e-4\n",
    "model.train()\n",
    "\n",
    "#Add to gpu before making the optimizer\n",
    "if use_gpu:\n",
    "    model.cuda(0)\n",
    "\n",
    "optimizer = t.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epoch_summary_text = '\\rEpoch {0}/{1}'\n",
    "loss_summary_text = '{0} loss: {1:.5}'\n",
    "\n",
    "percent_complete = '\\r{0:.1%}'\n",
    "\n",
    "n_data = len(training_set)\n",
    "for epoch in range(epochs):\n",
    "    recon_loss, latent_loss = train_vae(model, train_loader, optimizer)\n",
    "\n",
    "    #Create summary loss\n",
    "    epoch_summary = epoch_summary_text.format(epoch+1, epochs)\n",
    "    latent_summary = loss_summary_text.format('latent', latent_loss)\n",
    "    recon_summary = loss_summary_text.format('recon', recon_loss)\n",
    "    total_summary = loss_summary_text.format('total', latent_loss+recon_loss)\n",
    "    print(epoch_summary, recon_summary, latent_summary, total_summary, sep=' - ')\n",
    "    \n",
    "t.save(model.state_dict(), file_path.format(data_name, model_name))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.load_state_dict(t.load(file_path.format(data_name, model_name), map_location='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generation\n",
    "\n",
    "## Image Reconstruction\n",
    "\n",
    "A VAE can be used to reconstruct examples. This is shown below. The images in the top row are the originals, while the images in the bottom row are the reconstructions. This VAE tends to blur examples together. That is, images tend towards their most generic, with colors and edges being blurred. There are a few ways to approach fixing this, which are mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "from random import sample\n",
    "\n",
    "def image_plot(axis, image, **kwargs):\n",
    "    axis.imshow(image.view(28,28), cmap='Greys', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up picture\n",
    "n_images = 11\n",
    "ax_settings = {'aspect':'equal', 'xticklabels':[], 'yticklabels':[], 'xticks':[], 'yticks':[]}\n",
    "fig, ax = plt.subplots(2, n_images, subplot_kw=ax_settings, figsize=(n_images*3//2, 3))\n",
    "\n",
    "\n",
    "model.cpu()\n",
    "model.eval()\n",
    "with t.no_grad():\n",
    "    image_sample = sample(list(testing_set), n_images)\n",
    "    #Find the mean in latent space\n",
    "    for index, (image, _) in enumerate(image_sample):\n",
    "        image = image.view((1,)+image.shape)\n",
    "        recon_image, _ = model(image)\n",
    "\n",
    "        image_plot(ax[0, index], image)\n",
    "        image_plot(ax[1, index], t.sigmoid(recon_image))\n",
    "        \n",
    "plt.savefig('./images/{0}-{1}-reconstructions.png'.format(data_name, model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Sampling\n",
    "\n",
    "My interest in VAE are mostly tied to the fact that you can use them to generate new images. In this portion, we sample a random variable and use that to create a new image. That is, once a network is trained, we can use it to create new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 15\n",
    "fig, ax = plt.subplots(1, n_images, subplot_kw=ax_settings, figsize=(n_images*3/2, 3/2))\n",
    "with t.no_grad():\n",
    "    for index in range(n_images):\n",
    "        latent_sample = t.randn(1, latent_dim)\n",
    "        image = model.decoder(latent_sample)\n",
    "        image_plot(ax[index], t.sigmoid(image))\n",
    "        \n",
    "plt.savefig('./images/{0}-{1}-latent_samples.png'.format(data_name, model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of examples in the latent space\n",
    "\n",
    "Below we can see a scatter plot of the means of each example. The groups tend towards being distributed in an elliptic manner. This is due to our diagonal assumption on variance. Using IVF can improve the distribution of the examples.\n",
    "\n",
    "Note: The plot is misleading unless the latent dimension is equal to 2. To get around this difficulty, you can use a dimension reduction technique (e.g. t-SNE, umap) to reduce to two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "n_images = 0\n",
    "\n",
    "outputs = t.zeros((10, 2, n_images))\n",
    "with t.no_grad():\n",
    "    for index in range(n_images):\n",
    "        image, label = testing_set[index]\n",
    "        _, (mean, _) = model(image)\n",
    "        outputs[label, 0, index] = mean[0][0]\n",
    "        outputs[label, 1, index] = mean[0][1]\n",
    "        \n",
    "    for index, label in enumerate(testing_set.classes):\n",
    "        ax.scatter(outputs[index, 0, :], outputs[index, 1, :], label=label)\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A: The problem with per-pixel difference measures\n",
    "\n",
    "![](./images/per-pixel-difference.png)\n",
    "\n",
    "The image above demonstrates why using a per-pixel similarity measurement is undesirable. We have three images. One is nearly identical (made by shifting every pixel of the original up by 1). Another is similar, but blurred. The last is blank. I would intuitively say the first image is the most similar to the original. However, it is actually the least similar as measured per-pixel. The image that is the most similar is actually the blank image! The problem is that the similarity measurement we are using does not look at any feature of the image. Instead, it pushes us towards recreating each image exactly. This leads to blurred images as we hedge towards generic images rather than sharp images. \n",
    "\n",
    "This problem can be alleviated by using an adversarial model. The adversarial model creates a separate network that is tasked with determining the original. This encourages the generator network to produce sharper, more realistic images. In essence, it is measuring the difference between the images by looking at features rather than at pixels. \n",
    "\n",
    "The code below will create an image like the above. However, the reconstruction will vary due to the stochastic nature of the model. Because of this, you may end up with a higher or lower loss than image 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick example image\n",
    "image = testing_set[index][0]\n",
    "\n",
    "#Naming it for convenience\n",
    "dissimilarity = lambda x, t: F.binary_cross_entropy(x, t, reduction='sum')\n",
    "\n",
    "#Create a blank image\n",
    "blank_image = t.zeros_like(image)\n",
    "\n",
    "#Create reconstruction image\n",
    "with t.no_grad():\n",
    "    recon_image, _ = model(image)\n",
    "    recon_image = t.sigmoid(recon_image[0])\n",
    "\n",
    "#Create shifted image\n",
    "shifted_image = t.zeros_like(image)\n",
    "shifted_image[0,:-1,:] = image[0,1:,:]\n",
    "\n",
    "#Measure dissimilarities\n",
    "blank_loss = dissimilarity(image, blank_image)\n",
    "recon_loss = dissimilarity(image, recon_image)\n",
    "shifted_loss = dissimilarity(image, shifted_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create subplot\n",
    "fig, ax = plt.subplots(1, 4, subplot_kw=ax_settings, figsize=(7, 2))\n",
    "\n",
    "#Plot and label images.\n",
    "image_plot(ax[0], image)\n",
    "ax[0].set_title('Original')\n",
    "\n",
    "image_plot(ax[1], shifted_image)\n",
    "ax[1].set_title('Image 1')\n",
    "\n",
    "image_plot(ax[2], recon_image)\n",
    "ax[2].set_title('Image 2')\n",
    "\n",
    "image_plot(ax[3], blank_image)\n",
    "ax[3].set_title('Image 3')\n",
    "\n",
    "fig.suptitle('Per-pixel differences for various images')\n",
    "\n",
    "\n",
    "loss_text = 'Difference between original and image {0}: {1:.5}\\n'\n",
    "shifted_text = loss_text.format(1, shifted_loss)\n",
    "recon_text = loss_text.format(2, recon_loss)\n",
    "blank_text = loss_text.format(3, blank_loss)\n",
    "ax[0].text(0, 32, shifted_text+recon_text+blank_text, verticalalignment='top')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
