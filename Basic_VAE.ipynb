{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Very Brief Look of Variational Autoencoders\n",
    "\n",
    "## Summary\n",
    "This notebook describes and implements a (basic) Variational Autoencoder (VAE) in pytorch. VAEs are a type of generative model. By providing a dataset, a variational autoencoder uses variational Bayesian methods to generate similar examples. Once trained, it can be used to turn noise into images.\n",
    "\n",
    "These notes are based off this [VAE tutorial](https://arxiv.org/abs/1606.05908) and the explanation provided in [these notes](https://deepgenerativemodels.github.io/notes/vae/). The target audience for this notebook is someone who has read about VAEs but want to see another explanation or an implementation. Further reading is provided below. If you are just interested in the implementation, you can skip the first half of this notebook.\n",
    "\n",
    "Finally, it should be known that $\\simeq$ is used below to denote that two values are equal up to a constant difference. Specifically, if $a \\simeq b$, then $a = b + \\text{constant}$. This is used as constants are ignored during optimization. This is not to be confused with $a \\approx b$, which means that a is approximately b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Objective\n",
    "Suppose we have a set of observations $D = \\{x_1, \\dots, x_N\\}$. We want to generate examples similar to the observations in $D$, but how do we start? One approach is to find a probability distribution $p$ such that $p(x)$ is high for $x$ similar to an observation in $D$, and low for dissimilar observations. However, if the observations are high dimensional, then the dependencies between dimensions can be complex. Untangling these dependencies can make finding $p$ difficult. To alleviate this, we can introduce a latent variable.\n",
    "\n",
    "A latent variable is a variable that is inferred from the observation. In the right situations, it encodes high level information that can be used to recreate the observation. Introducing this latent variable means that we are now looking for $p(x, z)$, where $z$ is our latent variable. We can break down this distribution into $p(x|z)p_\\ell(z)$. The benefit is that this gives us a generative process: we can first generate a $z$ according to $p_\\ell$ and then use $p(x|z)$ to generate an $x$.\n",
    "\n",
    "At this point, we can always recover $p(x)$ using integration as we have\n",
    "\n",
    "$$p(x) = \\int_\\mathcal{Z} p(x|z)p_\\ell(z)dz,$$\n",
    "\n",
    "where $\\mathcal{Z}$ is the space of all possible latent variables. However, this integral is generally intractable. This difficulty is important as our goal is to make $p$ similar to $p_D$. Instead of optimizing $p$ directly, we will use variational Bayesian methods.\n",
    "\n",
    "Variational Bayesian methods are aimed at approximation the posterior. We are attempting to approximate $p(z|x)$ with another distribution $q_\\lambda(z)$, where $\\lambda = \\lambda(x)$. We are forced to use an approximation as $p(z|x)$ is just as difficult to evaluate as $p(x)$. However, if we had the posterior $p(z|x)$, then we could find $p(x)$ using \n",
    "\n",
    "$$p(x) = \\frac{p(x, z)}{p(z|x)} \\approx \\frac{p(x, z)}{q_\\lambda(z)}.$$\n",
    "\n",
    "The benefit of introducing this is that we now have\n",
    "\n",
    "$$\\log p(x) \n",
    "\\geq \\int_\\mathcal{Z} q_\\lambda(z) \\log\\left(\\frac{p(x, z)}{q_\\lambda(z)}\\right) dz\n",
    "= \\mathbb{E}_{q_\\lambda}\\left[\\log(p(x, z)) - \\log(q_\\lambda(z))\\right].$$\n",
    "\n",
    "This lower bound (called the Evidence Lower Bound, or ELBO) is possible to approximate with an unbiased Monte Carlo estimator when $q$ is simple to sample from. (Big note: this estimate uses Jensen's inequality. See either tutorial above for a derivation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoders\n",
    "### The probabilistic model\n",
    "Variational autoencoders are a type of latent variable model where we use a network to learn the distributions. The basic variational autoencoder was defined with specific distribution choices.\n",
    "\n",
    "The latent distribution $p_\\ell$ is chosen to be the standard normal $N(0, I)$. In this situation, we do not optimize over $p_\\ell$ as it does not change. This builds in the assumption that our latent variables are normally distributed.\n",
    "\n",
    "The posterior $q_\\lambda$ is a normal distribution. That is, $\\lambda(x) = (\\mu(x), \\Sigma(x))$ and $q_\\lambda = N(\\mu(x), \\Sigma(x))$. Generally $\\Sigma$ is restricted to be diagonal. This results in a factorized Gaussian (where each dimension is statistically independent). In this case, if $\\Sigma_{ii} = \\sigma_i^2$ (so that $\\sigma_i$ is the scale in the $i$th coordinate), then we specifically have\n",
    "\n",
    "$$-\\log q(z|x, \\theta) \n",
    "\\simeq \\sum_i \\left(\\frac{(x-\\mu_i)^2}{2\\sigma_i^2}+\\log \\sigma_i\\right).$$\n",
    "\n",
    "The distribution $p(x|z)$ is typically chosen to be either a normal distribution or a Bernoulli distribution, depending on the target data. We will use $x \\sim N(\\mu(z), I)$, so that we have that\n",
    "\n",
    "$$-\\log p(x|z) \\simeq \\tfrac{1}{2}\\Vert x-\\mu(z) \\Vert^2_2,$$\n",
    "\n",
    "which is a multiple of the mean squared error. If we chose $p(x|z)$ to be Bernoulli, then we would have binary cross entropy instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network\n",
    "To turn this into a variational autoencoder, we need to introduce some networks. In this situation, that means we use a network to learn the parameters for the distributions $p(x|z)$ and $q_\\lambda(z)$. In practice, we separate this into two modules that are both parameterized by $\\theta$.\n",
    "\n",
    "The first is the encoder $f_\\theta$. The encoder takes an observation $x$ and outputs statistical parameters used to determine the latent variable. This is done using\n",
    "\n",
    "$$f_\\theta(x) = \\lambda(x) = (\\mu_\\theta(x), \\Sigma_\\theta(x)).$$\n",
    "\n",
    "These parameters are then used for $q_\\lambda(z)$. An important note is that if we draw $z \\sim q_\\lambda$, then we have no way to recover the parameters $\\lambda$ for back propogation. To avoid this, we use the 'reparamiterization trick' and define $z$ as a transformation of a random variable. This means we draw $\\epsilon \\sim N(0, I)$ and let $z = \\mu+\\Sigma^{1/2}\\epsilon$, where $\\Sigma^{1/2}$ is the Choletsky factorization of $\\Sigma$ (effectively giving us a square root of the matrix). This is equivalent to sampling from $N(\\mu, \\Sigma)$ but allows us to differentiate $z$ (w.r.t. $\\theta$). The variable $\\epsilon$ can be used in our log probability to instead give us\n",
    "\n",
    "$$\\log q_\\lambda(z) \\simeq -\\sum_i \\left(\\tfrac{1}{2}\\epsilon_i^2+\\log \\sigma_i\\right).$$\n",
    "\n",
    "The second module is the decoder $g_\\theta$. The decoder turns a latent variable $z$ and outputs the statistical parameter $\\mu(z)$. More explicitely, we now have that\n",
    "\n",
    "$$-\\log p(x|z) \\simeq \\tfrac{1}{2}\\Vert x-g_\\theta(z) \\Vert^2_2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final form of the loss function\n",
    "With the choices above, we are now ready to write our loss function. We are trying to maximize $\\log p(x)$ for $x \\in D$. As Torch works by minimizing, this means we are going to instead minimize $-\\log p(x)$. We then replace this with the evidence lower bound to change goals to minimizing $-\\mathbb{E}_{q_\\lambda}\\left[\\log(p(x, z)) - \\log(q_{\\lambda}(z))\\right]$. If we expand the first term in the expectation, we have\n",
    "\n",
    "$$-\\log p(x, z) \n",
    "= -\\log (p(x|z)p_\\ell(z)) \n",
    "= - \\log p(x|z) - \\log p_\\ell(z) \n",
    "\\simeq \\tfrac{1}{2}\\Vert x-g_\\theta(z) \\Vert^2_2+\\tfrac{1}{2}\\Vert z \\Vert^2_2.$$\n",
    "\n",
    "This gives us a final loss function of \n",
    "$$-\\mathbb{E}_{q_\\lambda}\\left[\\log(p(x, z)) - \\log(q_\\lambda(z))\\right]\n",
    "= \\mathbb{E}_{q_\\lambda}\\left[\\log(q_\\lambda(z))-\\log(p(x, z))\\right] \n",
    "= \\mathbb{E}_{q_\\lambda}\\left[\\tfrac{1}{2}\\Vert x-g_\\theta(z) \\Vert^2_2 + \\tfrac{1}{2}\\Vert z \\Vert^2_2 -\\sum_i \\left(\\tfrac{1}{2}\\epsilon_i^2+\\log \\sigma_i\\right)\\right].$$\n",
    "\n",
    "We compute this last quantity using Monte Carlo methods, meaning we approximate it by using a number of samples.\n",
    "\n",
    "It's worth noting what exactly we are getting when we optimize using this bound. The $\\epsilon$ term can be ignored, as it's not subject to our parameters. The first term, $\\Vert x - g_\\theta(z) \\Vert^2_2$, is optimized by choosing parameters for the decoder that produce accurate reconstructions. The term $\\Vert z \\Vert^2_2$ is optimized by making the latent variable small; this is most directly done by decreasing the mean. The final term, $-\\sum_i \\log \\sigma_i$, has more flexibility as it's able to take both positive and negative values. If $\\sigma_i$ is small, then $z$ has little variance, but $-\\log\\sigma_i$ will be large. On the other hand, if $\\sigma_i$ is large, then $z$ has a lot of variance (which makes the decoder's job harder) but $-\\log\\sigma_i$ is now a negative value. In essence, the scale parameter can reduce the loss if chosen to be large, but this is only feasible if the decoder can still make accurate representations. Meanwhile, the entire expectation can be made smaller by adjusting $q_\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternate loss function\n",
    "An alternate choice for lower bound uses the [Kullback-Leibler (KL) divergence](http://mathworld.wolfram.com/Kullback-LeiblerDistance.html), $\\text{KL}(q||p_\\ell)$. With $q_\\lambda$ and $p_\\ell$ as chosen above, the divergence term can be computed analytically as\n",
    "\n",
    "$$\\text{KL}(q_\\lambda || p_\\ell) \\simeq \\tfrac{1}{2}\\sum_i (\\sigma_i^2 + \\mu_i^2 - \\log \\sigma_i^2 - 1).$$\n",
    "\n",
    "Using this with a variant evidence lower bound, we have\n",
    "\n",
    "$$-\\log p(x)\n",
    "\\leq -\\mathbb{E}_{q_\\lambda}\\left[\\log(p(x|z))\\right] + \\text{KL}(q_\\lambda||p_\\ell) \n",
    "\\simeq \\mathbb{E}_{q_\\lambda}\\left[\\tfrac{1}{2}\\Vert x - g_\\theta(z) \\Vert^2_2 \\right] + \\tfrac{1}{2}\\sum_i (\\sigma_i^2 + \\mu_i^2 - \\log \\sigma_i^2 - 1).$$\n",
    "\n",
    "While this form of loss can be useful, it turns out that the KL loss is too restrictive in some cases. Specifically, we have that if $q_\\lambda(z) = p(z|x)$, then the Monte Carlo loss above will be $0$, but the KL loss here will still have a constant entropy. Thus, the Monte Carlo error above should be used if we suspect that we can learn $\\lambda$ well enough that $q_\\lambda(z) \\approx p(z|x)$. This is explained in [Sticking the Landing](https://arxiv.org/abs/1703.09194) by Roeder et al, which also notes that the MC form is easier to apply when using normalizing flows.\n",
    "\n",
    "Similar to above, the result of optimization on the individual components can be interpreted for this loss. The new term, the KL divergence, is minimized when $q_\\lambda$ has the same distribution $p_\\ell$. Thus, the KL term pushes the posterior distribution towards being like the latent distribution. It's worth noting that this is not necessarily desirable, as $q_\\lambda$ is supposed to approximate $p(z|x)$. More directly, it's encouraging the encoded mean to be close to $0$ and encouraging $\\sigma_i$ to be close to 1 (so that $\\sigma_i^2-1+\\log \\sigma_i = 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "Below is the code for the network. You can run it in the notebook with (probably) few modifications needed. You can also copy the code into a python file. The necessary packages are pytorch, torchvision for the computations, and matplotlib for the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "#### Statistics Layer\n",
    "\n",
    "The statistics layer outputs the mean and scale (i.e. standard deviation) that will be used to generate the latent variables. It returns the latent variable and the associated loss. Below both the KL and fully Monte Carlo loss are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentLayer(nn.Module):\n",
    "    '''\n",
    "    Creates a layer that takes an input and outputs latent parameters.\n",
    "    :param input_size: (int) Size of input\n",
    "    :param latent_size: (int) Number of latent dimensions\n",
    "    '''\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(LatentLayer, self).__init__()\n",
    "        self.mean = nn.Linear(input_size, latent_size)\n",
    "        self.variance = nn.Linear(input_size, latent_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Encodes input to the latent statistics.\n",
    "        :param input: (t.Tensor)\n",
    "        :return: (t.Tensor, t.Tensor) Statistical parameters\n",
    "        '''\n",
    "        mean = self.mean(input)\n",
    "        log_var = self.variance(input)\n",
    "        scale = (log_var/2).exp()\n",
    "        return mean, scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAE Class\n",
    "\n",
    "This implements the VAE network. It's made up of two separate modules: an encoder network, and a decoder network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    '''\n",
    "    Creates a Variation Autoencoder\n",
    "    \n",
    "    :param encoder_network: (nn.Module)\n",
    "    :param decoder_network: (nn.Module)\n",
    "    :param loss: (str, opt) Either 'kl' or 'mc'. Sets how to calculate loss.\n",
    "    '''\n",
    "    def __init__(self, encoder_network, decoder_network, loss='mc'):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.add_module('encode', encoder_network)\n",
    "        self.add_module('generate', decoder_network)\n",
    "        self.loss = loss.lower()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Passes forward through encoder to create a sample latent\n",
    "        representation, then decodes that representation.\n",
    "        \n",
    "        :param input: (t.Tensor) \n",
    "        :return: (t.Tensor) Reconstruction of x and it's latent loss\n",
    "        '''\n",
    "        mean, scale = self.encode(input)\n",
    "        eps = t.randn_like(mean)\n",
    "        z = mean+scale*eps\n",
    "        if self.training:\n",
    "            if self.loss == 'mc':\n",
    "                loss = self.MC(z, eps, scale)\n",
    "            elif self.loss == 'kl':\n",
    "                loss = -self.KL(mean, scale)\n",
    "        else:\n",
    "            loss = t.tensor(0)\n",
    "        \n",
    "        return self.generate(z), loss\n",
    "\n",
    "    def generate_similar(self, input, noise=True):\n",
    "        '''\n",
    "        Creates similar examples by encoding the observation and then decoding the \n",
    "        latent observation. If noise is True, then it uses the latent variable\n",
    "        z = mean+eps*scale. Otherwise, z = mean.\n",
    "\n",
    "        :param input: (t.Tensor)\n",
    "        :param noise: (Bool, opt)\n",
    "        :return: (torch.Tensor) Reconstruction of input\n",
    "        '''\n",
    "        mean, scale = self.encode(input)\n",
    "\n",
    "        if noise:\n",
    "            eps = t.randn_like(mean)\n",
    "            z = mean+scale*eps\n",
    "        else:\n",
    "            z = mean\n",
    "\n",
    "        return self.generate(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def KL(mean, scale):\n",
    "        '''\n",
    "        Computes the KL divergence of a normal distribution with the parameters above\n",
    "        from N(0, 1) on a batch and then returns the mean.\n",
    "\n",
    "        :param means: (torch.Tensor)\n",
    "        :param scale: (torch.Tensor)\n",
    "        :return: The KL divergence\n",
    "        '''\n",
    "        variance = scale.pow(2)\n",
    "        loss = t.sum(variance+means.pow(2)-t.log(variance)-1, 1)/2.\n",
    "        #Return batch average\n",
    "        return loss.mean()\n",
    "    \n",
    "    @staticmethod\n",
    "    def MC(z, eps, scale):\n",
    "        '''\n",
    "        Computes fully monte carlo loss for the statistics layer.\n",
    "        \n",
    "        :param z: (t.Tensor) latent variable\n",
    "        :param eps: (t.Tensor) random noise\n",
    "        :param scale: (t.Tensor) scale (or standard deviation)\n",
    "        '''     \n",
    "        loss = t.sum(z.pow(2)/2-(eps.pow(2)/2+scale.log()), 1)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility layer\n",
    "This layer reshapes the input. This is done for convenience when using `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer(nn.Module):\n",
    "    '''\n",
    "    Reshapes the input to be the output using view. Shapes are checked\n",
    "    on forward pass to verify that they are compatible.\n",
    "\n",
    "    :param view_shape: The shape to cast the input to. Given a batch\n",
    "        input of shape (n, _) will be cast to (n, view_shape).\n",
    "    '''\n",
    "    def __init__(self, view_shape):\n",
    "        super(ReshapeLayer, self).__init__()\n",
    "        self.view_shape = view_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Reshapes x to initialized shape.\n",
    "        :param x: (t.Tensor)\n",
    "        :return: (t.Tensor)\n",
    "        '''\n",
    "        output_shape = (x.shape[0],) + self.view_shape\n",
    "        assert self.dimension(x.shape) == self.dimension(output_shape), \\\n",
    "            '{0} and {1} are not compatabile'.format(x.shape, output_shape)\n",
    "        return x.view(output_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def dimension(shape):\n",
    "        #Helper function for checking dimensions\n",
    "        out = 1\n",
    "        for s in shape:\n",
    "            out *= s\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network\n",
    "\n",
    "Here we build the encoder and decoder. This is done using the Sequential function. The encoder network needs to end with the statistics layer. The decoder is just a standard decoder. The size of the hidden layers are chosen experimentally.\n",
    "\n",
    "The encoder and decoder networks are based on the Deep Convolution GAN architecture. The images are encoded using strided convolutions with dropout, batch normalization and Leaky ReLU units. The latent encoding is then decoded using strided convolution tranpositions with dropout, batch normalization and ReLU units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "filters = 16\n",
    "drop_prob = 0.05\n",
    "\n",
    "#Build networks\n",
    "encoder_net = nn.Sequential(\n",
    "    nn.Dropout2d(p=drop_prob),\n",
    "    nn.Conv2d(1, filters, 4, 2),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.BatchNorm2d(filters),\n",
    "    nn.Dropout2d(p=drop_prob),\n",
    "    nn.Conv2d(filters, 2*filters, 3, 2),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.BatchNorm2d(2*filters),\n",
    "    nn.Dropout2d(p=drop_prob),\n",
    "    nn.Conv2d(2*filters, 4*filters, 4),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.BatchNorm2d(4*filters),\n",
    "    nn.Dropout2d(p=drop_prob),\n",
    "    nn.Conv2d(4*filters, 8*filters, 3),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Flatten(),\n",
    "    LatentLayer(8*filters, latent_dim)\n",
    ")\n",
    "\n",
    "decoder_net = nn.Sequential(\n",
    "    nn.Linear(latent_dim, 8*filters),\n",
    "    ReshapeLayer((8*filters, 1, 1)),\n",
    "    nn.BatchNorm2d(8*filters),\n",
    "    nn.Dropout2d(p=drop_prob),\n",
    "    nn.ConvTranspose2d(8*filters, 4*filters, 3),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(4*filters),\n",
    "    nn.Dropout2d(p=drop_prob),\n",
    "    nn.ConvTranspose2d(4*filters, 2*filters, 4),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(2*filters),\n",
    "    nn.Dropout2d(p=drop_prob),\n",
    "    nn.ConvTranspose2d(2*filters, filters, 3, 2),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(filters),\n",
    "    nn.Dropout2d(p=drop_prob),\n",
    "    nn.ConvTranspose2d(filters, 1, 4, 2),\n",
    "    nn.Tanh(),\n",
    ")\n",
    "\n",
    "model = VAE(encoder_net, decoder_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "For this notebook, we use the MNIST dataset. A drop-in replacement that can be used is the FashionMNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from torchvision.utils import make_grid, save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the normalize changes the tensors so that the values are distributed between [-1, 1]\n",
    "transforms = Compose([ToTensor(), Normalize([0.5], [0.5])])\n",
    "training_set = datasets.MNIST(root='./data/', train=True, download=False, transform=transforms)\n",
    "batch_size = 100\n",
    "training_loader = data.DataLoader(dataset=training_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the network parameters\n",
    "\n",
    "Now that we have the network, we need to find parameters that minimizes our estimate above. In order to accomplish this, we run the model over a batch of observations, and then compute the loss. Using stochastic gradient descent methods, we can then find a change of parameters that will (usually) decrease the loss.\n",
    "\n",
    "More explicitely, the encoder network takes in an observation $x$ and produces statistics, $(\\mu, \\Sigma)$, for the latent variable, $z$. Using the Monte Carlo loss or KL divergence above, we can determine a loss for the encoder. We then use $z$ to generate a reconstruction of the observation, $\\hat{x}$. By comparing $x$ and $\\hat{x}$, we have a loss for the decoder. The total loss of the variational autoencoder is the sum of these two losses. Because we are minimizing the sum, there is no guarantee that both components will decrease. In fact, the encoder loss typically increases while the decoder loss typically decreases. This is seen in the training summary printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(input, target):\n",
    "    loss = (input-target).pow(2)/2\n",
    "    #Return batch average\n",
    "    return loss.mean(0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model, data_loader, optimizer, use_gpu=True):\n",
    "    average_recon = 0\n",
    "    average_latent = 0\n",
    "    for n_batch, (x, _) in enumerate(data_loader):\n",
    "        if use_gpu:\n",
    "            x = x.cuda()\n",
    "        \n",
    "        x_pred, latent_loss = model(x)\n",
    "        \n",
    "        #compute -log p(x|z)\n",
    "        recon_loss = mse_loss(x, x_pred)\n",
    "        loss = recon_loss+latent_loss\n",
    "        \n",
    "        #Compute average\n",
    "        average_latent += (latent_loss-average_latent)/(n_batch+1)\n",
    "        average_recon += (recon_loss-average_recon)/(n_batch+1)\n",
    "\n",
    "        #Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Print percent complete\n",
    "        print(percent_complete.format(batch_size*n_batch/n_data), end='')\n",
    "        \n",
    "    return average_recon, average_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - recon loss: 187.08 - latent loss: 10.036 - total loss: 197.12\n",
      "Epoch 2/50 - recon loss: 76.822 - latent loss: 15.47 - total loss: 92.292\n",
      "Epoch 3/50 - recon loss: 57.278 - latent loss: 17.006 - total loss: 74.284\n",
      "Epoch 4/50 - recon loss: 49.34 - latent loss: 17.535 - total loss: 66.875\n",
      "Epoch 5/50 - recon loss: 45.277 - latent loss: 17.854 - total loss: 63.131\n",
      "Epoch 6/50 - recon loss: 43.195 - latent loss: 17.94 - total loss: 61.135\n",
      "Epoch 7/50 - recon loss: 41.454 - latent loss: 17.997 - total loss: 59.45\n",
      "Epoch 8/50 - recon loss: 40.363 - latent loss: 17.998 - total loss: 58.361\n",
      "Epoch 9/50 - recon loss: 39.25 - latent loss: 18.037 - total loss: 57.287\n",
      "Epoch 10/50 - recon loss: 38.465 - latent loss: 17.984 - total loss: 56.449\n",
      "Epoch 11/50 - recon loss: 37.669 - latent loss: 17.993 - total loss: 55.662\n",
      "Epoch 12/50 - recon loss: 37.219 - latent loss: 18.015 - total loss: 55.235\n",
      "Epoch 13/50 - recon loss: 36.668 - latent loss: 17.984 - total loss: 54.652\n",
      "Epoch 14/50 - recon loss: 36.191 - latent loss: 17.97 - total loss: 54.161\n",
      "Epoch 15/50 - recon loss: 36.054 - latent loss: 17.969 - total loss: 54.023\n",
      "Epoch 16/50 - recon loss: 35.728 - latent loss: 17.94 - total loss: 53.668\n",
      "Epoch 17/50 - recon loss: 35.428 - latent loss: 17.946 - total loss: 53.374\n",
      "Epoch 18/50 - recon loss: 35.149 - latent loss: 17.929 - total loss: 53.078\n",
      "Epoch 19/50 - recon loss: 34.8 - latent loss: 17.911 - total loss: 52.712\n",
      "Epoch 20/50 - recon loss: 34.664 - latent loss: 17.896 - total loss: 52.561\n",
      "Epoch 21/50 - recon loss: 34.526 - latent loss: 17.9 - total loss: 52.426\n",
      "Epoch 22/50 - recon loss: 34.302 - latent loss: 17.901 - total loss: 52.202\n",
      "Epoch 23/50 - recon loss: 34.1 - latent loss: 17.897 - total loss: 51.997\n",
      "Epoch 24/50 - recon loss: 33.936 - latent loss: 17.898 - total loss: 51.834\n",
      "Epoch 25/50 - recon loss: 33.854 - latent loss: 17.883 - total loss: 51.737\n",
      "Epoch 26/50 - recon loss: 33.68 - latent loss: 17.908 - total loss: 51.588\n",
      "Epoch 27/50 - recon loss: 33.686 - latent loss: 17.89 - total loss: 51.575\n",
      "Epoch 28/50 - recon loss: 33.385 - latent loss: 17.905 - total loss: 51.29\n",
      "Epoch 29/50 - recon loss: 33.383 - latent loss: 17.885 - total loss: 51.267\n",
      "Epoch 30/50 - recon loss: 33.299 - latent loss: 17.879 - total loss: 51.177\n",
      "Epoch 31/50 - recon loss: 33.265 - latent loss: 17.87 - total loss: 51.135\n",
      "Epoch 32/50 - recon loss: 33.12 - latent loss: 17.858 - total loss: 50.979\n",
      "Epoch 33/50 - recon loss: 32.867 - latent loss: 17.853 - total loss: 50.72\n",
      "Epoch 34/50 - recon loss: 32.845 - latent loss: 17.849 - total loss: 50.694\n",
      "Epoch 35/50 - recon loss: 32.756 - latent loss: 17.864 - total loss: 50.62\n",
      "Epoch 36/50 - recon loss: 32.706 - latent loss: 17.848 - total loss: 50.554\n",
      "Epoch 37/50 - recon loss: 32.487 - latent loss: 17.887 - total loss: 50.375\n",
      "Epoch 38/50 - recon loss: 32.709 - latent loss: 17.825 - total loss: 50.533\n",
      "Epoch 39/50 - recon loss: 32.311 - latent loss: 17.872 - total loss: 50.183\n",
      "Epoch 40/50 - recon loss: 32.354 - latent loss: 17.851 - total loss: 50.205\n",
      "Epoch 41/50 - recon loss: 32.28 - latent loss: 17.849 - total loss: 50.129\n",
      "Epoch 42/50 - recon loss: 32.033 - latent loss: 17.847 - total loss: 49.881\n",
      "Epoch 43/50 - recon loss: 32.219 - latent loss: 17.834 - total loss: 50.053\n",
      "Epoch 44/50 - recon loss: 32.086 - latent loss: 17.846 - total loss: 49.932\n",
      "Epoch 45/50 - recon loss: 32.133 - latent loss: 17.842 - total loss: 49.975\n",
      "Epoch 46/50 - recon loss: 32.106 - latent loss: 17.809 - total loss: 49.915\n",
      "Epoch 47/50 - recon loss: 31.932 - latent loss: 17.828 - total loss: 49.76\n",
      "Epoch 48/50 - recon loss: 31.906 - latent loss: 17.824 - total loss: 49.731\n",
      "Epoch 49/50 - recon loss: 31.876 - latent loss: 17.832 - total loss: 49.708\n",
      "Epoch 50/50 - recon loss: 32.18 - latent loss: 17.795 - total loss: 49.975\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "use_gpu = t.cuda.is_available()\n",
    "model.train()\n",
    "\n",
    "#Add to gpu before making the optimizer\n",
    "if use_gpu:\n",
    "    model.cuda(0)\n",
    "\n",
    "optimizer = t.optim.Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "epoch_summary_text = '\\rEpoch {0}/{1}'\n",
    "loss_summary_text = '{0} loss: {1:.5}'\n",
    "\n",
    "percent_complete = '\\r{0:.1%}'\n",
    "\n",
    "n_data = len(training_set)\n",
    "for epoch in range(epochs):\n",
    "    recon_loss, latent_loss = train_vae(model, training_loader, optimizer, use_gpu)\n",
    "\n",
    "    #Create summary loss\n",
    "    epoch_summary = epoch_summary_text.format(epoch+1, epochs)\n",
    "    latent_summary = loss_summary_text.format('latent', latent_loss)\n",
    "    recon_summary = loss_summary_text.format('recon', recon_loss)\n",
    "    total_summary = loss_summary_text.format('total', latent_loss+recon_loss)\n",
    "    print(epoch_summary, recon_summary, latent_summary, total_summary, sep=' - ')\n",
    "    \n",
    "t.save(model.state_dict(), './vae.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Image Reconstruction\n",
    "\n",
    "Variational autoencoders can be used to reconstruct observations. This is exactly the training process: we encode the observation to a latent variable, and then use the latent variable to generate an image. After training, the network should be able to reconstruct both the example and similar examples. This is shown below. The images in the top row are the originals, while the images below are the reconstructions. The second row is a reconstruction using the mean, while the lower rows show what would be typically generated during training.\n",
    "\n",
    "The reconstructions are noticeably blurry. This is a fairly common issue with simple VAE models. There are a number of ways to improve the quality of the generated pictures. A few of these are mentioned below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up picture\n",
    "n_images = 6\n",
    "rows=5\n",
    "\n",
    "model.eval()\n",
    "with t.no_grad():\n",
    "    data_sample = next(iter(training_loader))\n",
    "    images, _ = data_sample\n",
    "    images = images[:n_images]\n",
    "    if use_gpu:\n",
    "        images = images.cuda()\n",
    "    \n",
    "    album = [images]\n",
    "    #Create most likely example (i.e. using the mean)\n",
    "    album.append(model.generate_similar(images, noise=False))\n",
    "    \n",
    "    #Create similar examples with noise\n",
    "    for row in range(2, rows):\n",
    "        album.append(model.generate_similar(images))\n",
    "\n",
    "album = t.cat(album, 0)\n",
    "save_image(album, './images/MNIST-VAE-reconstructions.png', nrow=n_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Reconstructions\n",
    "![Digit reconstructions](./images/MNIST-VAE-reconstructions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image generation using latent sampling\n",
    "\n",
    "My interest in VAEs are mostly tied to the fact that you can use them to generate new images. One of the original assumptions we made at the onset of our model is that $p_\\ell$ is normally distributed. Thus, we can generate $z$ by drawing from a standard normal distribution $N(0, I)$ and then decode that generated latent variable to form an observation. This is what we see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 10\n",
    "\n",
    "with t.no_grad():\n",
    "    latent_sample = t.randn(grid_size**2, latent_dim, device=album.device)\n",
    "    images = model.generate(latent_sample)\n",
    "\n",
    "save_image(images, './images/MNIST-VAE-latent_samples.png', nrow=grid_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly generated images\n",
    "![Latent Sample image](./images/MNIST-VAE-latent_samples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Interpolation\n",
    "\n",
    "Another way to use a Variational Autoencoder is to combine the latent encodings. That is, if we have an image $x_1$ and $x_2$ with latent encodings $z_1$ and $z_2$ respectively, we can form a third image $x_3$ from a combination of $z_1$ and $z_2$. This is shown below. The first and last images are the original. The second and fourth are the recreations of the original. The image in the middle, $x_3$, is formed using the latent encoding $(1-\\alpha)z_1+\\alpha z_2$, with $0 \\leq \\alpha \\leq 1$. By changing the sliders, the center changes. \n",
    "\n",
    "Note: if training the model locally would take too long, then I recommend downloading the file `/models/vae.pkl` and loading it using `t.load(model, 'vae.pkl')` to allow for experimentation with the below widget. This model can also be further trained if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_mix(image_idx_1, image_idx_2, mix_value):\n",
    "    with t.no_grad():\n",
    "        #Grab sample images\n",
    "        image_1 = training_set[image_idx_1-1][0]\n",
    "        image_2 = training_set[image_idx_2-1][0]\n",
    "        #Encode\n",
    "        input = t.stack([image_1, image_2], 0)\n",
    "        if use_gpu:\n",
    "            input = input.cuda()\n",
    "        latent = model.encode(input)[0]\n",
    "        #Mix latent codes\n",
    "        mixed_latent = (1-mix_value)*latent[0]+mix_value*latent[1]\n",
    "        #Decode into images\n",
    "        latent = t.stack([latent[0], mixed_latent, latent[1]], 0)\n",
    "        images = model.generate(latent)\n",
    "        #Turn into grid\n",
    "        images = make_grid(t.cat([input[0].unsqueeze(0), images, input[1].unsqueeze(0)]), 5)\n",
    "        #Permute for imshow\n",
    "        images = images.permute(1,2,0)\n",
    "        plt.imshow(images.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d774eb23fa8d4a02bf1115857c17314e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='Image 1', min=1), IntSlider(value=101, description='Imagâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "mix_slider = widgets.FloatSlider(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=1,\n",
    "    step=0.05,\n",
    "    description='Transition'\n",
    ")\n",
    "image_1_slider = widgets.IntSlider(\n",
    "    value=1,\n",
    "    min=1,\n",
    "    max=100,\n",
    "    description='Image 1'\n",
    ")\n",
    "\n",
    "image_2_slider = widgets.IntSlider(\n",
    "    value=101,\n",
    "    min=101,\n",
    "    max=200,\n",
    "    description='Image 2'\n",
    ")\n",
    "\n",
    "interactive_plot = widgets.interactive(image_mix, image_idx_1=image_1_slider, image_idx_2=image_2_slider, mix_value=mix_slider)\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '150px'\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "If you are interested in learning more about variational autoencoders, here is what I recommend:\n",
    "* [Sticking the Landing](https://arxiv.org/abs/1703.09194). As mentioned above, it explains why the MC loss is used rather than KL divergence.\n",
    "* [Towards a deeper understanding of VAEs](https://arxiv.org/abs/1702.08658v1), which explains in part why the blurry images above occur. I had originally believed it was a result of using an inappropriate loss function, but it turns out that this is actually caused by the decoder using the average reconstruction. In our notation, this means the decoder is learning to return $\\mathbb{E}_{p(x|z)}[x]$. The paper also discuss how to avoid blurry images.\n",
    "* [Improved Variational Inference with Inverse Autoregressive Flow](https://arxiv.org/abs/1606.04934v2), which describes how to encode to non-diagonal covariance matrices $\\Sigma(x)$ in a computationally straight-forward way. Another approach based on this paper is offered in [convex combination linear Inverse Autoregressive Flow](https://arxiv.org/abs/1706.0232), which is more easily applied.\n",
    "* It's also worth pointing out that you can use *several* latent variables. There are two variants I'm aware of for this: the Ladder Variational Autoencoder and the Variational Ladder Autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Comments\n",
    "\n",
    "I made this notebook to have an easy way of sharing my interest in VAEs with others. If you find an error or have any suggestions, please let me know."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
